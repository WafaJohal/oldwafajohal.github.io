<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wafa Johal</title>
    <link>https://wafajohal.github.io/</link>
      <atom:link href="https://wafajohal.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Wafa Johal</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en</language><lastBuildDate>Tue, 22 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wafajohal.github.io/media/icon_hua8722e2d0f9e76eb4e7e1bd847ea9615_2326_512x512_fill_box_center_3.png</url>
      <title>Wafa Johal</title>
      <link>https://wafajohal.github.io/</link>
    </image>
    
    <item>
      <title>Demo 1 OpenCV Basics</title>
      <link>https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo1_image_formation_opencvbasics/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo1_image_formation_opencvbasics/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# read an image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imread(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test1.jpg&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;,img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;waitKey(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;destroyAllWindows()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print image dimension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(225, 225, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print a pixel value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;px &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img[&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(px)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[  8 150 255]
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# get an ROI region&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;roi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img[&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# replace another region in the image with the selected ROI&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img[&lt;span style=&#34;color:#ae81ff&#34;&gt;180&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;180&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; roi
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# display the modified image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image2&amp;#39;&lt;/span&gt;,img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;waitKey(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;destroyAllWindows()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# write the image output into a new file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imwrite(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;res_1.png&amp;#39;&lt;/span&gt;,img)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x28e7f6ba550&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo1/output_9_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#convert BGR to RGB&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rgb_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(rgb_img)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x28e7f7680b8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo1/output_10_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## to look at the documentation of a function inside your notebook use a &amp;#39;?&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;cvtColor(src, code[, dst[, dstCn]]) -&amp;gt; dst
.   @brief Converts an image from one color space to another.
.   
.   The function converts an input image from one color space to another. In case of a transformation
.   to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note
.   that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the
.   bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue
.   component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and
.   sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.
.   
.   The conventional ranges for R, G, and B channel values are:
.   -   0 to 255 for CV_8U images
.   -   0 to 65535 for CV_16U images
.   -   0 to 1 for CV_32F images
.   
.   In case of linear transformations, the range does not matter. But in case of a non-linear
.   transformation, an input RGB image should be normalized to the proper value range to get the correct
.   results, for example, for RGB \f$\rightarrow\f$ L\*u\*v\* transformation. For example, if you have a
.   32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will
.   have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,
.   you need first to scale the image down:
.   @code
.   img *= 1./255;
.   cvtColor(img, img, COLOR_BGR2Luv);
.   @endcode
.   If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many
.   applications, this will not be noticeable but it is recommended to use 32-bit images in applications
.   that need the full range of colors or that convert an image before an operation and then convert
.   back.
.   
.   If conversion adds the alpha channel, its value will set to the maximum of corresponding channel
.   range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.
.   
.   @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision
.   floating-point.
.   @param dst output image of the same size and depth as src.
.   @param code color space conversion code (see #ColorConversionCodes).
.   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the
.   channels is derived automatically from src and code.
.   
.   @see @ref imgproc_color_conversions
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Demo Gaussian Blur</title>
      <link>https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_gaussianblur/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_gaussianblur/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; matplotlib &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imread(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;noisyImg_1.jpg&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#convert BGR to RGB&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x1de43671b70&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/gaussian_blur/output_1_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#Gaussian filtering&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;blur &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;GaussianBlur(img,(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;),&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(blur)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x1de437332e8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/gaussian_blur/output_2_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getGaussianKernel&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1;31mDocstring:[0m
getGaussianKernel(ksize, sigma[, ktype]) -&amp;gt; retval
.   @brief Returns Gaussian filter coefficients.
.   
.   The function computes and returns the \f$\texttt{ksize} \times 1\f$ matrix of Gaussian filter
.   coefficients:
.   
.   \f[G_i= \alpha *e^{-(i-( \texttt{ksize} -1)/2)^2/(2* \texttt{sigma}^2)},\f]
.   
.   where \f$i=0..\texttt{ksize}-1\f$ and \f$\alpha\f$ is the scale factor chosen so that \f$\sum_i G_i=1\f$.
.   
.   Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize
.   smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly.
.   You may also use the higher-level GaussianBlur.
.   @param ksize Aperture size. It should be odd ( \f$\texttt{ksize} \mod 2 = 1\f$ ) and positive.
.   @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as
.   `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`.
.   @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .
.   @sa  sepFilter2D, getDerivKernels, getStructuringElement, GaussianBlur
[1;31mType:[0m      builtin_function_or_method
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#Gaussian filtering v2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;G &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getGaussianKernel(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(G)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;GM &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(GM)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;matshow(GM)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[[0.17820326]
 [0.21052227]
 [0.22254894]
 [0.21052227]
 [0.17820326]]
[[0.0317564  0.03751576 0.03965895 0.03751576 0.0317564 ]
 [0.03751576 0.04431963 0.04685151 0.04431963 0.03751576]
 [0.03965895 0.04685151 0.04952803 0.04685151 0.03965895]
 [0.03751576 0.04431963 0.04685151 0.04431963 0.03751576]
 [0.0317564  0.03751576 0.03965895 0.03751576 0.0317564 ]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/gaussian_blur/output_4_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;blur2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter2D(img,&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,GM)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#show original and filtered images&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Original&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks([]), plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;yticks([])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(blur),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Blurred&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks([]), plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;yticks([])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(blur2),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Blurred 2&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks([]), plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;yticks([])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/gaussian_blur/output_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Demo 2 Color Spaces</title>
      <link>https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo2_image_formation_colorspaces/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo2_image_formation_colorspaces/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imread(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;beachBox.jpg&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;axis(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;off&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# if you want to hide the axis&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(743, 1280, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_1_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Documentation on &lt;a href=&#34;https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;imread&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bookmark this &lt;a href=&#34;https://docs.opencv.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.opencv.org/&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__version__
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&#39;3.4.5&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;plot-the-image&#34;&gt;Plot the image&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;RGB_im &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(RGB_im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(RGB_im)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(743, 1280, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_5_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;rgb-channel-decomposition&#34;&gt;RGB Channel Decomposition&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/ColorSpaces/RGB.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RGB_im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# (x,y,nb_channels = 3  [R, G, B])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# set green and red channels to 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# sets the value in the red channels to 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# sets the value in the green channels to 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;g &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RGB_im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# set blue and red channels to 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;g[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;g[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RGB_im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# set blue and green channels to 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;r[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;r[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(r,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_RGB2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(b,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_RGB2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(g,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_RGB2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(RGB_im)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(g)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(b)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_7_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_7_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_7_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;hsv&#34;&gt;HSV&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/ColorSpaces/HSV.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;imgHSV &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2HSV)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgHSV&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# set S and V channels to 255&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgHSV&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# set H to 179 and V to 255 (max)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;179&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgHSV&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# set H to 179 and S to 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;179&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h_RGB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(h,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_HSV2RGB) &lt;span style=&#34;color:#75715e&#34;&gt;# convert back to RGB for matplotlib to be able to plot it properly &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s_RGB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(s,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_HSV2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v_RGB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(v,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_HSV2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(RGB_im)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(h_RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(s_RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(v_RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_10_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_10_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_10_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;lab&#34;&gt;LAB&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/ColorSpaces/Lab.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;imgLAB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2LAB)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.colors &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; clr &lt;span style=&#34;color:#75715e&#34;&gt;# this library is used to create the color maps for matplotlib &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;l &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgLAB&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;l[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;l[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgLAB&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;127&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgLAB&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;127&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b[:, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# here we want to result to be encoded on one channel only, so we can convert the color image into a GRAY scale image. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Since opencv doesnt have the LAB2GRAY converter, we have to go through the RGB format and then to GRAY&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;l_RGB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(l,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_LAB2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;l_GRAY &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(l_RGB,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_RGB2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a_RGB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(a,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_LAB2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a_GRAY &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(a_RGB,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_RGB2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_RGB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(b,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_LAB2RGB)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_GRAY &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(b_RGB,cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_RGB2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(RGB_im)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(l_GRAY, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# L is on the gray scale&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# the &amp;#39;a&amp;#39; values are between red and green, so we create a colormap for matplotlib to display the color range correctly&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cmap_a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; clr&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LinearSegmentedColormap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_list(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;custom blue&amp;#39;&lt;/span&gt;, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Red&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Gray&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Green&amp;#39;&lt;/span&gt;], N&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(a_GRAY,  cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cmap_a)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Same for &amp;#39;b&amp;#39;, between yellow and blue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cmap_b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; clr&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LinearSegmentedColormap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_list(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;custom blue&amp;#39;&lt;/span&gt;, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Yellow&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Gray&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Blue&amp;#39;&lt;/span&gt;], N&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(b_GRAY, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cmap_b)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_13_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_13_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo2/output_13_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;ycrcb&#34;&gt;YCrCb&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;imgYCrCb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2YCrCb)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Your turn! &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Try to display the different channels of the YCrCb format individually for our given image using the method above or another method&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Data-Driven Curriculum Analysis</title>
      <link>https://wafajohal.github.io/project/curriculum_analysis/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/curriculum_analysis/</guid>
      <description>&lt;p&gt;Curriculum Analysis has for objectives: 1) to assess the curriculum to improve it, 2) to identify potential problem (gaps, overlap of topics), 3) to plan for updates of the curriculum and 4) to prepare audits of the curriculum. Unlike curriculum design or development, curriculum analysis looks at the current state of what is being taught and provides a means to unpack the components (e.g., knowledge, resources), investigate interconnections and reveal the relationships between these components (e.g., similarity, dependencies, alignment).&lt;/p&gt;
&lt;p&gt;Often curriculum analysis is performed manually by lecturing staff who outline the knowledge in a subject and its relationships to the rest of a course (pre-requisites …) creating a map. Unfortunately, this is time consuming, and can be difficult to track and achieve completeness. Previous works have used knowledge maps and concept mapping to understand the links between subjects. We also found instances in which researchers proposed to use handbook entries to design an automatic tool to perform curriculum analysis. While the handbook of a course gives an overview of its content, it can be quite abstract and often doesn’t inform much about all the concepts covered in a course.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PSQuJKMRKmA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In this project, we propose a novel method to build high quality representations of courses that illustrate the relationships between lectures, subjects, and courses. Using the curriculum and teaching materials generated for the purpose of teaching a subject (e.g., lecture slides, assignment sheets, handbook), this project will generate interactive visualisations of the curriculum that can be used to inform curriculum updates and design.  Teaching material This feature-based representation can then be used to assess subject similarity, infer pre-requisites, build concept maps, and perform clustering to design streams. We foresee this tool to be useful for both academic staff, designing and assessing curriculum and students who could use the tool to recommend subjects based on similarity or other relationships.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making the headline of  Robohub</title>
      <link>https://wafajohal.github.io/post/20220205_nao_blogpost/</link>
      <pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/20220205_nao_blogpost/</guid>
      <description>&lt;p&gt;Our paper &amp;ldquo;10 years of Human-NAO Interaction Research: A Scoping Review&amp;rdquo; was featured in a robohub blog article:
&lt;a href=&#34;https://robohub.org/happy-birthday-nao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://robohub.org/happy-birthday-nao/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Women in AI co-chair at AJCAI</title>
      <link>https://wafajohal.github.io/post/2022_women-in-ai_2022/</link>
      <pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/2022_women-in-ai_2022/</guid>
      <description>&lt;p&gt;For the first time ever, the Australasian Joint Conference in Artificial Intelligence had two Women-in-AI chairs.
Mary-Anne Williams, General Chair of the 2021 edition of the conference contacted me and Prof. Sue Keay to take this new role of Women in AI co-chairs for the conference.&lt;/p&gt;
&lt;p&gt;Since there had never been WAI chairs, the floor was open for us to propose any activities.
The conference was planned to be help in person and hence we had planned plently of opportunities for our participants to network (Panel Discussion, Networking lunch, training event on inclusion and diversity).
Unofrtunately the conference moved online. We pivoted by running the Panel discussion online and by having an open lunch discussion with participants.&lt;/p&gt;
&lt;p&gt;We also proposed and wrote a Diversity and Inclusion Policy for AJCAI.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Demonstration of the Taxonomy of Functional Augmented Reality for Human-Robot Interaction</title>
      <link>https://wafajohal.github.io/publication/phaijit-2022-demonstration/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/phaijit-2022-demonstration/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Taxonomy of Functional Augmented Reality for Human-Robot Interaction</title>
      <link>https://wafajohal.github.io/publication/phaijit-obaid-johal-sammut-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/phaijit-obaid-johal-sammut-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatic assessment of motor impairments in Autism Spectrum Disorders: a systematic review</title>
      <link>https://wafajohal.github.io/publication/gargot-2022-automatic/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/gargot-2022-automatic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Envisioning social drones in education</title>
      <link>https://wafajohal.github.io/publication/johal-2022-envisioning/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2022-envisioning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint Action, Adaptation, and Entrainment in Human-Robot Interaction</title>
      <link>https://wafajohal.github.io/publication/fourie-2022-joint/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/fourie-2022-joint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lets Compete! The Influence of Human-Agent Competition and Collaboration on Agent Learning and Human Perception</title>
      <link>https://wafajohal.github.io/publication/phaijit-sammut-johal-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/phaijit-sammut-johal-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Leveraging eye tracking to understand children’s attention during game-based, tangible robotics activities</title>
      <link>https://wafajohal.github.io/publication/olsen-2022-leveraging/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/olsen-2022-leveraging/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Social Drones for Health and Well-Being</title>
      <link>https://wafajohal.github.io/publication/obaid-2022-socialdrone/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/obaid-2022-socialdrone/</guid>
      <description></description>
    </item>
    
    <item>
      <title>To Transfer or Not To Transfer: Engagement Recognition within Robot-assisted Autism Therapy</title>
      <link>https://wafajohal.github.io/publication/rakhymbayeva-2022-transfer/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/rakhymbayeva-2022-transfer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🏆 GROW Faculty Engineering Grant - Awarded!</title>
      <link>https://wafajohal.github.io/post/grow2021/</link>
      <pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/grow2021/</guid>
      <description>&lt;p&gt;I have been awarded 20,000AUD by the Faculty of Engineering to start a new prject called GIRL &amp;ldquo;Gamified and Interactive Robot Learning&amp;rdquo;.
I really look forward to starting to work on this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>📣 2 PhD Positions in Human-Robot Interaction 🤖</title>
      <link>https://wafajohal.github.io/post/2phdoffering/</link>
      <pubDate>Wed, 17 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/2phdoffering/</guid>
      <description>&lt;h2 id=&#34;faculty-of-engineering-school-of-computer-science-and-engineering-unsw-sydney&#34;&gt;Faculty of Engineering, School of Computer Science and Engineering, UNSW Sydney&lt;/h2&gt;
&lt;h3 id=&#34;start-date-fall-2021&#34;&gt;Start Date: Fall, 2021&lt;/h3&gt;
&lt;h3 id=&#34;area-of-research-human-robot-interaction&#34;&gt;Area of Research Human-Robot Interaction.&lt;/h3&gt;
&lt;p&gt;Robots are foreseen to be used in a variety of home contexts and interacting with novice users, such as assisting the dependent elderly in daily housework (i.e., caring and unpacking groceries, fetching, and pouring a glass of water, etc). Enabling robots to adapt to their environment through learning context specific tasks would be necessary for them to be used adequately by non-programming users. Our research aims to investigate novel methods to enable users to provide accurate and robust data for a social robot to learn from them.&lt;/p&gt;
&lt;h3 id=&#34;candidate-profile&#34;&gt;Candidate Profile&lt;/h3&gt;
&lt;h4 id=&#34;required-skills&#34;&gt;Required Skills&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Bachelor or masters degree in Computer Sciences or a related field&lt;/li&gt;
&lt;li&gt;Good programming skills are required: C++, Python or others&lt;/li&gt;
&lt;li&gt;Training and skills in AI, machine learning or robotics is preferable&lt;/li&gt;
&lt;li&gt;Knowledge of classical programming frameworks in these domains is appreciated: Keras, PyTorch, TensorFlow, ROS, etc.&lt;/li&gt;
&lt;li&gt;English proficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;skills-nice-to-have&#34;&gt;Skills Nice to Have&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A taste for user/application and impact driven research&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;research-environment&#34;&gt;Research Environment&lt;/h3&gt;
&lt;p&gt;Depending on the research focus, joint supervisors specialised in Design, Robotics, AI or HCI, will be invited. UNSW and CSE will provide you with access to the HCI and Robotics Labs, GPU clusters and the HRI experimental facility equipped with more than 200 sensors (available to run studies). UNSW Sydney is in the top 100 universities in the world, with more than 59,000 students and a 7,000-strong research community. At UNSW, you will benefit from an international and vibrant research community in a pleasant environment.
The net amount of the scholarship will be approximately AUD 2400 per month, increasing annually. You will also receive a holiday allowance. Additional financial support is available for attending conferences and workshops. There will also be possibilities for a research visit (industry or international) during the PhD.&lt;/p&gt;
&lt;h3 id=&#34;queries&#34;&gt;Queries&lt;/h3&gt;
&lt;p&gt;For informal queries, do not hesitate to contact &lt;a href=&#34;mailto:wafa.johal@unsw.edu.au&#34;&gt;wafa.johal@unsw.edu.au&lt;/a&gt;&lt;br&gt;
Your application should include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A letter highlighting your motivation for your application including: Why do you wish to pursue a PhD in human-robot interaction? What prior experience do you have? How would you approach human-robot interaction research? Also indicate your research interests.&lt;/li&gt;
&lt;li&gt;A CV, with copies of relevant grades, masters thesis work or publications.&lt;/li&gt;
&lt;li&gt;The names and contact details of at least 2 referees.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Selected candidates will be invited for an interview, which can be organised over Zoom/Teams.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Join the Group</title>
      <link>https://wafajohal.github.io/project/join_the_group/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/join_the_group/</guid>
      <description>&lt;p&gt;Enabling robots to navigate in indoors environments in a safe and socially acceptable manner around groups of humans is still an open research area. Socially aware navigation considers the multi-modal assessment of the group dynamics, group formation inferring, path planning, real-time path adaptation, and human-robot communication.
Up to now the research in the field has considered a couple of classical scenarios such as crossing a group in a corridor or passing a door. Limitations in terms of lack of realistic datasets is often mentioned in the field. In this research project, we will aim to design several scenarios involving groups of humans in realistic settings. Our work will focus on three main tasks for the robot: 1) approach and join a group, 2) passing by a group and 3) greeting a group. A first step of the project will be to \textbf{record a novel dataset} with rich interactions between the humans (H-H scenarios) and between humans and a teleoperated robot (H-R scenarios). The dataset will be collected at the HRI facility allowing multimodal synchronous recording. After that, a \textbf{new model for path planning} will be developed. For the model, we will explore rule-based constraints (i.e. not passing between two persons speaking together) and learned constrained using the dataset recorded to infer implicit social norms. Finally, the \textbf{model will be tested} empirically with new users in which the robot will have to take real-time path planning decisions.&lt;/p&gt;
&lt;h2 id=&#34;call-for-participation-to-experiment&#34;&gt;Call for participation to experiment&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>10 Years of Human-NAO Interaction Research: A Scoping Review</title>
      <link>https://wafajohal.github.io/publication/amirova-2021-nao/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/amirova-2021-nao/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robots for Learning - Learner-Centred Design</title>
      <link>https://wafajohal.github.io/publication/johal-2021-r-4-l/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2021-r-4-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speech-Based Gesture Generation for Robots and Embodied Agents: A Scoping Review</title>
      <link>https://wafajohal.github.io/publication/liu-2021-speech/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/liu-2021-speech/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The transferability of handwriting skills: from the Cyrillic to the Latin alphabet</title>
      <link>https://wafajohal.github.io/publication/asselborn-2021-transferability/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/asselborn-2021-transferability/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Valley of non-Distraction: Effect of Robot&#39;s Human-likeness on Perception Load</title>
      <link>https://wafajohal.github.io/publication/ingle-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ingle-2021/</guid>
      <description>&lt;a href=&#34;https://wafajohal.github.io/files/papers/LBR_HRI2021_Anthropo_Load.pdf&#34; target=&#34;_blank&#34;&gt; PDF &lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>🏆 Faculty Engineering Award for our ECAN crew!</title>
      <link>https://wafajohal.github.io/post/20201215_ecan_facultyaward/</link>
      <pubDate>Fri, 25 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/20201215_ecan_facultyaward/</guid>
      <description>&lt;p&gt;Our Early Career Academic Network Committee,  &lt;a href=&#34;https://research.unsw.edu.au/ecan-engineering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eng.  ECAN&lt;/a&gt;, received the Faculty Award of Equity and Diversity from the Dean of the Faculty of Engineering, Prof. Stephen Foster.&lt;/p&gt;
&lt;p&gt;I am delighted to be part of such a vibrant team, working together to make ECAs shine!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>🏆 I was awarded with an ARC DECRA fellowship!</title>
      <link>https://wafajohal.github.io/post/decra2021/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/decra2021/</guid>
      <description>&lt;p&gt;About a year after landing foot in Australia, I have been awarded with a Discovery Early Career Research Award (DECRA) by the Asutralian Research Council (acceptance 16%).&lt;/p&gt;
&lt;p&gt;The project I proposed is inline with my vision to build co-learning system in which robots can teach novices some skills (see &lt;a href=&#34;https://wafajohal.github.io/project/cowriter/&#34;&gt;CoWriter Project&lt;/a&gt;) but also learn from human trainers.
To find out more about the project see the page: &lt;a href=&#34;https://wafajohal.github.io/project/hurl/&#34;&gt;HURL&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HURL</title>
      <link>https://wafajohal.github.io/project/hurl/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/hurl/</guid>
      <description>&lt;p&gt;Robots are foreseen to be used in a variety of home contexts and interacting with novice users, such as assisting the dependent elderly in daily housework (i.e., caring and unpacking groceries, fetching, and pouring a glass of water, etc). Enabling robots to adapt to their environment through learning context specific tasks would be necessary for them to be used adequately by non-programming users.&lt;/p&gt;
&lt;p&gt;Several methods have been proposed to teach new skills to robots while keeping the human in the loop. Among these methods, the Reinforcement Learning (“RL”) approach is the most common one. However, the literature reports several issues with including human trainers in RL scenarios. Significant research reports a positive bias in RL rewards, and that human-generated reward signals change as the learning progress being inconsistent over time (the trainer adapts her strategy). This can be explained by the difficulty for human trainers to teach basic procedural motions. They generally tend to exaggerate their demonstrations or be more kind with time.&lt;/p&gt;
&lt;p&gt;In education, a good instructor maintains a mental model of the learner’s state (what has been learned and what needs clarification). This helps the teacher to appropriately structure the upcoming learning tasks with timely feedback and guidance. The learner can help the instructor by expressing their internal state via communicative acts that reveal their understanding, confusion, and attention. Robot’s learning parameters, however, can be overwhelming for a novice user and may increase the human workload (by increasing inaccurate feedbacks, and hence decreasing the robot’s learning). The challenge lies on training humans to be efficient trainers and enabling them to plan, assess and manage the robot’s learning.&lt;/p&gt;
&lt;p&gt;Another noticeable issue is the disengagement of humans during the training task. Teaching procedural skills to a robot learner can be time consuming and repetitive. This often results in increased noise in human feedback making their input less reliable. Some researchers have imagined several strategies for the robot to cope with this, such as detecting inconsistencies and asking for additional feedback. This project proposes to investigate how collaborative and competitive games could enable better quality feedback when robots are learning from humans. Inspired by instructional design, we will study how building teaching tools for human teachers can effectively improve the robot’s learning. We will also aim to engage the trainer longer by identifying and integrating a gamification element in the training.&lt;/p&gt;
&lt;p&gt;This project is a 3-year endeavour funded by the Australian Research Council starting mid-2021. Under this umbrella, we are looking at hiring two PhD students in the Faculty of Engineering at the School of Computer Science. This project will make an extensive use of the National Facility for Human Robot Interaction Research.&lt;/p&gt;
&lt;p&gt;For more information about the PhD positions contact Dr Wafa Johal: &lt;a href=&#34;mailto:wafa.johal@unsw.edu.au&#34;&gt;wafa.johal@unsw.edu.au&lt;/a&gt;  and consult &lt;a href=&#34;https://wafa.johal.org/prospective/phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wafa.johal.org/prospective/phd/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Domestic Drones: Context of Use in Research Literature</title>
      <link>https://wafajohal.github.io/publication/obaid_hai_2020/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/obaid_hai_2020/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vSn43ovcyD-23eboGn2c_2ss1gy5gCN2pac5N6QUUsxH10Br3WYeTBmYBvNqDZTS8sam8XaoxpIbHjB/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Social (Flying) Robots for Education. September 2020.</title>
      <link>https://wafajohal.github.io/talk/2020_edudrone/</link>
      <pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/2020_edudrone/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQ01VhR7_mmKu26bLyZNljEbgiYHXJ4EB_2piY3gJ8QdbbtSwmPaZ2FkO4JiLnfnw/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The SydCHI Local Chapter just got accepted</title>
      <link>https://wafajohal.github.io/post/sydchi_accepted/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/sydchi_accepted/</guid>
      <description>&lt;p&gt;Super excited about the new SydCHI local chapter.
Stay tune!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Tabletop Robots to promote Inclusive Classroom Experiences</title>
      <link>https://wafajohal.github.io/publication/neto-using-2020/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/neto-using-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🏆 Best Demo Award at HRI 2020!</title>
      <link>https://wafajohal.github.io/post/hri2020_demo_award/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/hri2020_demo_award/</guid>
      <description>&lt;p&gt;Our international team received the best demo award at @HRI2020 @dillenbo @AnaraSandy! #SNSF #CoKaz
&lt;a href=&#34;http://humanrobotinteraction.org/2020/best-paper-awards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://humanrobotinteraction.org/2020/best-paper-awards/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demo 3 Resolution</title>
      <link>https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo3_image_formation_resolution/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo3_image_formation_resolution/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imread(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;minion.jpg&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# read as a grayscale image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(532, 800)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo3/output_1_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;, vmin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, vmax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo3/output_2_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Grayscale Image&amp;#39;&lt;/span&gt;, img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;waitKey(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# press any key&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;destroyAllWindows()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;resize-image&#34;&gt;Resize Image&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s play with the resolution of this image and try to resize it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1;31mDocstring:[0m
resize(src, dsize[, dst[, fx[, fy[, interpolation]]]]) -&amp;gt; dst
.   @brief Resizes an image.
.   
.   The function resize resizes the image src down to or up to the specified size. Note that the
.   initial dst type or size are not taken into account. Instead, the size and type are derived from
.   the `src`,`dsize`,`fx`, and `fy`. If you want to resize src so that it fits the pre-created dst,
.   you may call the function as follows:
.   @code
.   // explicitly specify dsize=dst.size(); fx and fy will be computed from that.
.   resize(src, dst, dst.size(), 0, 0, interpolation);
.   @endcode
.   If you want to decimate the image by factor of 2 in each direction, you can call the function this
.   way:
.   @code
.   // specify fx and fy and let the function compute the destination image size.
.   resize(src, dst, Size(), 0.5, 0.5, interpolation);
.   @endcode
.   To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to
.   enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR
.   (faster but still looks OK).
.   
.   @param src input image.
.   @param dst output image; it has the size dsize (when it is non-zero) or the size computed from
.   src.size(), fx, and fy; the type of dst is the same as of src.
.   @param dsize output image size; if it equals zero, it is computed as:
.   \f[\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\f]
.   Either dsize or both fx and fy must be non-zero.
.   @param fx scale factor along the horizontal axis; when it equals 0, it is computed as
.   \f[\texttt{(double)dsize.width/src.cols}\f]
.   @param fy scale factor along the vertical axis; when it equals 0, it is computed as
.   \f[\texttt{(double)dsize.height/src.rows}\f]
.   @param interpolation interpolation method, see #InterpolationFlags
.   
.   @sa  warpAffine, warpPerspective, remap
[1;31mType:[0m      builtin_function_or_method
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;half &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize(img, &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;, fx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, fy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(half&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(half,  cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;, vmin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, vmax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(266, 400)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo3/output_6_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# if you run this cell several times, you will see the resolution decreasing very fast.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;half &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize(half, &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;, fx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, fy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(half&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(half,  cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;, vmin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, vmax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(133, 200)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week1/demo3/output_7_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;half&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(133, 200)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;half&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;26600
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;half&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>\&#34;If You&#39;ve Gone Straight, Now, You Must Turn Left\&#34; - Exploring the Use of a Tangible Interface in a Collaborative Treasure Hunt for People with Visual Impairments</title>
      <link>https://wafajohal.github.io/publication/chibaudel-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/chibaudel-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Comparison of Social Robot to Tablet and Teacher in a New Script Learning Context</title>
      <link>https://wafajohal.github.io/publication/zhanel-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/zhanel-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Acquisition of handwriting in children with and without dysgraphia: A computational approach</title>
      <link>https://wafajohal.github.io/publication/gargot-2020-acquisition/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/gargot-2020-acquisition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CoWriting Kazakh: Learning a New Script with a Robot</title>
      <link>https://wafajohal.github.io/publication/sandygulova-hri-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/sandygulova-hri-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CoWriting Kazakh: Learning a New Script with a Robot - Demonstration</title>
      <link>https://wafajohal.github.io/publication/bolat-hri-2020-demo/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/bolat-hri-2020-demo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CoWriting Kazakh: Learning a New Script with a Robot - Video </title>
      <link>https://wafajohal.github.io/publication/zhanel-hri-2020-video/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/zhanel-hri-2020-video/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Demo Bit-Planes</title>
      <link>https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_bitplanes/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_bitplanes/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; matplotlib &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imread(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;minion.jpg&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/bitplanes/output_2_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# create an image for each k bit plane&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    plane &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;full((img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]), &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; k, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uint8)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# execute bitwise and operation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bitwise_and(plane, img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# multiply ones (bit plane sliced) with 255 just for better visualization&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;equalizeHist(res)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    out&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(res)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,k&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(res,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;plane &amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;str(k))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks([]), plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;yticks([])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/bitplanes/output_3_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ms_planes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; out[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;out[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;out[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(ms_planes,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; plane 5-7&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(&amp;lt;matplotlib.image.AxesImage at 0x243a2f83be0&amp;gt;, Text(0.5, 1.0, &#39; plane 5-7&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/bitplanes/output_4_1.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Demo Fourier Transform and Frequency filtering</title>
      <link>https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_fourier_smoothing/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_fourier_smoothing/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; matplotlib &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imread(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;minion.jpg&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#convert BGR to RGB&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# show the image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img, cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# display the historgram&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatten())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_2_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# create the centering matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;centering_m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;full((img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] , img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]), &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        centering_m[x,y] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(centering_m[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(centering_m&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flat)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;centered_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;centering_m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(centered_img, cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(centered_img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatten())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_4_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# compute the 2D FFT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dft &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft2(centered_img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(dft)), cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x1dfe72c33c8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_5_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;distance&lt;/span&gt;(point1,point2):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt((point1[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;point2[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (point1[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;point2[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;filters&#34;&gt;Filters&lt;/h2&gt;
&lt;h2 id=&#34;ideal-lowpass-filter&#34;&gt;Ideal LowPass Filter&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/equations/idealLP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;idealFilterLP&lt;/span&gt;(D0,imgShape):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    base &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rows, cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    center &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (rows&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,cols&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(cols):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(rows):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; distance((y,x),center) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; D0:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                base[y,x] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; base
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(idealFilterLP(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape), cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Low Pass Filter&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ideal-highpass-filter&#34;&gt;Ideal HighPass Filter&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/equations/idealHP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;idealFilterHP&lt;/span&gt;(D0,imgShape):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    base &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rows, cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    center &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (rows&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,cols&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(cols):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(rows):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; distance((y,x),center) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; D0:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                base[y,x] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; base
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(idealFilterHP(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape), cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;High Pass Filter&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;gaussian-lowpass-filter&#34;&gt;Gaussian LowPass Filter&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/equations/gaussianLP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gaussianLP&lt;/span&gt;(D0,imgShape):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    base &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rows, cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    center &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (rows&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,cols&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(cols):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(rows):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            base[y,x] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;distance((y,x),center)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(D0&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; base
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(gaussianLP(&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape), cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Gaussian Low Pass Filter&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks([]), plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;yticks([])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;gaussian-highpass-filter&#34;&gt;Gaussian HighPass Filter&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/equations/gaussianHP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gaussianHP&lt;/span&gt;(D0,imgShape):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    base &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rows, cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; imgShape[:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    center &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (rows&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,cols&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(cols):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(rows):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            base[y,x] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;distance((y,x),center)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(D0&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; base
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(gaussianHP(&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape), cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;),plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Gaussian High Pass Filter&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks([]), plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;yticks([])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;apply-filter-on-dft&#34;&gt;Apply Filter on DFT&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# filter the DFT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fdft &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dft &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; idealFilterLP(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape) &lt;span style=&#34;color:#75715e&#34;&gt;## note that we do a multiplication cause we are in the frequency domain&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# inverse the filtered DFT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;idft &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ifft2(fdft)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# take the real part and de center it&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;real_centered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;real &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; centering_m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(fdft)), cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Filtered DFT&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(real_centered, cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;,vmin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, vmax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Inversed Filtered DFT&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# filter the DFT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fdft &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dft &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gaussianLP(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;,img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# inverse the filtered DFT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;idft &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ifft2(fdft)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# take the real part and de center it&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;real_centered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;real &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; centering_m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(fdft)), cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Filtered DFT&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(real_centered, cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;, vmin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, vmax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Inversed Filtered DFT&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;demonstraring-that-we-can-go-back-and-forth-from-image-to-frequency-without-loss&#34;&gt;Demonstraring that we can go back and forth from image to frequency without loss&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# inverse the filtered DFT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;idft &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ifft2(dft)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# take the real part and de center it&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;real_centered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;real &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; centering_m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img, cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; Original Image&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(real_centered, cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;, vmin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, vmax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Inversed DFT&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(real_centered &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; img, cmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;, vmin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, vmax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Difference&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/teaching/computer_vision/week2/fourier/output_20_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Role of Perspective Taking in Educational Child-Robot Interaction</title>
      <link>https://wafajohal.github.io/publication/yadollahi-2020-exploring/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/yadollahi-2020-exploring/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gamified Motor Training with Tangible Robots in Older Adults: a Feasibility Study and Comparison with Young</title>
      <link>https://wafajohal.github.io/publication/guneysu-ozgur-gamified-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/guneysu-ozgur-gamified-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>iReCheck</title>
      <link>https://wafajohal.github.io/project/irecheck/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/irecheck/</guid>
      <description>&lt;p&gt;Nearly 8% of children between 4 and 12 are dysgraphic or encounter serious difficulties in learning handwriting. Dysgraphia, just like dyslexia, can seriously impair the everyday school life of children and have damageable impact on their academic achievements. The detection of such difficulties should be done as soon as possible to minimize its effect on the child’s school performances. And an adaptive remediation should be provided. The CHILI Lab at EPFL showed how a robotic co-learner agent could have positive effects on the child&amp;rsquo;s motivation to practice handwriting. However, motivation is only one aspect that needs to be taken into account in order to provide a long-term adaptive training experience for children learning how to write.&lt;/p&gt;
&lt;p&gt;In the iReCHeCk project, we propose a finer characterization of handwriting, as a multimodal activity, taking into account the body posture to capture important features in the handwriting process. We will develop engaging training activities with a robot to monitor the learning status of the child, allowing therapists to evaluate progresses, while adapting the robot&amp;rsquo;s attitude and the training task to the needs of the learner. Our goal being to touch a wide range of handwriting learners, these activities will be tested on two populations: Typically Developed Children that are learning how to write at school; Children with Neuro-Developmental Disorders, presenting handwriting difficulties along with other disorders (i.e. attentional, autistic).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Iterative Design and Evaluation of a Tangible Robot-Assisted Handwriting Activity for Special Education</title>
      <link>https://wafajohal.github.io/publication/guneysu-ozgur-iterative-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/guneysu-ozgur-iterative-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>List of Typical Questionnaires used in HRI</title>
      <link>https://wafajohal.github.io/teaching/human-robot-interaction/questionnaires/questionnaires/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/teaching/human-robot-interaction/questionnaires/questionnaires/</guid>
      <description></description>
    </item>
    
    <item>
      <title>P. 114 Automatic assessment of motors impairments in autism spectrum disorders: a systematic review</title>
      <link>https://wafajohal.github.io/publication/gargot-2020-p/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/gargot-2020-p/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research Trends in Social Robots for Learning</title>
      <link>https://wafajohal.github.io/publication/johal-2020-research/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2020-research/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Swarm Robots in Education: A Review of Challenges and Opportunities</title>
      <link>https://wafajohal.github.io/publication/johal-2020-swarm/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2020-swarm/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQZcoqR-wwo6cZBj55T4ZyLd5FPTBXon6Hsljqr3ytAA5nFJ7LSmlpq-O3_SVFwPVqqwO64yi3mk2eI/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration</title>
      <link>https://wafajohal.github.io/talk/ozchi2019/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/ozchi2019/</guid>
      <description>&lt;p&gt;Presented our work on the design of tangible devices for classroom orchestration.&lt;/p&gt;
&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vSAbsZww1VUF4gDexQLD7Goez5ApAkyU85mnNXrNaPgeiHIzSs9Aeo1RNkZWp3WaAK-aT4gjm0zMDID/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>[HRI2020 Paper Accepted!] on Transposing CoWriter to Learning the New Kazakh Alphabet</title>
      <link>https://wafajohal.github.io/post/hri2020_accepted/</link>
      <pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/hri2020_accepted/</guid>
      <description></description>
    </item>
    
    <item>
      <title>UC San Diego, California, USA, November 2019</title>
      <link>https://wafajohal.github.io/talk/ucsandiego_hrimini2019/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/ucsandiego_hrimini2019/</guid>
      <description>&lt;p&gt;I gave a short talk at the HRI Mini symposium on Orchestration of Robot Populated Classrooms, a recent work started with &lt;a href=&#34;https://wafajohal.github.io/authors/sina-shahmoradi/&#34;&gt;Sina Shahmoradi&lt;/a&gt;, &lt;a href=&#34;https://wafajohal.github.io/authors/jauwairia-nasir/&#34;&gt;Jauwairia Nasir&lt;/a&gt; and &lt;a href=&#34;https://wafajohal.github.io/authors/utku-norman/&#34;&gt;Utku Norman&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vTnhl9vGI8PfwTmAdzq8uh9axoV-atvvN87Xrlt4_ctt5YNDIO-mZbe4O9McfIyGlaQ2Vierw7k9tXO/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Orchestration of robot-populated classrooms. UC Sand Diego, California, USA. November 2019.</title>
      <link>https://wafajohal.github.io/talk/2020_ucsd/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/2020_ucsd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Best Reviewer Award for ICCE</title>
      <link>https://wafajohal.github.io/post/best_reviewer_icce2019/</link>
      <pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/best_reviewer_icce2019/</guid>
      <description>&lt;p&gt;I am hounoured that the 27th International Conference on Computers in Education awarded me with the Best Reviewer Award.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://ilt.nutn.edu.tw/icce2019/04_Best%20Reviewer%20Award.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://ilt.nutn.edu.tw/icce2019/04_Best%20Reviewer%20Award.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4 Full Papers Presented @RoMan 2019, New Delhi!</title>
      <link>https://wafajohal.github.io/post/roman2019/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/roman2019/</guid>
      <description>&lt;p&gt;I was very glad to meet my colleagues and friends at RoMan.
We presented four papers and enjoyed a day off in Jaipur.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper accepted! Allohaptic</title>
      <link>https://wafajohal.github.io/post/allohaptic2020/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/allohaptic2020/</guid>
      <description>&lt;p&gt;I was very glad to meet my colleagues and friends virtually at RoMan2020.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Augmented Robotics for Learners: A Case Study on Optics</title>
      <link>https://wafajohal.github.io/publication/johal-augmented-2019/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-augmented-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The iReCheck project accepted!</title>
      <link>https://wafajohal.github.io/post/irechech-accepted/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/irechech-accepted/</guid>
      <description>&lt;p&gt;The iReCheck project submitted last April will be funded by the French ANR and the Swiss NSF.
The goal of the project is to explore the learning by teaching sceanrio proposed in the cowriter project in with both performance and social adaptation, and with TD learners and learners with NDD.
The project brings educational expertise of the EPFL team with the NDD expertise of the French partners to build a system that can suit all types of learners.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bridging multi-level time scales for HRI analysis</title>
      <link>https://wafajohal.github.io/post/framework-hri-analysis/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/framework-hri-analysis/</guid>
      <description>&lt;p&gt;You collected Gigabits of multi-modal data from an #HRI experiment? Check out our new paper on THRI proposing an analysis framework based on Newells&amp;rsquo; time scales. w T. Asselborn, K. Sharma &amp;amp;&lt;br&gt;
@dillenbo
&lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3338809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dl.acm.org/citation.cfm?id=3338809&lt;/a&gt; #HRI2020 #humanrobotinteraction&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First day at UNSW as Lecturer (Assistant Prof)</title>
      <link>https://wafajohal.github.io/post/unsw-start/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/unsw-start/</guid>
      <description>&lt;p&gt;Officially starting at  @UNSWEngineering today. Looking forward to this exciting adventure! Stay tuned, Phd job offers coming soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alex&#39;s Bachelor Design Presentation</title>
      <link>https://wafajohal.github.io/post/alex-defense/</link>
      <pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/alex-defense/</guid>
      <description>&lt;p&gt;Super excited about the presentation of my first design intern.
It&amp;rsquo;s been a fruitful experience to work with him this past few months.
Bravo Alex! #epfl #ecal&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>London/Brighton UK, One week workshop, Find you Voice project</title>
      <link>https://wafajohal.github.io/post/fyv-ukmeeting/</link>
      <pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/fyv-ukmeeting/</guid>
      <description>&lt;p&gt;The Find your Voice project (seed project, funded by the Jacobs Foundation and the SRCD) aims to investigate the potential use of voice assistant/robot to train children on joke telling to improve their self-confidence and language literacy.&lt;/p&gt;
&lt;p&gt;A whole week workshop ehlp us to clearly set objectives for the first part of teh project.
We conducted a small literature review and discussed the mini projects to be conducted by Mai 2020.&lt;/p&gt;
&lt;p&gt;The workshop started at UCLIC. Hosted by Prof. Yvonne Rogers, we visited the HCI facilities and the offices of the startup developping the Olly robot.&lt;/p&gt;
&lt;p&gt;After two days in London, we travelled to Brighton, on the sourthen coast of England, were we presented the FyV project to the members of the Chat Lab at Sussex Universty (lead by Prof. Nicola Yulli). The discussions and feedbacks were very interesting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expert for the Focus group on Robots in Education</title>
      <link>https://wafajohal.github.io/post/post_focustaswiss/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/post_focustaswiss/</guid>
      <description>&lt;p&gt;The TA Swiss (Swiss Technology Assessment Foundation) invited me to praticipate to a focus group about robotics as an expert in robots for education.
I was glad to present an overview of ther research of robotics in education.
The day ended with a very interesting panl in which many asptects such as ethics, and regulations were discussed.&lt;/p&gt;
&lt;p&gt;Some pictures of the workshop:
&lt;a href=&#34;https://www.ta-swiss.ch/en/projects/participative-ta/focus-robots/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ta-swiss.ch/en/projects/participative-ta/focus-robots/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The full report is available here  &lt;a href=&#34;https://www.ta-swiss.ch/Focus-Robots-Broschuere-fr-web.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ta-swiss.ch/Focus-Robots-Broschuere-fr-web.pdf&lt;/a&gt; (in french)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TA-Swiss, Bern, Switzerland Mai 2019</title>
      <link>https://wafajohal.github.io/talk/bern2019/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/bern2019/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vRdyAjthe6r9UgIUV1ig1vq9VZbIbOjUhSN5NqQrMmRp9sf6PqwX_NMmNesdaFDAgriEpn9z0D0IgVV/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Organising a Winter School on Developing Technologies in Educational Settings</title>
      <link>https://wafajohal.github.io/post/edtech-winterschool/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/edtech-winterschool/</guid>
      <description>&lt;p&gt;22 participants attended the 4 days training event at EPFL on Developing Technologies in Educational Settings.
The winter school recieved the support of the NCCR Robotcs and the Animatas H2020 EU project.
We had invited great speakers and got excellent feedback from the participants.
I specially wanted to thank Florence Colomb, Dr. Jennifer Olsen, Prof. Pierre Dillenourg, Dr. Catharine Oertel, Utku Norman, Jauwairia Nasir and Sina Shahmodari for helping me organising the event.&lt;/p&gt;
&lt;p&gt;The official page of the winter school is here:
&lt;a href=&#34;https://chili.epfl.ch/digital_learning_ws/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chili.epfl.ch/digital_learning_ws/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Applied Machine Learning Days, Lausanne Switzerland, January 2019</title>
      <link>https://wafajohal.github.io/talk/amld2018/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/amld2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Astana, Kazakhstan, January 2019</title>
      <link>https://wafajohal.github.io/talk/astana2019/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/astana2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>✈️ 🇰🇿  Astana! Visiting our partner for the CoWriting Kazakh project</title>
      <link>https://wafajohal.github.io/post/visit-cokaz/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/visit-cokaz/</guid>
      <description>&lt;p&gt;Thibault Assselborn and I travelled to Astana, Kazakhstan to install CoWriter with Anara.
We discussed learning sceanrios to help Kazakh students learn the new Latin Kazakh alphabet.&lt;/p&gt;
&lt;p&gt;It was definitly a fruitful visit, but the weather was not ideal for us to sightsee (-40°C).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bridging Multilevel Time Scales in HRI: An Analysis Framework</title>
      <link>https://wafajohal.github.io/publication/asselborn-2019-bridging/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/asselborn-2019-bridging/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CoWriting Kazakh: Transitioning to a New Latin Script using Social Robots</title>
      <link>https://wafajohal.github.io/publication/kim-cowriting-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/kim-cowriting-2019/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/CmRBnVcBluo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories?</title>
      <link>https://wafajohal.github.io/publication/ozgur-designing-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-designing-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning By Collaborative Teaching: An Engaging Multi-Party CoWriter Activity</title>
      <link>https://wafajohal.github.io/publication/el-hamamsy-learning-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/el-hamamsy-learning-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Symmetry with Tangible Robots</title>
      <link>https://wafajohal.github.io/publication/johal-2019-learning/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2019-learning/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vS7LTYFCBHDmQzuvNjSAjuOjZKcJchxHyecVMc4wkikeL7Z3ffiHfuzX9SQChHHBhLl_nmRUE5hLR3F/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Magnet-assisted ball drive</title>
      <link>https://wafajohal.github.io/publication/ozgur-2019-magnet/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2019-magnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Orchestration of Robotic Activities in Classrooms: Challenges and Opportunities</title>
      <link>https://wafajohal.github.io/publication/shahmoradi-2019-orchestration/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/shahmoradi-2019-orchestration/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reply: Limitations in the creation of an automatic diagnosis tool for dysgraphia</title>
      <link>https://wafajohal.github.io/publication/asselborn-2019-reply/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/asselborn-2019-reply/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robot Analytics: What Do Human-Robot Interaction Traces Tell Us About Learning?</title>
      <link>https://wafajohal.github.io/publication/nasir-2019-robot/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/nasir-2019-robot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robots for Learning-R4L: Adaptive Learning</title>
      <link>https://wafajohal.github.io/publication/johal-2019-robots/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2019-robots/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Studying the Effect of Robot Frustration on Children&#39;s Change of Perspective</title>
      <link>https://wafajohal.github.io/publication/yadollahi-studying-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/yadollahi-studying-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Dynamics of Handwriting Improves the Automated Diagnosis of Dysgraphia</title>
      <link>https://wafajohal.github.io/publication/zolna-2019-dynamics/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/zolna-2019-dynamics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration</title>
      <link>https://wafajohal.github.io/publication/johal-2019-tip/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2019-tip/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vSAbsZww1VUF4gDexQLD7Goez5ApAkyU85mnNXrNaPgeiHIzSs9Aeo1RNkZWp3WaAK-aT4gjm0zMDID/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Towards an Adaptive Upper Limb Rehabilitation Game with Tangible Robots</title>
      <link>https://wafajohal.github.io/publication/ozgur-2019-towards/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2019-towards/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ETH, Zurich, Switzerland, December 2018</title>
      <link>https://wafajohal.github.io/talk/ethz2018/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/ethz2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human-Robot Interaction Mini Symposium,  Standford, November 2018</title>
      <link>https://wafajohal.github.io/talk/hris2018/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/hris2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Standford, November 2018</title>
      <link>https://wafajohal.github.io/talk/standford2018/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/standford2018/</guid>
      <description>&lt;p&gt;Visiting Lukasc Kidnzinski at Stanford, I gave a talk on our paper on diagnosis of disgraphia using RF applied to a series of novel features based on pen&amp;rsquo;s postion, pressure and tilt.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talk at the HRI 2018 Mini Symposium, Standford CA</title>
      <link>https://wafajohal.github.io/post/talk-hris2018/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/talk-hris2018/</guid>
      <description>&lt;p&gt;Teaching children to write using robots with haptic feedback.
I gave a small talk on our IDC paper [?] using the Cellulo robots for a handwriting activity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teacher symposium on Computational Thinking, Yverdon-les-Bains, Switzerland, November 2018</title>
      <link>https://wafajohal.github.io/talk/entre2lac/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/entre2lac/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bold Blog Article on R4L is out</title>
      <link>https://wafajohal.github.io/post/writingbold/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/writingbold/</guid>
      <description>&lt;p&gt;Pierre Dillenbour and I wonder &amp;ldquo;What are robots doing in schools?&amp;rdquo; in a blog post on Bold : &lt;a href=&#34;https://bold.expert/what-are-robots-doing-in-schools/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bold.expert/what-are-robots-doing-in-schools/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demos at NCCR Robotics Industry Days</title>
      <link>https://wafajohal.github.io/post/nccr_id2019/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/nccr_id2019/</guid>
      <description>&lt;p&gt;The CHILI Lab presented several demo at the Swiss Converntion Center.
Always a great opportunity to meet with colleagues from the NCCR and robotics actor of the reagion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNSW, November 2018</title>
      <link>https://wafajohal.github.io/talk/unsw2018/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/unsw2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Workshop on Swarm Robotics, Rome, October 2018</title>
      <link>https://wafajohal.github.io/talk/rome2018/</link>
      <pubDate>Wed, 17 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/rome2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Outreach activities</title>
      <link>https://wafajohal.github.io/post/post_outreach/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/post_outreach/</guid>
      <description>&lt;p&gt;This last weekend, we had a booth at the Robotic Showcase of the EPFL Drone Days. In the summer, we received 80 young girls who were doing a coding camp on campus for demos at the CHILI Lab. I also participate to other outreach activities with the NCCR Robotics (Swiss Industry Days) or with the Service for Promoting Sciences – SPS at EPFL by being interviewed by small girls on my job as researcher in robotics.&lt;/p&gt;
&lt;p&gt;During these events we could present some learning activities that we developed for children and explain our research goals. This kind of events are very tiring, people come and go and are often impatient to test. It can occur during the weekend and we can often feel that we could be working on something more valuable for our research instead and it is a waste of time. But I do think that these events help us as researchers. First, it shows the face of scientists to the public, they can talk to us. As a scientist you also hear the concerns that people have with some of your work and it is interesting to try to reassure the public and take into account these ethical concerns. Robotics has had a bad image in the society these past decades, as the 4th industrial revolution announces that many people will loose their jobs. I personally work on assistive robotics and I want to show the public that robots could be beneficial for their life or for society and that we are researching for what applications they can be an added value. You also want to show the limits of AI and Robotics system. We sometimes make our work sound fantastic when writing papers (the limitation section being a small paragraph at the end of the paper), but when running experiments / demo in the wild, you see the limits of autonomous systems and you show the public that you still need a human to be around to interact with the robots. I also helps me to network. I often do user studies and need to find participants for it. I have presented a recruitment sheet during this kind of events and collected contact of people willing to participate to my next experiment. At least once during the showcase, you will meet this guy (more often a guy but could be a woman) that will want to debate with you. This guy is often affiliated to the field, he knows about cs and robotics, he is retired and thinks that we are now going too far. I usually take my time with these people, try to show and explain that we are doing to research and testing new things, that there is a lot of room for improvement, and that I appreciate his feedback. A lot of people think like him and it is interesting as a scientist and developer to build your argumentation for this kind of debate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Project CoWriting Kazakh Accepted!</title>
      <link>https://wafajohal.github.io/post/post_cokazakh/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/post_cokazakh/</guid>
      <description>&lt;p&gt;We got a new grant for travel mainly that will enable me to collaborate more with Anara Sandygulova from Nazarbayev University. As Kazakhstan is planning to move from Cyrillic to Roman alphabet, we are planning to use the Cowriter setup to motivate children to learn the new alphabet. This work will also continue our work on dysgraphia by collecting handwriting data samples from children in the two alphabet. The goal will be to discover features that are independent of the alphabet to characterize handwriting skills. Excited for my first visit to Kazakhstan. 🙂&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working on a Computational Thinking application</title>
      <link>https://wafajohal.github.io/post/compthining/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/compthining/</guid>
      <description>&lt;p&gt;Computational Thinking, Kezako? Computational Thinking such a buzz word these days. I got more and more interested by CT when I saw that there was so much debate on its definition and utility to be taught in school. I recently submitted a proposal to get funding for a project dealing with CT but I will not say much more now as it is being reviewed. In July, we received the visit of my good friend Anara from Kazakhstan, who stayed at the lab for a month; just enough to design an experiment and start the development. To get some inspiration we browsed and got some great ideas from the CSunplugged website : &lt;a href=&#34;https://csunplugged.org/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://csunplugged.org/en/&lt;/a&gt; We chose to work on Sorting Networks.&lt;/p&gt;
&lt;p&gt;If you are an CS engineer you are very familiar with the classical sorting algorithms such as Insertion Sort, Quick Sort and Bubble Sort (Obama’s favorite) but maybe less familiar with the Sorting Networks. Sorting is everywhere and you use sorting machines all the time, each time you do a Google search, as Google is sorting the search results for you to get the most relevant one on the top of the list. This sorting by relevance can be done in multiple ways. Sorting Networks is one of the ways that as the advantage of allowing for parallel processing of the sorting task. In sorting networks, the only operation applied is comparison. Each thread compares elements of the network two by two and tag them as the higher or lower element of the two. Sorting networks can hence be applied to any kind of element as long as they can be compared (numbers, letters, etc). Another advantage of Sorting Networks is that the number of operation (and hence the execution time of the sorting) can be calculated from the just knowing the number of elements to be sorted (the shape of the network is determined at that time also).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Honorable Mention Award at IDC2018!</title>
      <link>https://wafajohal.github.io/post/bpa_idc2018/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/bpa_idc2018/</guid>
      <description>&lt;p&gt;Awarded in #IDC2018 for our study on Deictic Gestures in a Reading Learning by Teaching scenario with a Nao. Checkout the paper &lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3202743&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dl.acm.org/citation.cfm?id=3202743&lt;/a&gt; . Bravo
@ElmiraYadollahi
Ana Paiva and
@dillenbo
#CHILI
@nccrrobotics&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated human-level diagnosis of dysgraphia using a consumer tablet</title>
      <link>https://wafajohal.github.io/publication/asselborn-2018-automated/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/asselborn-2018-automated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bringing letters to life: handwriting with haptic-enabled tangible robots</title>
      <link>https://wafajohal.github.io/publication/asselborn-2018-bringing/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/asselborn-2018-bringing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Declarative Physicomimetics for Tangible Swarm Application Development</title>
      <link>https://wafajohal.github.io/publication/ozgur-2018-declarative/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2018-declarative/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iterative design of an upper limb rehabilitation game with tangible robots</title>
      <link>https://wafajohal.github.io/publication/guneysu-2018-iterative/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/guneysu-2018-iterative/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robots for Learning-R4L: Inclusive Learning</title>
      <link>https://wafajohal.github.io/publication/johal-2018-robots/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2018-robots/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The near future of children&#39;s robotics</title>
      <link>https://wafajohal.github.io/publication/charisi-2018-near/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/charisi-2018-near/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Une architecture cognitive et affective oriente interaction</title>
      <link>https://wafajohal.github.io/publication/pellier-2018-architecture/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/pellier-2018-architecture/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When deictic gestures in a robot can harm child-robot collaboration</title>
      <link>https://wafajohal.github.io/publication/yadollahi-2018-deictic/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/yadollahi-2018-deictic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New CFP Robots for Inclusive Learning @HRI2018</title>
      <link>https://wafajohal.github.io/post/r4lhri2018cfp/</link>
      <pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/r4lhri2018cfp/</guid>
      <description>&lt;p&gt;The 4th Workshop on Robots for Learning will focus on Inclusive Learning trying to address how robots&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;could benefit to learners with special needs or disabilities,&lt;/li&gt;
&lt;li&gt;improve social well-being in learning&lt;/li&gt;
&lt;li&gt;enhance learners collaborations and creativity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Checkout the website for more information: &lt;a href=&#34;http://r4l.epfl.ch/HRI2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://r4l.epfl.ch/HRI2018&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Swiss NCCR Robotics Industry Days</title>
      <link>https://wafajohal.github.io/post/nccr-id2017/</link>
      <pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/nccr-id2017/</guid>
      <description>&lt;p&gt;We presented our robotics projects and had some demos for industrial and academics partners.
It is always the best place to network with great actors of robotics in Switzerland.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=zv6nDMQCWCo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.youtube.com/watch?v=zv6nDMQCWCo&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ERA.Net Plus RUS – Robotics Panelist</title>
      <link>https://wafajohal.github.io/post/eraplus/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/eraplus/</guid>
      <description>&lt;p&gt;It was a real adventure to travel to Moldavia for this panel meeting for which I was invited, as a HRI specialist, to review russo-european projects.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best Paper Award @HRI2017!</title>
      <link>https://wafajohal.github.io/post/bphri2017/</link>
      <pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/bphri2017/</guid>
      <description>&lt;p&gt;Our paper on the Cellulo project got the Best Paper Award in Vienna at the HRI2017!
We had another study paper and a demo paper at HRI this year with this same project. 🙂&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reviewing ...</title>
      <link>https://wafajohal.github.io/post/review/</link>
      <pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/review/</guid>
      <description>&lt;p&gt;Currently or done reveiwing papers  for CHI2017, HRI2017, CSCW2017, IJSORO, Transaction on Learning Technologies and the Journal of Child Computer Interaction&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cellulo: Versatile handheld robots for education</title>
      <link>https://wafajohal.github.io/publication/ozgur-2017-cellulo/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2017-cellulo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Expressing Motivations By Facilitating Other’s Inverse Reinforcement Learning</title>
      <link>https://wafajohal.github.io/publication/jacq-2017-expressing/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/jacq-2017-expressing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Haptic-enabled handheld mobile robots: Design and analysis</title>
      <link>https://wafajohal.github.io/publication/ozgur-2017-haptic/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2017-haptic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Keep on moving! Exploring anthropomorphic effects of motion during idle moments</title>
      <link>https://wafajohal.github.io/publication/asselborn-2017-keep/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/asselborn-2017-keep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Windfield: demonstrating wind meteorology with handheld haptic robots</title>
      <link>https://wafajohal.github.io/publication/ozgur-2017-windfielddemo/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2017-windfielddemo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Windfield: learning wind meteorology with handheld haptic robots</title>
      <link>https://wafajohal.github.io/publication/ozgur-2017-windfield/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2017-windfield/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Workshop on Robots for Learning: R4L</title>
      <link>https://wafajohal.github.io/publication/johal-2017-workshop/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2017-workshop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CfP Special Issue in JSORO on Robots for Learning</title>
      <link>https://wafajohal.github.io/post/cfpjsoro/</link>
      <pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/cfpjsoro/</guid>
      <description>&lt;p&gt;I am happy to announce that we are launching a special issue on Robots for Learning in the Springer International Journal of Social Robotics.
&lt;a href=&#34;http://r4l.epfl.ch/IJSORO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://r4l.epfl.ch/IJSORO&lt;/a&gt;
#R4L #robots4learning&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talk @Xerox R&amp;D, Grenoble</title>
      <link>https://wafajohal.github.io/post/xerox/</link>
      <pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/xerox/</guid>
      <description>&lt;p&gt;Today, I gave a seminar at Xerox R&amp;amp;D in a lovely manor near Grenoble.
Glad to see people interested to port their NLP and ML algorithms to robots.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Xerox Europe R-D Grenoble, France, December 2016</title>
      <link>https://wafajohal.github.io/talk/xerox2016/</link>
      <pubDate>Wed, 14 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/xerox2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interviewed by The Long and Short</title>
      <link>https://wafajohal.github.io/post/interviewlongshort/</link>
      <pubDate>Thu, 11 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/interviewlongshort/</guid>
      <description>&lt;p&gt;Interviewed on our Educational Robotics Projects at CHILI/EPFL:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://thelongandshort.org/machines/could-robots-teach-our-children&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://thelongandshort.org/machines/could-robots-teach-our-children&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Workshop on Social Robots in Education @NewFriends 2016</title>
      <link>https://wafajohal.github.io/post/wsnewfriends2016/</link>
      <pubDate>Thu, 11 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/wsnewfriends2016/</guid>
      <description>&lt;p&gt;I am co-organizing a workshop on Social Robots in Education. For more info : &lt;a href=&#34;https://socialrobotsineducation.wordpress.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://socialrobotsineducation.wordpress.com/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Animatas</title>
      <link>https://wafajohal.github.io/project/animatas/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/animatas/</guid>
      <description>&lt;p&gt;Animatas is a H2020 MCS International Training Network involving 8 insitutions from Europe and several international partners (amoong which UNSW).
The goal of the project is to investigate social learning and embodiement of autonomous agents in learning contexts.&lt;/p&gt;
&lt;p&gt;Three aspects are investigated at EPFL with some collaborators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Modelling Engagement in Collaborative Learning Scenario&lt;/strong&gt; - Jauwairia&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How can we build a group model of engagement? Can we quantify engagement with regard to learning and find the optimal engagement for the group of learners?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mutual Understanding in Learner-Robot Interaction&lt;/strong&gt; - Utku&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Can we build an agent able to detect mis-understanding? to repair them or create them (for the sake of learning)?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Orchestration of Robot Populated Classroom&lt;/strong&gt; - Sina&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to enable teachers to manage and monitor their classroom during robotics practical?
What are the advantages and drawbacks of robots in the classroom in terms of teacher&amp;rsquo;s orchestration?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Behavioural Analysis in HRI</title>
      <link>https://wafajohal.github.io/project/engagement/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/engagement/</guid>
      <description>&lt;p&gt;In the field of human-robot interaction (HRI) as in many other technical fields, an innumerable number of different
metrics to analyze the interaction can be found in as many studies. Even-though many of these metrics are not&lt;/p&gt;
&lt;p&gt;comparable between studies, we observe that the research community in HRI, but also in many other
research domain, is starting to seek for reproducibility [10]; a consensus begins to appear concerning
common measures that can be used across a wide range of studies. In social HRI, the evaluation of
the quality of an interaction is complex because it is very task dependent. However, certain metrics
such as engagement seem to well reflect the quality of interactions between a robot agent and the
human.&lt;/p&gt;
&lt;p&gt;One aspect of acceptability of a robot is the home environment in which it is to be able to perceive when it will be
solicited. The goal for the robot is to not disturb the user and to be able to predict that it will be solicited. This is
something we do all the time as humans (we can see a street vendor approaching us and we know they will talk
to us). The process for us human relies on proxemics (speed and angle of approach) but not only.&lt;/p&gt;
&lt;p&gt;In my work on modeling engagement [11, 12], I used multi-modal data to train a SVM to detect engagement. We&lt;/p&gt;
&lt;p&gt;collected data from various sensors embedded in the Kompai robot and reduced the number of crucial features
from 32 to 7. Interestingly shoulder orientation and position of the face in the image are among these crucial
features. If we transpose these features to what humans do, these features seem coherent with behavioral
analysis.
The previous model aimed to predict the engagement, but once the user is engaged, it is important to evaluate its
attitude and mood. Using the COST and HAART dataset, we trained a model to detect social touch. Together with
colleagues, we won the Social Touch Challenge at ICMI 2015 improving the gestures recognition from 60% accuracy
to 70% training a Random Forest Model. [13]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Behavioural Styles</title>
      <link>https://wafajohal.github.io/project/stylebot/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/stylebot/</guid>
      <description>&lt;p&gt;I have been developing a model of the so called behavioral styles. These styles act like a filter over
communicative gestures to transpose a way of performing an action. We used multiple platforms (Nao – humanoid,
and Reeti – facial expression) to test this rendering on facial and bodily communication [14]. We showed
that these styles were perceptible and could influence the attitude of the child interacting with the
robot[15, 16].
More recently, we showed that idle movements (movements that have no communicative intention) when
displayed by a humanoid robot increases the anthropomorphic perception of the robot by the user
[17].
These findings help in designing more natural interaction with humanoid robots, making them more acceptable
and socially intelligent.
Research Perspectives: We will be continuing research in this area working within the ANIMATAS EU project
(starting in January 2018) on synchrony and how alignment can keep learners engaged in a collaborative task with a
robot.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/YH4ywpgM1OU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Xg49gsWKMLY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Behavioural Styles</title>
      <link>https://wafajohal.github.io/project/under_grad_past_projects/curriculum_analysis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/under_grad_past_projects/curriculum_analysis/</guid>
      <description>&lt;p&gt;I have been developing a model of the so called behavioral styles. These styles act like a filter over
communicative gestures to transpose a way of performing an action. We used multiple platforms (Nao – humanoid,
and Reeti – facial expression) to test this rendering on facial and bodily communication [14]. We showed
that these styles were perceptible and could influence the attitude of the child interacting with the
robot[15, 16].
More recently, we showed that idle movements (movements that have no communicative intention) when
displayed by a humanoid robot increases the anthropomorphic perception of the robot by the user
[17].
These findings help in designing more natural interaction with humanoid robots, making them more acceptable
and socially intelligent.
Research Perspectives: We will be continuing research in this area working within the ANIMATAS EU project
(starting in January 2018) on synchrony and how alignment can keep learners engaged in a collaborative task with a
robot.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/YH4ywpgM1OU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Xg49gsWKMLY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cellulo</title>
      <link>https://wafajohal.github.io/project/cellulo/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/cellulo/</guid>
      <description>&lt;p&gt;With the Cellulo project, a part of the Educational Grand Challenge of the NCCR Robotics, we introduced a
new robotic platform which is small and easily deployable. A dotted pattern printed on regular paper enables the
Cellulo robots with absolute localization with a precision of 270 microns. The robots also have a new
locomotion system with a drive relying on a permanent magnet to actuate coated metal balls. This new drive
design allows backdrivability; i.e. it allows the robot to move and to be moved without damaging it. With this
system, we also implemented a haptic feedback modality, allowing the user to feel forces when grasping the robot.
The robots are connected via Bluetooth to a master (PC or tablet) that handles the logical and computation of the
activity. The onboard PCB of the robots only allows for proceeding the localization (image capture and decoding of
the pattern) and the control of the three wheels actuation.
During four years, we developed several learning activities using the robots. In the Feel
the Wind activity, the learners were taught that the wind was formed by air moving from with high to low
pressure points.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/zv6nDMQCWCo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In the Cellulo project, we  started to explore the use of haptic feedback for learners. Haptic feedback enables us to
render forces, but also borders, angles, or points. We developed a series of haptic capabilities and small interaction
tasks that can be included in learning activities to inform the learner. We tested the haptic feedback with
children for instance in the symmetry activity, in which the child is able to formulate hypothesis on the placement
of the symmetrical shape and to verify their claims by feel haptically the shape on paper. We also tested
with some pilot with visually impaired children who were able to explore a map of their classroom using the Cellulo
robots.
Research Perspectives for Tangible Swarm Interaction and Haptic for Learners: We are now exploring the
dynamics of the group of learners in manipulating the robots. The collaboration among learner is not always
optimal, and a challenge would be to use the swarm robots to analyses and regulate the collaboration among
learners. As these shared resource can be intelligent agents, they could rearrange themselves according to the
collaborative state of the group.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/g_7glQmTIVo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/s3KAoQNUPZs&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we have been exploring the use of Cellulo robots for upper arm-rehabitation with the goal to develop a kit for occupational theraputic games.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/f-7lrzXFtr0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>CoWriter</title>
      <link>https://wafajohal.github.io/project/cowriter/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/cowriter/</guid>
      <description>&lt;p&gt;In the CoWriter Project, we explore how the learning by teaching approach can be employed to motivate children to practice their handwriting skills with a small humanoid robot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CoWriting Kazakh</title>
      <link>https://wafajohal.github.io/project/cowriting_kazakh/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/cowriting_kazakh/</guid>
      <description>&lt;h1 id=&#34;project-summary&#34;&gt;Project Summary&lt;/h1&gt;
&lt;p&gt;This research occurred in a special context where Kazakhstan’s recent decision to switch from Cyrillic to the Latin-based alphabet has resulted in challenges connected with teaching literacy, addressing a rare combination of research hypotheses and technical objectives about language learning. Teachers are not necessarily trained to teach the new alphabet, and this could result in a harder challenge for children with difficulties. Prior research studies in Human-Robot Interaction (HRI) have proposed the use of a robot to teach handwriting to children. Drawing on the Kazakhstani case, our study takes an interdisciplinary approach by bringing together smart solutions from robotics, computer vision areas and educational frameworks, language and cognitive studies that will benefit diverse groups of stakeholders.&lt;/p&gt;
&lt;p&gt;Relatively little is known about the transfer of writing skills across scripts and it is extremely difficult to isolate the role of motor skills in this transfer, as compared to other cognitive and linguistic skills. Since handwriting combines visual, language, and motor dimensions, neuroimaging does not provide a clear account of cerebral substrates of handwriting.  A recent change of policy in Kazakhstan gave us an opportunity to measure transfer, as the Latin-based Kazakh alphabet has not yet been introduced. With the aim to understand if the script change of the Kazakh language from Cyrillic to Latin would have an effect on handwriting performance of young children and potentially increase the number of children with learning disabilities such as dyslexia and dysgraphia, we investigated the transfer of handwriting skills (from Cyrillic to Latin) in primary school children. To this end, we conducted an analysis of children’s handwriting dynamic data in both scripts (acquired from 200 children aged 6-11 years old who were asked to write a short text on a digital tablet using its stylus) in order to identify if the number of years spent practicing Cyrillic has an effect on the quality of handwriting in the Latin alphabet. The results showed that some of the differences between the two scripts were constant across all grades. These differences thus reflect the intrinsic differences in the handwriting dynamics between the two alphabets. For instance, several features related to the pen pressure on the tablet are quite different.  Other features, however, revealed decreasing differences between the two scripts across grades. While we found that the quality of Cyrillic writing increased from grades 1 to 4, due to increased practice, we also found that the quality of the Latin writing increased as well, despite the fact that all of the pupils had the same absence of experience in writing in Latin. We can therefore interpret this improvement in Latin script as an indicator of the transfer of fine motor control skills from Cyrillic to Latin. This result is especially surprising given that one could instead hypothesize a negative transfer, i.e., that the finger controls automated for one alphabet would interfere with those required by the other alphabet.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We proposed the first dataset for handwriting recognition of Cyrillic-based languages such as Kazakh and Russian, which is appropriate for the use by machine learning approaches. It contains 121,234 samples of 42 Cyrillic letters. The performance of Cyrillic-MNIST is evaluated using standard deep learning approaches and is compared to the Extended MNIST (EMNIST) dataset. The dataset is available at &lt;a href=&#34;https://github.com/bolattleubayev/cmnist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/bolattleubayev/cmnist&lt;/a&gt;. This work is under review in Frontiers in Big Data journal. It was conducted in Kazakhstan.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We implemented an autonomous social robot that would assist and motivate children in transition from the old Cyrillic alphabet to a new Latin alphabet. The hardware components of the system include a humanoid NAO robot and a tablet with a stylus. The software components of the CoWriting Kazakh system include: a) handwriting recognition of Cyrillic languages trained on over 120,000 samples of the Cyrillic-MNIST dataset; b) Learning by Demonstration (LbD) in robot font to dynamically adapt to each child&amp;rsquo;s individual handwriting style where trajectories of human handwriting are collected in real time; c) Latin-to-Latin Learning by Teaching (LbT): a humanoid robot plays a unique social role and asks for help to learn Kazakh language but since it cannot read Cyrillic script, a child becomes robot&amp;rsquo;s teacher who is committed to try her best to write the words in Latin so that the robot can read and learn Kazakh. Such an approach allows us to keep a child engaged in the learning process that we utilized in the experiment with 67 children. The system was submitted as a demonstration paper titled “CoWriting Kazakh: Learning a new script with a robot - Demonstration”. This demonstration got the “Best Demonstration” award at the prestigious HRI 2020 conference. This work was conducted in Kazakhstan.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Find Your Voice</title>
      <link>https://wafajohal.github.io/project/fyv/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/fyv/</guid>
      <description>&lt;p&gt;Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, not speak the native language as their first language, be new to a culture (migrants), or have a special need involving communication impairments (e.g., autism, speech, and hearing) [1]. Their voices often go unheard as they find it hard to contribute to conversations and group work, in both face to face and online settings, which puts them at risk for poor socio-emotional and achievement outcomes.&lt;br&gt;
Various interventions have been proposed in an effort to help these children integrate better into social settings, including ice-breaking activities, support groups, [2][3]. Another promising alternative is to teach children how to interpret and use humor effectively to facilitate social interaction [4]. Having a sense of humour might help children to reduce stress [5] and ease social interactions, enabling them to overcome their social inhibitions and make friends more easily. Also, humour could promotes children’s language, literacy, and inference skills, since understanding jokes require a mastery of language ambiguity, and comprehension [6].
One manner to use humor effectively is by ‘joke telling’. A joke is the shortest clearly-structured conversation one can have with one or more peers. Telling jokes can help children improve their confidence, socialization, [4], literacy [8], and cultural learning [7]. For example, a child could start a ‘joke’ that implies a question. He/she knows the answer, but not his/her peers. Then, he/she has the advantage of knowing what the answer is. This helps him/her to gain confidence. In terms of literacy, telling a joke requires the reinterpretation of the syntax underlying the question, then, understanding the ambiguity in a joke might help them in the comprehension of language [8]. Regarding socialization, this dynamic allows children to take part in role-taking, and understanding what other people needs to know to get it. Also, it can create a positive environment where children can laugh together and share their understanding. In terms of learning, a jokes encapsulate cultural assumptions, for example, differences between pronunciation, a dual meaning of a word, and the structural and ambiguity of sentences [7].  The question this raises is: how can we help children from all walks of life learn the art of telling jokes and importantly feel comfortable, and in doing so, increase their confidence, social skills and even literacy?
To help children develop the ability to tell jokes with their peers, we propose leveraging a new interactive technology – voice assistant robots (e.g., Alexa), which have become increasingly affordable and commonplace in homes. These robots, which are portable and have lifelike voices, can model how to tell a joke and using voice recognition software, respond to the jokes that children tell. Children thus have unlimited opportunities for scaffolded practice and can receive timely feedback in a non-threatening environment before telling their jokes to family and friends. In addition to the accessibility and human-like verbal responses of voice assistant robots, robots can be customized to tell jokes that are appropriate for each child’s age, language, and culture. The goal of our project therefore is to enable children to improve their ‘joke telling’ skills by learning the intonation, timing, through interacting with a virtual assistants such as Alexa.
Our research project will explore the feasibility of coding Alexa and other voice assistants to be able to respond and engage with children telling a diversity of jokes. Several aspects need to be taken into account when delivering a joke that children can imitate and practice with. We will look into how the timing and intonation will need to be evaluated from the audio recording in order to provide an adaptive feedback to the child training to deliver jokes. This feedback could be from both sides, the voice assistant robot helping the child in how telling jokes, or the child helping the assistant to improve how telling jokes. The final goal is, once the child has learnt how to tell jokes, to understand when and how will he/she be able to share the joke with their peers, and how long might this process take and how would we be able to tell they have become more confident.
Finally, an engaging joke-telling learning scenario will need to be designed in order to train children for a long-term impact. Our project will also consider how this might materialize.  In the future, virtual assistants will be able to be designed to give encouraging feedback to the child as they interact with it. This could be verbal, visual or both. This feedback could encourage children for finding their voice. Based on the initial findings from the pilot project  we would like to develop a more extensive project proposal with the goal of investigating how to develop new interventions using the next generation of interactive robots, such as Olly, whose developers, we are also collaborating with (&lt;a href=&#34;https://heyolly.com/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://heyolly.com/)&lt;/a&gt;, that have more scope for providing customized feedback can be used.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robots for Learning (R4L)</title>
      <link>https://wafajohal.github.io/project/robots4learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/robots4learning/</guid>
      <description>&lt;p&gt;With the increasing number of engineering jobs, politics have more recently turned towards introduction of
engineering subjects in early stages of curricula. More and more countries have started to introduce programming
(and even robotics) to young children. However, this constitutes a real challenge for the teaching
professionals, who are not trained for that, and are often skeptical to use new technologies in classrooms.
Hence, the challenge is to introduce robots as tools to be used not only in programming or robotics
based courses, but in the whole curricula in order to make them attractive to teachers as a pedagogical
tool.
”Educational robotics” does not really constitute a research community per say. On one hand, there are scholars
working on ”how to teach robotics” using robotics platforms such as Thymio and Mindstorms. Some scholars
perform research in this domain (for example, measuring which learning activities produce faster learning), but
their numbers are scarce, they typically meet in half-day workshops before ed’tech conferences (CSCL, AI&amp;amp;Ed, ….).
On the other hand, one finds scholars who mainly do research on HRI and consider education as an interesting
place for testing child-robot interactions.
I launched in 2016 a series of workshops to build a Robots for Learning (R4L) research community. The first event
was a workshop, along with the RoMan 2016 conference. The event gathered 30 participants from all around the
world. The second event was a workshop along the HRI 2017 conference in Vienna, Austria, with 60 participants
and 10 presentations. This fall, we hosted the first stand alone event, in Switzerland, for which we invited main
actors of research in HRI for Learning. A new workshop is planned for HRI 2018. These workshops aim to
mix scientists from field of robotics with those from digital education and learning technologies (See
&lt;a href=&#34;https://robot4learning.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://robot4learning.github.io/&lt;/a&gt; ).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visitors and PostDoc</title>
      <link>https://wafajohal.github.io/prospective/visitors/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/visitors/</guid>
      <description>&lt;p&gt;Grants for postdocs&lt;/p&gt;
&lt;p&gt;Grans for visitors:
french
unsw
others&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cellulo @ the Robots Zoo, Cambridge Science Festival</title>
      <link>https://wafajohal.github.io/post/cellulocsf2016/</link>
      <pubDate>Sat, 16 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/cellulocsf2016/</guid>
      <description>&lt;p&gt;We demonstrated an activity on wind with Cellulo robots at the Cambridge Science Festival.
About 2000 people came to our stand. This demonstration was prepared by all the Cellulo project members. We were kindly invited by Swissnex Boston.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/g_7glQmTIVo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://youtu.be/g_7glQmTIVo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/s3KAoQNUPZs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://youtu.be/s3KAoQNUPZs&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>French Translation of Bartneck’s Godspeed Questionnaire</title>
      <link>https://wafajohal.github.io/post/frenchgodspeed/</link>
      <pubDate>Sat, 16 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/frenchgodspeed/</guid>
      <description>&lt;p&gt;I recently contributed to the translation in French of the godspeed questionnaire.&lt;/p&gt;
&lt;p&gt;This questionnaire aims to measure user’s perception of robots.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Social Touch Grand Challenge @ICMI 2015</title>
      <link>https://wafajohal.github.io/post/icmi2015/</link>
      <pubDate>Sat, 16 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/icmi2015/</guid>
      <description>&lt;p&gt;My team and I won the Social Touch Grand Challenge of ICMI 2015.&lt;/p&gt;
&lt;p&gt;I presented our results at the Grand Challenge session. We had the best scores for recognition of social gestures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visit the Personal Robots Group at MIT Media Lab</title>
      <link>https://wafajohal.github.io/post/visitmitprg2016/</link>
      <pubDate>Sat, 16 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/visitmitprg2016/</guid>
      <description>&lt;p&gt;I gave a talk and discussed with some members of the Personal Robots Group. I also got to see Tega, Leonardo and Nexi for real. 🙂&lt;/p&gt;
&lt;p&gt;Unfortunately, no Jibo around yet.&lt;/p&gt;
&lt;p&gt;We also took a tour in the MIT Media Lab building.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MIT Media Lab, Personal Robotics Group, April 2016</title>
      <link>https://wafajohal.github.io/talk/mit2016/</link>
      <pubDate>Fri, 15 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/mit2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Swissnex Boston, April 2016</title>
      <link>https://wafajohal.github.io/talk/swissnex2016/</link>
      <pubDate>Thu, 14 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/talk/swissnex2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Child-robot spatial arrangement in a learning by teaching activity</title>
      <link>https://wafajohal.github.io/publication/johal-2016-child/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2016-child/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vSqJ_JQtZQ8C9jo5IomgDuCi7wAK4d8asJYW3UnFRr4fmh_uOrXdX5rfc949x8fqaruRFhLYdePjLfG/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cognitive architecture for mutual modelling</title>
      <link>https://wafajohal.github.io/publication/jacq-2016-cognitive/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/jacq-2016-cognitive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Permanent magnet-assisted omnidirectional ball drive</title>
      <link>https://wafajohal.github.io/publication/ozgur-2016-permanent/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2016-permanent/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R2T2: Robotics to integrate educational efforts in South Africa and Europe</title>
      <link>https://wafajohal.github.io/publication/mondada-2016-r-2-t-2/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/mondada-2016-r-2-t-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Social human-robot interaction: A new cognitive and affective interaction-oriented architecture</title>
      <link>https://wafajohal.github.io/publication/adam-2016-social/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/adam-2016-social/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Starting engagement detection towards a companion robot using multimodal features</title>
      <link>https://wafajohal.github.io/publication/vaufreydaz-2016-starting/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/vaufreydaz-2016-starting/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A cognitive and affective architecture for social human-robot interaction</title>
      <link>https://wafajohal.github.io/publication/johal-2015-cognitive/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2015-cognitive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Companion Robots Behaving with Style: Towards Plasticity in Social Human-Robot Interaction</title>
      <link>https://wafajohal.github.io/publication/johal-2015-companion/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2015-companion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Non-verbal Signals in HRI: Interference in Human Perception</title>
      <link>https://wafajohal.github.io/publication/johal-2015-non/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2015-non/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robots Interacting with Style</title>
      <link>https://wafajohal.github.io/publication/johal-2015-robots/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2015-robots/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Grenoble system for the social touch challenge at ICMI 2015</title>
      <link>https://wafajohal.github.io/publication/ta-2015-grenoble/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ta-2015-grenoble/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Demo at Eclipse IOT Day 2014, Grenoble</title>
      <link>https://wafajohal.github.io/post/eclipseiot2014/</link>
      <pubDate>Wed, 16 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/post/eclipseiot2014/</guid>
      <description>&lt;p&gt;Demo using iPOPO, OpenHab via MQTT with Nao as natural remote control of the smart home sensors and actuators. (in collaboration with Shadi Abras, Thomas Calmant and Amr Alyafi).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/4vBSJ7csp8g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://youtu.be/4vBSJ7csp8g&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Robot with Style, because you are Worth it!</title>
      <link>https://wafajohal.github.io/publication/johal-2014-robot/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2014-robot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Acceptability of a companion robot for children in daily life situations</title>
      <link>https://wafajohal.github.io/publication/johal-2014-acceptability/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2014-acceptability/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Des Styles pour une Personnalisation de l&#39;Interaction Homme-Robot</title>
      <link>https://wafajohal.github.io/publication/johal-2014-styles/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2014-styles/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Expressing Parenting Styles with Companion Robots</title>
      <link>https://wafajohal.github.io/publication/johal-2014-expressing/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2014-expressing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards companion robots behaving with style</title>
      <link>https://wafajohal.github.io/publication/johal-2014-towards/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2014-towards/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Acceptabilité d&#39;un robot compagnon dans des situations de la vie quotidienne.</title>
      <link>https://wafajohal.github.io/publication/adam-2013-acceptabilite/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/adam-2013-acceptabilite/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling interactions in a mixed agent world.</title>
      <link>https://wafajohal.github.io/publication/johal-2013-modelling/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/johal-2013-modelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Detection of non-verbal communication cues using multi-modal sensors : engagement detection </title>
      <link>https://wafajohal.github.io/publication/benkaouar-2012/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/benkaouar-2012/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-sensors engagement detection with a robot companion in a home environment</title>
      <link>https://wafajohal.github.io/publication/benkaouar-2012-multi/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/benkaouar-2012-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://wafajohal.github.io/home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/home/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://wafajohal.github.io/publication/bruno-2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/bruno-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://wafajohal.github.io/publication/ozgur-2022/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/ozgur-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://wafajohal.github.io/publication/tozadore-2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/publication/tozadore-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CAIO</title>
      <link>https://wafajohal.github.io/project/caio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/project/caio/</guid>
      <description>&lt;p&gt;One of my interests is to make robots able to autonomously sustain interactions with users, and in order to do
so, they have to be able to reason about the users’ and their environment. During my PhD, I have
been working on a Cognitive Architecture able to reason on Emotion, named CAIO (Cognitive and
Affective Interaction-Oriented) Architecture [3, 4]. This architecture, based on symbolic reasoning,
showed promising results in modeling cognitive processes and specifically allowing decision making
based on emotions. As shown in the figure, this architecture works as a two loops process, similar to the Dual-Process Theory - a deliberative loop generating intentions and a sensorimotor loop handling
reflexes.&lt;/p&gt;
&lt;p&gt;More recently, we have been working on second order reasoning in the context of the
CoWriter project [5]. In the CoWriter project, the child’s teaches a Nao robot how to write. We use the
learning by teaching paradigm to enhance motivation and engagement. In a collaborative learning task
between a robot and a child, the idea is to model the child’s understanding and the child’s believes of the
understanding of the co-learner robot. This way the robot could detect misunderstandings in view to
correct them; or the robot could even create misunderstandings to enhance learning (by fostering
questioning).
Since my arrival on the CoWriter project, we initiated a project on diagnosis of dysgraphia using data collected
via a graphic tablet (Wacom). Our first results using RNN are very promising (a patent and a journal paper have
been submitted). This work will later on be integrated in the CoWriter handwriting activities to adapt the learning
path according to the diagnosis and the learner’s handwriting difficulties.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Find Your Voice: Use of Voice Assistant for Learning</title>
      <link>https://wafajohal.github.io/prospective/undergrad/fyv_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/fyv_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/student-projects/fyv2020.png&#34; alt=&#34;FYV&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development  (e.g., autism spectrum disorder, speech or hearing disorders).&lt;/p&gt;
&lt;p&gt;The voices of these children often go unheard, as they find it hard to contribute to a conversation.&lt;/p&gt;
&lt;p&gt;The Find your Voice (FyV, &lt;a href=&#34;http://wafa.johal.org/project/fyv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://wafa.johal.org/project/fyv/&lt;/a&gt;) project was initiated to investigate how joke telling could help children to speak up and gain confidence. We are also interested in story telling and general conversation. Improvements in communication can have a significant impact in confidence, and  help children:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduce stress&lt;/li&gt;
&lt;li&gt;improve self-confidence&lt;/li&gt;
&lt;li&gt;ease social interactions&lt;/li&gt;
&lt;li&gt;make friends more easily&lt;/li&gt;
&lt;li&gt;improving literacy and language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To help children develop the ability to communicate, tell jokes or stories to their peers, we propose leveraging social robots (e.g. NAO) and voice assistants (e.g., Alexa, Olly and Google Home) to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model how to tell jokes/stories and respond to other children during conversations .&lt;/li&gt;
&lt;li&gt;Practice joke/story telling with a ‘friendly’ and ‘non-judgmental’ audience.&lt;/li&gt;
&lt;li&gt;Practice turn taking during conversation.&lt;/li&gt;
&lt;li&gt;TLearn jokes, stories and interesting facts to tell other children.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The overall goals of the project are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To enable children to improve their social communication skills by learning intonation and timing, through interacting with voice assistants&lt;/li&gt;
&lt;li&gt;To learn to how to perform in front of peers and family&lt;/li&gt;
&lt;li&gt;To make children more confident in social situations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The FyV project involves partners in London and California.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;At UNSW, our main goal will be to develop a ‘Learning by Teaching’ application using a robot or voice assistant. This application will allow the user to teach a virtual agent (robot or voice assistant) a joke/story. As the agent learns by demonstration, the user can practice and refine how the story/joke is told until the voice assistant (and the child) is able to tell the joke/story in a satisfactory way.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design the Learning Scenario&lt;/li&gt;
&lt;li&gt;Explore TTS software for speech conversion&lt;/li&gt;
&lt;li&gt;Implement a new Alexa Skill&lt;/li&gt;
&lt;li&gt;Run a Pilot demonstrating the learning of joke/story telling features (e.g. pauses and intonations)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Voice Assistant, Machine Learning, HCI&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python or C++. Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/soobinseo/Transformer-TTS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/soobinseo/Transformer-TTS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/barronalex/Tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/barronalex/Tacotron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.amazon.com/en-US/alexa/alexa-skills-kit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://developer.amazon.com/en-US/alexa/alexa-skills-kit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Human Action Recognition from AD Movies</title>
      <link>https://wafajohal.github.io/prospective/undergrad/human_action_recognition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/human_action_recognition/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Action Recognition is curcial for robots to perfoma around humans.
Indeed, robot need to asses human action and intentions in order to assist them in everyday life tasks and to collaborative efficiently.&lt;/p&gt;
&lt;p&gt;The field of action recognition has  aimed to use typical sensors found on robots to recognize agnts, objects and actions they are performing.
Typical approach is to record a dataset of various action and label them. But often theses actions are not natural and it can be difficult to represent the variety of ways to perform actions with a lab built dataset.
In this project we propose to use audio desription movies to label actions.
AD movies integrate a form of narration to allow virually impaired veiwers to undertsnad the visual element sowed on screen.
This information often deals with action actually depicted on the scene.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop a pipeline to collect and crop clip of AD movies for at home actions.
This extraction tool should be fexible and allow for integration of next action. It will for instance feature video and text processing to extract [Subject+ Action + Object] type of data.&lt;/li&gt;
&lt;li&gt;Investigate methods for HAR&lt;/li&gt;
&lt;li&gt;Implement a tree model combaning HAR with YOLO to identify agent and objects&lt;/li&gt;
&lt;li&gt;Evaluate the HAR pipeline with the Toyota Robot HSR&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Human Action Recognition,&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://prior.allenai.org/projects/charades&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://prior.allenai.org/projects/charades&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.02696.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1708.02696.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.11230.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1806.11230.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>I Spy - An AR Game with a Robot in Home Environment</title>
      <link>https://wafajohal.github.io/prospective/undergrad/ispy_hololens_2021/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/ispy_hololens_2021/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;In the context of the (HURL project)[https://wafa.johal.org/project/hurl/], we investigate various ways to enable novice users to train robots to perform everyday life tasks. In particular, we employ gamification and Augmented Reality (AR) to make the training more engaging for the human trainer.&lt;/p&gt;
&lt;p&gt;In this project, your goal will be to implement a game &amp;ldquo;I spy&amp;rdquo; to be played with a robot in a home environment. For this game, we would like to use AR with the Hololens 2&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn about Hololens and Augmented Reality development&lt;/li&gt;
&lt;li&gt;Develop a game such as &amp;ldquo;I spy&amp;rdquo; using object recognition models (such as YOLO) for both the human and robot to play&lt;/li&gt;
&lt;li&gt;Evaluate the game in the robotics lab at UNSW&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Augmented Reality, Object Recognition, Machine Learning, Human-Robot Interaction&lt;/p&gt;
&lt;h2 id=&#34;prerequisites-or-willing-to-learn&#34;&gt;Prerequisites or willing to learn&lt;/h2&gt;
&lt;p&gt;Skills: C#, Python, Git
D or above in a machine learning or AI course&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/AlturosDestinations/Alturos.Yolo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/AlturosDestinations/Alturos.Yolo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HURL project: &lt;a href=&#34;https://wafa.johal.org/project/hurl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wafa.johal.org/project/hurl/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/doughtmw/YoloDetectionHoloLens-Unity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/doughtmw/YoloDetectionHoloLens-Unity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Social Interaction Modelling</title>
      <link>https://wafajohal.github.io/prospective/undergrad/hri-dataset-mine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/hri-dataset-mine/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;The field of social human-robot interaction is growing.
Understanding how communication between humans (human-human) generalises during human-robot communication is crucial in building fluent and enjoyable interactions with social robots.
Everyday, more datasets that feature social interaction between humans and between humans and robots are made freely available online.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d3i71xaburhd42.cloudfront.net/37c0bc28388902869d904b002fe789083b610ee1/4-Figure1-1.png&#34; alt=&#34;MHHRI&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this project we propose to take a data-driven approach to build predictive models of social interactions between humans (HH) ( and between humans and robots (HR) interaction using 3 different datasets. Relevant research questions include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which multi-modal features can be transferable from HH to HR setups?&lt;/li&gt;
&lt;li&gt;Are there common features that discriminate human behaviour in HH or HR scenarios (e.g. &amp;lsquo;Do people speak less or slower with robots?&amp;rsquo; &amp;hellip; )&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore datasets (PinSoRo, MHHRI and P2PSTORY): type of data (video; audio, point cloud), available labels and annotation &amp;hellip;&lt;/li&gt;
&lt;li&gt;Extract relevant features multimodal on each dataset&lt;/li&gt;
&lt;li&gt;Evaluate predictive models for each dataset (i.e. engagement)&lt;/li&gt;
&lt;li&gt;Explore transfer learning from one dataset to another&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also potential to use UNSW’s National Facility for Human-Robot Interaction Research to create a new dataset.&lt;/p&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Machine Learning, Human-Robot Interaction&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.media.mit.edu/projects/p2pstory/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.media.mit.edu/projects/p2pstory/overview/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Persuasive Robots - Exploring Behavioural Styles</title>
      <link>https://wafajohal.github.io/prospective/undergrad/behavioural-styles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/behavioural-styles/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few).
Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user’s knowledge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/student-projects/styles.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;Behavioural styles  allow  robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture.
Behavioural styles have been studied in the past to improve robot&amp;rsquo;s behaviour during human-robot interaction [2].&lt;/p&gt;
&lt;p&gt;In this project, we will explore how behavioural styles can influence engagement, trust and persuasion during human-robot interaction.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implement behavioural styles for the Nao robot (voice and behaviour) and for a voice assistant (voice only)&lt;/li&gt;
&lt;li&gt;Design at least two behaviour styles based on human behaviour and personality styles&lt;/li&gt;
&lt;li&gt;Evaluate and compare these styles via experimentation&lt;/li&gt;
&lt;li&gt;Design a scenario similar to the one described in paper [3]&lt;/li&gt;
&lt;li&gt;Setup a data collection environment (posture, video and audio) in the HRI Lab facility of UNSW&lt;/li&gt;
&lt;li&gt;Select appropriate tasks and/or questionnaires to measure engagement, trust and/or persuasion&lt;/li&gt;
&lt;li&gt;Evaluate the system via an experiment with users&lt;/li&gt;
&lt;li&gt;Complete the data analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Robotics, HRI, Psychology&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, ROS and Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2019/10/14081257/Robots_social_impact_eng.pdf&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.279&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johal, W., Pesty, S., &amp;amp; Calvary, G. (2014, August). Towards companion robots behaving with style. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1063-1068). IEEE.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] Bainbridge, W. A., Hart, J. W., Kim, E. S., &amp;amp; Scassellati, B. (2011). The benefits
of interactions with physically present robots over video-displayed agents.
International Journal of Social Robotics, 3(1), 41-52.&lt;/li&gt;
&lt;li&gt;[4] Peters, R., Broekens, J., Li, K., &amp;amp; Neerincx, M. A. (2019, July). Robot Dominance Expression Through Parameter-based Behaviour Modulation. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp. 224-226). ACM.&lt;/li&gt;
&lt;li&gt;[5] Shane Saunderson et al. It Would Make Me Happy if You Used My Guess: Comparing Robot Persuasive Strategies in Social Human–Robot Interaction, IEEE Robotics and Automation Letters (2019). DOI: 10.1109/LRA.2019.2897143&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PhD Positions</title>
      <link>https://wafajohal.github.io/prospective/phd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/phd/</guid>
      <description>&lt;p&gt;Checkout the current offering:
&lt;a href=&#34;https://wafajohal.github.io/files/PhD_Offering_UniMelb_S12022.pdf&#34; target=&#34;_blank&#34;&gt; PDF &lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robot Writing</title>
      <link>https://wafajohal.github.io/prospective/undergrad/writing_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/writing_2020/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school.&lt;/p&gt;
&lt;p&gt;In this project we propose to explore new methods for a robot to write using a pen. The project will use the Fetch Robot and aim to evaluate how a robot could learn handwriting trajectories using human demonstrations.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn about ROS&lt;/li&gt;
&lt;li&gt;Develop a ROS package that enables to control the Fetch Robot arm given a series of handwriting strokes&lt;/li&gt;
&lt;li&gt;Explore the use of different methods for the robot to learn letter writing from demonstrations&lt;/li&gt;
&lt;li&gt;Evaluate the implemented method compare to other state of the art methods&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;ROS, Learning by Demonstration, Robotics, Handwriting&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++,  ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;See Zotero Collection &lt;a href=&#34;https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tangible e-Ink Paper Interfaces for Learners</title>
      <link>https://wafajohal.github.io/prospective/undergrad/tip_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/tip_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/student-projects/tip.png&#34; alt=&#34;FYV&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;While digital tools are more and more used in classrooms, teachers&amp;rsquo; common practice remains to use photocopied paper documents to share and collect learning exercises from their students.
With the Tangible e-Ink Paper (TIP) system, we aim to explore the use of tangible manipulatives interacting with paper sheets as a bridge between digital and paper traces of learning.
Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs are envisioned to be used as a versatile tool across various curriculum activities.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Literature Review on Tangible UI (TUI) in Education&lt;/li&gt;
&lt;li&gt;Implement and test a proof of concept of TIPs for learning&lt;/li&gt;
&lt;li&gt;Assemble 3 TIPs (3D printing of parts, soldering, etc.)&lt;/li&gt;
&lt;li&gt;Install libraries on Rasberry PI ( e.g. libdots - used for self-paper-based localisation, bluetooth communication)&lt;/li&gt;
&lt;li&gt;Develop two demo applications using TIPs for
&lt;ul&gt;
&lt;li&gt;individual work (on A4 sheet of paper)&lt;/li&gt;
&lt;li&gt;collaborative work (on min A2)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Tangible User Interfaces, HCI&lt;/p&gt;
&lt;h2 id=&#34;prerequisites-and-learning-outcomes&#34;&gt;Prerequisites and Learning Outcomes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Javascript, Python or C++. Git.&lt;/li&gt;
&lt;li&gt;Qt, Rasberry Pi&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://infoscience.epfl.ch/record/271833/files/paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://infoscience.epfl.ch/record/271833/files/paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infoscience.epfl.ch/record/224129/files/paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://infoscience.epfl.ch/record/224129/files/paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tangible Human Swarm Interaction</title>
      <link>https://wafajohal.github.io/prospective/undergrad/h-swarm_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/h-swarm_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/student-projects/swarm.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context:&lt;/h2&gt;
&lt;p&gt;Visuo-Motor coordination problems can impair children in  their academic achievements and in their everyday life.
Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children&amp;rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing.
Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder  or cerebral palsy and need undergo physical therapy.
The therapy sessions are often not engaging for children and conducted individually.
In this project, we aim to design new forms of interaction with a swarm for enhance visuo-motor coordination. We propose to develop a game that allows multiple children to play collaboratively on the same table.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implement the set of basic swarm behaviour using 4 Cellulo robots&lt;/li&gt;
&lt;li&gt;Integrate collaorative and tangible interactions&lt;/li&gt;
&lt;li&gt;Test the system with a participants. We plan ti integrate a measure of cognitive load using eye tracking data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;HCI, Health, Game, Swarm Robotics&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++, Js&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.epfl.ch/labs/chili/index-html/research/cellulo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.epfl.ch/labs/chili/index-html/research/cellulo/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tangible Robots for Collaborative Online Learning</title>
      <link>https://wafajohal.github.io/prospective/undergrad/tangible_online/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/tangible_online/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wafajohal.github.io/img/cellulo.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context:&lt;/h2&gt;
&lt;p&gt;Online learning  presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student&amp;rsquo;s part to plan and stay assiduous in their learning.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots.
The project will consist in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;developing a tool allowing the design of online activities for two or more robots to be connected&lt;/li&gt;
&lt;li&gt;implementing a demonstrator for this new library that will embed a series of small exercises hilightling the new capability of remote haptic-assisted collaboration&lt;/li&gt;
&lt;li&gt;evaluating the demonstrator with a user experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;HCI, Haptics, Robot, Collaborative Work (Training/Gaminig)&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: C++, Js,&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;See Zotero Collection &lt;a href=&#34;https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Schneider, B., Jermann, P., Zufferey, G., &amp;amp; Dillenbourg, P. (2011). Benefits of a Tangible Interface for Collaborative Learning and Interaction. IEEE Transactions on Learning Technologies, 4(3), 222–232. &lt;a href=&#34;https://doi.org/10.1109/TLT.2010.36&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/TLT.2010.36&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Asselborn, T., Guneysu, A., Mrini, K., Yadollahi, E., Ozgur, A., Johal, W., &amp;amp; Dillenbourg, P. (2018). Bringing letters to life: Handwriting with haptic-enabled tangible robots. Proceedings of the 17th ACM Conference on Interaction Design and Children, 219–230.&lt;/li&gt;
&lt;li&gt;East, B., DeLong, S., Manshaei, R., Arif, A., &amp;amp; Mazalek, A. (2016). Actibles: Open Source Active Tangibles. Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces, 469–472. &lt;a href=&#34;https://doi.org/10.1145/2992154.2996874&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/2992154.2996874&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2019a). RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 318–328. &lt;a href=&#34;https://doi.org/10.1145/3308561.3353804&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/3308561.3353804&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2019b). RoboGraphics: Using Mobile Robots to Create Dynamic Tactile Graphics. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 673–675. &lt;a href=&#34;https://doi.org/10.1145/3308561.3354597&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/3308561.3354597&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2018). The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations. Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces, 203–211. &lt;a href=&#34;https://doi.org/10.1145/3279778.3279805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/3279778.3279805&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guneysu, A., Johal, W., Ozgur, A., &amp;amp; Dillenbourg, P. (2018). Tangible Robots Mediated Collaborative Rehabilitation Design: Can we Find Inspiration from Scripting Collaborative Learning? Workshop on Robots for Learning R4L HRI2018.&lt;/li&gt;
&lt;li&gt;Guneysu Ozgur, A., Wessel, M. J., Johal, W., Sharma, K., Ozgur, A., Vuadens, P., Mondada, F., Hummel, F. C., &amp;amp; Dillenbourg, P. (2018). Iterative design of an upper limb rehabilitation game with tangible robots. Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 241–250.&lt;/li&gt;
&lt;li&gt;Guneysu Ozgur, A., Wessel, M. J., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., &amp;amp; Dillenbourg, P. (2020). Gamified Motor Training with Tangible Robots in Older Adults: A Feasibility Study and Comparison with Young. Frontiers in Aging Neuroscience, 12. &lt;a href=&#34;https://doi.org/10.3389/fnagi.2020.00059&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.3389/fnagi.2020.00059&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ishii, H., &amp;amp; Ullmer, B. (1997). Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 234–241. &lt;a href=&#34;https://doi.org/10.1145/258549.258715&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/258549.258715&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johal, W., Tran, A., Khodr, H., Özgür, A., &amp;amp; Dillenbourg, P. (2019). TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration. Proceedings of the 31st Australian Conference on Human-Computer-Interaction, 595–598. &lt;a href=&#34;https://doi.org/10.1145/3369457.3369539&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/3369457.3369539&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Loparev, A., Westendorf, L., Flemings, M., Cho, J., Littrell, R., Scholze, A., &amp;amp; Shaer, O. (2017). BacPack: Exploring the Role of Tangibles in a Museum Exhibit for Bio-Design. Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction, 111–120. &lt;a href=&#34;https://doi.org/10.1145/3024969.3025000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/3024969.3025000&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Okerlund, J., Shaer, O., &amp;amp; Latulipe, C. (2016). Teaching Computational Thinking Through Bio-Design (Abstract Only). Proceedings of the 47th ACM Technical Symposium on Computing Science Education, 698. &lt;a href=&#34;https://doi.org/10.1145/2839509.2850569&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/2839509.2850569&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;O’Malley, C., &amp;amp; Fraser, D. S. (2004). Literature review in learning with tangible technologies.&lt;/li&gt;
&lt;li&gt;Ozgur, A. G., Wessel, M. J., Asselborn, T., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., &amp;amp; Dillenbourg, P. (2019). Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories? 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 5326–5330. &lt;a href=&#34;https://doi.org/10.1109/EMBC.2019.8857508&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/EMBC.2019.8857508&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ozgur, A., Johal, W., Mondada, F., &amp;amp; Dillenbourg, P. (2017). Haptic-enabled handheld mobile robots: Design and analysis. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2449–2461.&lt;/li&gt;
&lt;li&gt;Ozgur, A., Lemaignan, S., Johal, W., Beltran, M., Briod, M., Pereyre, L., Mondada, F., &amp;amp; Dillenbourg, P. (2017). Cellulo: Versatile handheld robots for education. 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI, 119–127.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TeleOp Game - Explore Gamification of Robot Teleoperation</title>
      <link>https://wafajohal.github.io/prospective/undergrad/cleanup_2021/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/cleanup_2021/</guid>
      <description>&lt;h1 id=&#34;context&#34;&gt;Context:&lt;/h1&gt;
&lt;p&gt;Service robots are becoming more common in households. Tasks that they may be assigned include vacuuming the floor or grabbing objects from the top shelf. Augmented reality (AR) is one of the many interfaces the robot and the human user may communicate through. During teleoperation or robot training, human disengagement can be an issue (the task demonstrated by the human often repetitive).&lt;/p&gt;
&lt;p&gt;As a preliminary step, we aim to explore how gamification can affect human engagement in an AR game setting where the objective of the game is to complete a common household task (such as clearing the table). The AR application will be developed for the Microsoft HoloLens 2 headset. The game can be thought of as encouraging the user to complete the task.&lt;/p&gt;
&lt;p&gt;The student will in particular explore how game mechanics and appealing visuals could be inserted through AR to engage the user in repetitive tasks.&lt;/p&gt;
&lt;h1 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Design the game&lt;/li&gt;
&lt;li&gt;Implement teleoperation module for the Kinova Gen 3 Robot.&lt;/li&gt;
&lt;li&gt;Implement gamification elements&lt;/li&gt;
&lt;li&gt;Evaluate the impact of the gamification elements via a user study&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;topics&#34;&gt;Topics:&lt;/h1&gt;
&lt;p&gt;Augmented Reality, Games, Gamification, Robot Teleoperation&lt;/p&gt;
&lt;h1 id=&#34;prerequisites-and-skills-demonstrated-or-willing-to-learn&#34;&gt;Prerequisites and Skills (demonstrated or willing to learn):&lt;/h1&gt;
&lt;p&gt;Python, C#, Unity Framework, ROS (Robotics OS)
Game Design, Computer Vision&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cs.brown.edu/people/er35/publications/testing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://cs.brown.edu/people/er35/publications/testing.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8673306/?casa_token=YjUuijSVKagAAAAA:_SEgCzetsb-CGXU9TLM_3D62yvn8cTyTiMooaiVSbsw2Jm1jj-kgb4QYCOFKc7vCPQTekd_pQWU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/abstract/document/8673306/?casa_token=YjUuijSVKagAAAAA:_SEgCzetsb-CGXU9TLM_3D62yvn8cTyTiMooaiVSbsw2Jm1jj-kgb4QYCOFKc7vCPQTekd_pQWU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3171221.3171251?casa_token=n9ai5qrDzZcAAAAA:Zr_zUFNrar-Zdidk2cMwzWcySh1SeEAIYtagwLHgZFyuze_lbn9cm0ybobYFGQYit6VQYa7rn_KssFA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dl.acm.org/doi/abs/10.1145/3171221.3171251?casa_token=n9ai5qrDzZcAAAAA:Zr_zUFNrar-Zdidk2cMwzWcySh1SeEAIYtagwLHgZFyuze_lbn9cm0ybobYFGQYit6VQYa7rn_KssFA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Voice for ROS</title>
      <link>https://wafajohal.github.io/prospective/undergrad/voice-robot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wafajohal.github.io/prospective/undergrad/voice-robot/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1066/1*OCuPx7AmWofWptQdPN-TPA.png&#34; alt=&#34;robot-va&#34;&gt;&lt;/p&gt;
&lt;p&gt;A scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn about Google DialogFlow and ROS&lt;/li&gt;
&lt;li&gt;Develop a ROS package that enables to access and manipulates DialogFlow features&lt;/li&gt;
&lt;li&gt;Develop a Cellulo Rehabilitation Game&lt;/li&gt;
&lt;li&gt;Test the game with a pilot experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Voice-Assistant, Human-Robot Interaction, ROS&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++,  ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dialogflow.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dialogflow.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ros.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ros.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wafa.johal.org/project/cellulo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://wafa.johal.org/project/cellulo/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., &amp;amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE.&lt;/li&gt;
&lt;li&gt;Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore.&lt;/li&gt;
&lt;li&gt;Beirl, D., Yuill, N., &amp;amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., Lavoué, E., Gweon, C. H., &amp;amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
