[{"authors":["admin"],"categories":null,"content":"I am a Senior Lecturer at the School of Computing \u0026amp; Information Systems, Faculty of Engineering and Information Technology, University of Melbourne.\nPreviously I was a researcher at the CHILI üå∂Ô∏è Lab and Mobots ü§ñ Group at EPFL. I hold a PhD in Computer Sciences from the University of Grenoble Alps. My research aims at creating acceptable and useful assistive robot interactions using social signal sensing, affective and cognitive reasonning and natural expressivity. My latest work has investigated the use of tangible robots in education and rehabiliation.\nI am currently recruiting PhD students.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://wafajohal.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Senior Lecturer at the School of Computing \u0026amp; Information Systems, Faculty of Engineering and Information Technology, University of Melbourne.\nPreviously I was a researcher at the CHILI üå∂Ô∏è Lab and Mobots ü§ñ Group at EPFL. I hold a PhD in Computer Sciences from the University of Grenoble Alps. My research aims at creating acceptable and useful assistive robot interactions using social signal sensing, affective and cognitive reasonning and natural expressivity.","tags":null,"title":"Wafa Johal","type":"authors"},{"authors":null,"categories":null,"content":"import numpy as np import cv2 import matplotlib.pyplot as plt # read an image img = cv2.imread(\u0026#39;test1.jpg\u0026#39;,1) cv2.imshow(\u0026#39;image\u0026#39;,img) cv2.waitKey(0) cv2.destroyAllWindows() # print image dimension print(img.shape) (225, 225, 3)  # print a pixel value px = img[100,100] print(px) [ 8 150 255]  # get an ROI region roi = img[100:120, 100:120] # replace another region in the image with the selected ROI img[180:200, 180:200] = roi # display the modified image cv2.imshow(\u0026#39;image2\u0026#39;,img) cv2.waitKey(0) cv2.destroyAllWindows() # write the image output into a new file cv2.imwrite(\u0026#39;res_1.png\u0026#39;,img) True  plt.imshow(img) \u0026lt;matplotlib.image.AxesImage at 0x28e7f6ba550\u0026gt;  #convert BGR to RGB rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.imshow(rgb_img) \u0026lt;matplotlib.image.AxesImage at 0x28e7f7680b8\u0026gt;  ## to look at the documentation of a function inside your notebook use a \u0026#39;?\u0026#39; cv2.cvtColor? cvtColor(src, code[, dst[, dstCn]]) -\u0026gt; dst . @brief Converts an image from one color space to another. . . The function converts an input image from one color space to another. In case of a transformation . to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note . that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the . bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue . component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and . sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on. . . The conventional ranges for R, G, and B channel values are: . - 0 to 255 for CV_8U images . - 0 to 65535 for CV_16U images . - 0 to 1 for CV_32F images . . In case of linear transformations, the range does not matter. But in case of a non-linear . transformation, an input RGB image should be normalized to the proper value range to get the correct . results, for example, for RGB \\f$\\rightarrow\\f$ L\\*u\\*v\\* transformation. For example, if you have a . 32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will . have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor , . you need first to scale the image down: . @code . img *= 1./255; . cvtColor(img, img, COLOR_BGR2Luv); . @endcode . If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many . applications, this will not be noticeable but it is recommended to use 32-bit images in applications . that need the full range of colors or that convert an image before an operation and then convert . back. . . If conversion adds the alpha channel, its value will set to the maximum of corresponding channel . range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F. . . @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision . floating-point. . @param dst output image of the same size and depth as src. . @param code color space conversion code (see #ColorConversionCodes). . @param dstCn number of channels in the destination image; if the parameter is 0, the number of the . channels is derived automatically from src and code. . . @see @ref imgproc_color_conversions  ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"e28900ad11d91b4cebaa06261c7aa5aa","permalink":"https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo1_image_formation_opencvbasics/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/computer_vision/week1/w1_demo1_image_formation_opencvbasics/","section":"teaching","summary":"Show first Opencv commands","tags":["opencv"],"title":"Demo 1 OpenCV Basics","type":"book"},{"authors":null,"categories":null,"content":"import cv2 import numpy as np from matplotlib import pyplot as plt img = cv2.imread(\u0026#39;noisyImg_1.jpg\u0026#39;,1) #convert BGR to RGB img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.imshow(img) \u0026lt;matplotlib.image.AxesImage at 0x1de43671b70\u0026gt;  #Gaussian filtering blur = cv2.GaussianBlur(img,(5,5),3) plt.imshow(blur) \u0026lt;matplotlib.image.AxesImage at 0x1de437332e8\u0026gt;  cv2.getGaussianKernel? \u001b[1;31mDocstring:\u001b[0m getGaussianKernel(ksize, sigma[, ktype]) -\u0026gt; retval . @brief Returns Gaussian filter coefficients. . . The function computes and returns the \\f$\\texttt{ksize} \\times 1\\f$ matrix of Gaussian filter . coefficients: . . \\f[G_i= \\alpha *e^{-(i-( \\texttt{ksize} -1)/2)^2/(2* \\texttt{sigma}^2)},\\f] . . where \\f$i=0..\\texttt{ksize}-1\\f$ and \\f$\\alpha\\f$ is the scale factor chosen so that \\f$\\sum_i G_i=1\\f$. . . Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize . smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly. . You may also use the higher-level GaussianBlur. . @param ksize Aperture size. It should be odd ( \\f$\\texttt{ksize} \\mod 2 = 1\\f$ ) and positive. . @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as . `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. . @param ktype Type of filter coefficients. It can be CV_32F or CV_64F . . @sa sepFilter2D, getDerivKernels, getStructuringElement, GaussianBlur \u001b[1;31mType:\u001b[0m builtin_function_or_method  #Gaussian filtering v2 G = cv2.getGaussianKernel(5,3) print(G) GM = G*G.T; print(GM) plt.matshow(GM) plt.show() [[0.17820326] [0.21052227] [0.22254894] [0.21052227] [0.17820326]] [[0.0317564 0.03751576 0.03965895 0.03751576 0.0317564 ] [0.03751576 0.04431963 0.04685151 0.04431963 0.03751576] [0.03965895 0.04685151 0.04952803 0.04685151 0.03965895] [0.03751576 0.04431963 0.04685151 0.04431963 0.03751576] [0.0317564 0.03751576 0.03965895 0.03751576 0.0317564 ]]  blur2 = cv2.filter2D(img,-1,GM) #show original and filtered images plt.figure(figsize=(20,10)) plt.subplot(1,3,1),plt.imshow(img),plt.title(\u0026#39;Original\u0026#39;) plt.xticks([]), plt.yticks([]) plt.subplot(1,3,2),plt.imshow(blur),plt.title(\u0026#39;Blurred\u0026#39;) plt.xticks([]), plt.yticks([]) plt.subplot(1,3,3),plt.imshow(blur2),plt.title(\u0026#39;Blurred 2\u0026#39;) plt.xticks([]), plt.yticks([]) plt.show() ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"763837c23346f7ce1e3f94980b2795e7","permalink":"https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_gaussianblur/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/computer_vision/week2/w2_-demo_image_processing_gaussianblur/","section":"teaching","summary":"    ","tags":["opencv"],"title":"Demo Gaussian Blur","type":"teaching"},{"authors":null,"categories":null,"content":"import cv2 import matplotlib.pyplot as plt import numpy as np img = cv2.imread(\u0026#34;beachBox.jpg\u0026#34;) print(img.shape) plt.axis(\u0026#39;off\u0026#39;) # if you want to hide the axis plt.imshow(img) plt.show() (743, 1280, 3)  Documentation on imread\nBookmark this https://docs.opencv.org/\ncv2.__version__ '3.4.5'  Plot the image RGB_im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) print(RGB_im.shape) plt.imshow(RGB_im) plt.show() (743, 1280, 3)  RGB Channel Decomposition b = RGB_im.copy() # (x,y,nb_channels = 3 [R, G, B]) # set green and red channels to 0 b[:, :, 0] = 0 # sets the value in the red channels to 0 b[:, :, 1] = 0 # sets the value in the green channels to 0 g = RGB_im.copy() # set blue and red channels to 0 g[:, :, 0] = 0 g[:, :, 2] = 0 r = RGB_im.copy() # set blue and green channels to 0 r[:, :, 2] = 0 r[:, :, 1] = 0 rg = cv2.cvtColor(r,cv2.COLOR_RGB2GRAY) bg = cv2.cvtColor(b,cv2.COLOR_RGB2GRAY) gg = cv2.cvtColor(g,cv2.COLOR_RGB2GRAY) plt.imshow(RGB_im) plt.show() plt.imshow(r) plt.show() plt.imshow(g) plt.show() plt.imshow(b) plt.show() HSV imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) h = imgHSV.copy() # set S and V channels to 255 h[:, :, 1] = 255 h[:, :, 2] = 255 s = imgHSV.copy() # set H to 179 and V to 255 (max) s[:, :, 0] = 179 s[:, :, 2] = 255 v = imgHSV.copy() # set H to 179 and S to 0 v[:, :, 0] = 179 v[:, :, 1] = 0 h_RGB = cv2.cvtColor(h,cv2.COLOR_HSV2RGB) # convert back to RGB for matplotlib to be able to plot it properly  s_RGB = cv2.cvtColor(s,cv2.COLOR_HSV2RGB) v_RGB = cv2.cvtColor(v,cv2.COLOR_HSV2RGB) plt.imshow(RGB_im) plt.show() plt.imshow(h_RGB) plt.show() plt.imshow(s_RGB) plt.show() plt.imshow(v_RGB) plt.show() LAB imgLAB = cv2.cvtColor(img, cv2.COLOR_BGR2LAB) import matplotlib.colors as clr # this library is used to create the color maps for matplotlib  l = imgLAB.copy() l[:, :, 1] = 0 l[:, :, 2] = 0 a = imgLAB.copy() a[:, :, 0] = 127 a[:, :, 2] = 0 b = imgLAB.copy() b[:, :, 0] = 127 b[:, :, 1] = 0 # here we want to result to be encoded on one channel only, so we can convert the color image into a GRAY scale image.  # Since opencv doesnt have the LAB2GRAY converter, we have to go through the RGB format and then to GRAY l_RGB = cv2.cvtColor(l,cv2.COLOR_LAB2RGB) l_GRAY = cv2.cvtColor(l_RGB,cv2.COLOR_RGB2GRAY) a_RGB = cv2.cvtColor(a,cv2.COLOR_LAB2RGB) a_GRAY = cv2.cvtColor(a_RGB,cv2.COLOR_RGB2GRAY) b_RGB = cv2.cvtColor(b,cv2.COLOR_LAB2RGB) b_GRAY = cv2.cvtColor(b_RGB,cv2.COLOR_RGB2GRAY) plt.imshow(RGB_im) plt.show() plt.imshow(l_GRAY, cmap=\u0026#34;gray\u0026#34;) # L is on the gray scale plt.show() # the \u0026#39;a\u0026#39; values are between red and green, so we create a colormap for matplotlib to display the color range correctly cmap_a = clr.LinearSegmentedColormap.from_list(\u0026#39;custom blue\u0026#39;, [\u0026#39;Red\u0026#39;,\u0026#34;Gray\u0026#34;,\u0026#39;Green\u0026#39;], N=255) plt.imshow(a_GRAY, cmap=cmap_a) plt.show() # Same for \u0026#39;b\u0026#39;, between yellow and blue cmap_b = clr.LinearSegmentedColormap.from_list(\u0026#39;custom blue\u0026#39;, [\u0026#39;Yellow\u0026#39;,\u0026#34;Gray\u0026#34;,\u0026#39;Blue\u0026#39;], N=255) plt.imshow(b_GRAY, cmap=cmap_b) plt.show() YCrCb imgYCrCb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb) # Your turn!  # Try to display the different channels of the YCrCb format individually for our given image using the method above or another method ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a940809baf142d52de372107833edc9e","permalink":"https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo2_image_formation_colorspaces/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/computer_vision/week1/w1_demo2_image_formation_colorspaces/","section":"teaching","summary":"Converting and displaying different colorspace channels in opencv","tags":["opencv"],"title":"Demo 2 Color Spaces","type":"book"},{"authors":null,"categories":null,"content":"I have been awarded 20,000AUD by the Faculty of Engineering to start a new prject called GIRL \u0026ldquo;Gamified and Interactive Robot Learning\u0026rdquo;. I really look forward to starting to work on this.\n","date":1619136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619136000,"objectID":"c8dde6c0402857fa2ece969da63b1ae7","permalink":"https://wafajohal.github.io/post/grow2021/","publishdate":"2021-04-23T00:00:00Z","relpermalink":"/post/grow2021/","section":"post","summary":"I have been awarded 20,000AUD by the Faculty of Engineering to start a new prject called GIRL \u0026ldquo;Gamified and Interactive Robot Learning\u0026rdquo;. I really look forward to starting to work on this.","tags":null,"title":"üèÜ GROW Faculty Engineering Grant - Awarded!","type":"post"},{"authors":null,"categories":null,"content":"Faculty of Engineering, School of Computer Science and Engineering, UNSW Sydney Start Date: Fall, 2021 Area of Research Human-Robot Interaction. Robots are foreseen to be used in a variety of home contexts and interacting with novice users, such as assisting the dependent elderly in daily housework (i.e., caring and unpacking groceries, fetching, and pouring a glass of water, etc). Enabling robots to adapt to their environment through learning context specific tasks would be necessary for them to be used adequately by non-programming users. Our research aims to investigate novel methods to enable users to provide accurate and robust data for a social robot to learn from them.\nCandidate Profile Required Skills  Bachelor or masters degree in Computer Sciences or a related field Good programming skills are required: C++, Python or others Training and skills in AI, machine learning or robotics is preferable Knowledge of classical programming frameworks in these domains is appreciated: Keras, PyTorch, TensorFlow, ROS, etc. English proficiency  Skills Nice to Have  A taste for user/application and impact driven research  Research Environment Depending on the research focus, joint supervisors specialised in Design, Robotics, AI or HCI, will be invited. UNSW and CSE will provide you with access to the HCI and Robotics Labs, GPU clusters and the HRI experimental facility equipped with more than 200 sensors (available to run studies). UNSW Sydney is in the top 100 universities in the world, with more than 59,000 students and a 7,000-strong research community. At UNSW, you will benefit from an international and vibrant research community in a pleasant environment. The net amount of the scholarship will be approximately AUD 2400 per month, increasing annually. You will also receive a holiday allowance. Additional financial support is available for attending conferences and workshops. There will also be possibilities for a research visit (industry or international) during the PhD.\nQueries For informal queries, do not hesitate to contact wafa.johal@unsw.edu.au\nYour application should include:\n A letter highlighting your motivation for your application including: Why do you wish to pursue a PhD in human-robot interaction? What prior experience do you have? How would you approach human-robot interaction research? Also indicate your research interests. A CV, with copies of relevant grades, masters thesis work or publications. The names and contact details of at least 2 referees.  Selected candidates will be invited for an interview, which can be organised over Zoom/Teams.\n","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"d295aad4272eca72828f340353214304","permalink":"https://wafajohal.github.io/post/2phdoffering/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/post/2phdoffering/","section":"post","summary":"Faculty of Engineering, School of Computer Science and Engineering, UNSW Sydney Start Date: Fall, 2021 Area of Research Human-Robot Interaction. Robots are foreseen to be used in a variety of home contexts and interacting with novice users, such as assisting the dependent elderly in daily housework (i.e., caring and unpacking groceries, fetching, and pouring a glass of water, etc). Enabling robots to adapt to their environment through learning context specific tasks would be necessary for them to be used adequately by non-programming users.","tags":null,"title":"üì£ 2 PhD Positions in Human-Robot Interaction ü§ñ","type":"post"},{"authors":null,"categories":null,"content":"Enabling robots to navigate in indoors environments in a safe and socially acceptable manner around groups of humans is still an open research area. Socially aware navigation considers the multi-modal assessment of the group dynamics, group formation inferring, path planning, real-time path adaptation, and human-robot communication. Up to now the research in the field has considered a couple of classical scenarios such as crossing a group in a corridor or passing a door. Limitations in terms of lack of realistic datasets is often mentioned in the field. In this research project, we will aim to design several scenarios involving groups of humans in realistic settings. Our work will focus on three main tasks for the robot: 1) approach and join a group, 2) passing by a group and 3) greeting a group. A first step of the project will be to \\textbf{record a novel dataset} with rich interactions between the humans (H-H scenarios) and between humans and a teleoperated robot (H-R scenarios). The dataset will be collected at the HRI facility allowing multimodal synchronous recording. After that, a \\textbf{new model for path planning} will be developed. For the model, we will explore rule-based constraints (i.e. not passing between two persons speaking together) and learned constrained using the dataset recorded to infer implicit social norms. Finally, the \\textbf{model will be tested} empirically with new users in which the robot will have to take real-time path planning decisions.\nCall for participation to experiment ","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609718400,"objectID":"07ba5f9606aeda5873c6d48582b376e4","permalink":"https://wafajohal.github.io/project/join_the_group/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/project/join_the_group/","section":"project","summary":"Enable robots to navigate in social enviroment around groups","tags":["affective computing","social robot","navigation"],"title":"Join the Group","type":"project"},{"authors":["Aida Amirova","Nazerke Rakhymbayeva","Elmira Yadollahi","Anara Sandygulova","Wafa Johal"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637885071,"objectID":"94f165d8cb059d39c1fbeda0960c5e23","permalink":"https://wafajohal.github.io/publication/amirova-2021-nao/","publishdate":"2021-11-26T00:04:31.412017Z","relpermalink":"/publication/amirova-2021-nao/","section":"publication","summary":"The evolving field of human-robot interaction (HRI) necessitates that we better understand how social robots operate and interact with humans. This scoping review provides an overview of about 300 research works focusing on the use of the NAO robot from 2010 to 2020. This study presents one of the most extensive and inclusive pieces of evidence on the deployment of the humanoid NAO robot and its global reach. Unlike most reviews, we provide both qualitative and quantitative results regarding how NAO is being used and what has been achieved so far. We analyzed a wide range of theoretical, empirical, and technical contributions that provide multidimensional insights, such as general trends in terms of application, the robot capabilities, its input and output modalities of communication, and the human-robot interaction experiments that featured NAO (e.g. number and roles of participants, design, and the length of interaction). Lastly, we derive from the review some research gaps in current state-of-the-art and provide suggestions for the design of the next generation of social robots.","tags":["\"nao\"","\"social robot\"","\"scoping review\""],"title":"10 Years of Human-NAO Interaction Research: A Scoping Review","type":"publication"},{"authors":["Wafa Johal","Barbara Bruno","Jennifer K. Olsen","Mohamed Chetouani","S√©verin Lemaignan","Anara Sandygulova"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637885071,"objectID":"83fccf4ca317c125a6857812c68565cc","permalink":"https://wafajohal.github.io/publication/johal-2021-r-4-l/","publishdate":"2021-11-26T00:04:30.23467Z","relpermalink":"/publication/johal-2021-r-4-l/","section":"publication","summary":"The Robots for Learning workshop series aims at advancing the research topics related to the use of social robots in educational contexts. This year's half-day workshop follows on previous events in Human-Robot Interaction conferences focusing on efforts to dis-cuss potential benchmarks in design, methodology and evaluation of new robotics systems that help learners. In this 6th edition of the workshop, we will be investigating in particular methods from technologies for education and online learning. Since the past few months, online and remote learning has been put in place in several countries to cope with the health and safety measures due to theCovid-19 pandemic. In this workshop, we aim to discuss strategies to design robotics system able to provide embodied assistance to the remote learners and to demonstrate long-term learning effects.","tags":["\"robots for learning\"","\"human-robot interaction\"","\"robot in education\""],"title":"Robots for Learning - Learner-Centred Design","type":"publication"},{"authors":["Yu Liu","Gelareh Mohammadi","Yang Song","Wafa Johal"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637885071,"objectID":"84eac074b000ced196f37fccd894ec21","permalink":"https://wafajohal.github.io/publication/liu-2021-speech/","publishdate":"2021-11-26T00:04:31.218969Z","relpermalink":"/publication/liu-2021-speech/","section":"publication","summary":" Humans use gestures as a means of non-verbal communication. Often accompanying speech, these gestures have several purposes but in general, aim to convey an intended message to the receiver. Researchers have tried to develop systems to allow embodied agents to be better communicators when interacting with humans via using gestures. In this article, we present a scoping literature review of the methods and the metrics used to generate and evaluate co-speech gestures. After collecting a set of papers using a term search on the Scopus database, we analysed the content of these papers based on methodology (i.e., model, the dataset used), evaluation measures (i.e., objective and subjective) and limitations. The results indicate that data-driven approaches are used more frequently. In terms of evaluation measures, we found a trend of combining objective and subjective metrics, while no standards exist for either. This literature review provides an overview of the research in the area and, more specifically insights the trends and the challenges to be met in building a system to automatically generate gestures for embodied agents.","tags":["\"literature review\"","\"survey\"","\"robot\"","\"gesture generation\"","\"co-speech gestures\""],"title":"Speech-Based Gesture Generation for Robots and Embodied Agents: A Scoping Review","type":"publication"},{"authors":["Thibault Asselborn","Wafa Johal","Bolat Tleubayev","Zhanel Zhexenova","Pierre Dillenbourg","Catherine McBride","Anara Sandygulova"],"categories":["2"],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614465141,"objectID":"747406731679275cef88985b14a2c50f","permalink":"https://wafajohal.github.io/publication/asselborn-2021-transferability/","publishdate":"2021-02-27T22:32:21.025518Z","relpermalink":"/publication/asselborn-2021-transferability/","section":"publication","summary":"","tags":["cowriting_kazakh"],"title":"The transferability of handwriting skills: from the Cyrillic to the Latin alphabet","type":"publication"},{"authors":["Daisy Ingle","Nadine Marcus","Wafa Johal"],"categories":[],"content":"PDF  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611034569,"objectID":"8ab80eb36ce7574780eb416490e6df6f","permalink":"https://wafajohal.github.io/publication/ingle-2021/","publishdate":"2021-01-19T05:36:09.494485Z","relpermalink":"/publication/ingle-2021/","section":"publication","summary":"Previous research in psychology has found that human faces have the capability of being more distracting under high perceptual load conditions compared to non-face objects.  This project aims to assess the distracting potential of robot faces based on their human-likeliness. As a first step, this paper reports on our initial findings based on an online study. We used a letter search task where participants had to search for a target letter within a circle of 6 letters, whilst an irrelevant distractor image was also present.  The results of our experiment replicated previous results with human faces and non-face objects. Additionally, in the tasks where the irrelevant distractors are images of robot faces, the human-likeness of the robot influenced the response time (RT).  Interestingly, the robot Alter produced results significantly different than all other distractor robots.  The outcome of this is a distraction model related to human-likeness of robots. Our results show the impact of anthropomorphism on distracting potential and thus should be taken into account when designing robots.","tags":["\"anthropomorphism\"","\"robot design\"","\"human-likeness\"","\"perception load\""],"title":"The Valley of non-Distraction: Effect of Robot's Human-likeness on Perception Load","type":"publication"},{"authors":null,"categories":null,"content":"Our Early Career Academic Network Committee, Eng. ECAN, received the Faculty Award of Equity and Diversity from the Dean of the Faculty of Engineering, Prof. Stephen Foster.\nI am delighted to be part of such a vibrant team, working together to make ECAs shine!\n","date":1608854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608854400,"objectID":"f424233c6170f5afaddcf05ade4ffbdc","permalink":"https://wafajohal.github.io/post/20201215_ecan_facultyaward/","publishdate":"2020-12-25T00:00:00Z","relpermalink":"/post/20201215_ecan_facultyaward/","section":"post","summary":"Our Early Career Academic Network Committee, Eng. ECAN, received the Faculty Award of Equity and Diversity from the Dean of the Faculty of Engineering, Prof. Stephen Foster.\nI am delighted to be part of such a vibrant team, working together to make ECAs shine!","tags":null,"title":"üèÜ Faculty Engineering Award for our ECAN crew!","type":"post"},{"authors":null,"categories":null,"content":"Robots are foreseen to be used in a variety of home contexts and interacting with novice users, such as assisting the dependent elderly in daily housework (i.e., caring and unpacking groceries, fetching, and pouring a glass of water, etc). Enabling robots to adapt to their environment through learning context specific tasks would be necessary for them to be used adequately by non-programming users.\nSeveral methods have been proposed to teach new skills to robots while keeping the human in the loop. Among these methods, the Reinforcement Learning (‚ÄúRL‚Äù) approach is the most common one. However, the literature reports several issues with including human trainers in RL scenarios. Significant research reports a positive bias in RL rewards, and that human-generated reward signals change as the learning progress being inconsistent over time (the trainer adapts her strategy). This can be explained by the difficulty for human trainers to teach basic procedural motions. They generally tend to exaggerate their demonstrations or be more kind with time.\nIn education, a good instructor maintains a mental model of the learner‚Äôs state (what has been learned and what needs clarification). This helps the teacher to appropriately structure the upcoming learning tasks with timely feedback and guidance. The learner can help the instructor by expressing their internal state via communicative acts that reveal their understanding, confusion, and attention. Robot‚Äôs learning parameters, however, can be overwhelming for a novice user and may increase the human workload (by increasing inaccurate feedbacks, and hence decreasing the robot‚Äôs learning). The challenge lies on training humans to be efficient trainers and enabling them to plan, assess and manage the robot‚Äôs learning.\nAnother noticeable issue is the disengagement of humans during the training task. Teaching procedural skills to a robot learner can be time consuming and repetitive. This often results in increased noise in human feedback making their input less reliable. Some researchers have imagined several strategies for the robot to cope with this, such as detecting inconsistencies and asking for additional feedback. This project proposes to investigate how collaborative and competitive games could enable better quality feedback when robots are learning from humans. Inspired by instructional design, we will study how building teaching tools for human teachers can effectively improve the robot‚Äôs learning. We will also aim to engage the trainer longer by identifying and integrating a gamification element in the training.\nThis project is a 3-year endeavour funded by the Australian Research Council starting mid-2021. Under this umbrella, we are looking at hiring two PhD students in the Faculty of Engineering at the School of Computer Science. This project will make an extensive use of the National Facility for Human Robot Interaction Research.\nFor more information about the PhD positions contact Dr Wafa Johal: wafa.johal@unsw.edu.au and consult https://wafa.johal.org/prospective/phd/\n","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"d3bce157c6cd8b0631bb75d07f6f35ee","permalink":"https://wafajohal.github.io/project/hurl/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/project/hurl/","section":"project","summary":"ARC DECRA Fellowship on Human-centred Robot Learning","tags":["robot","learning"],"title":"HURL","type":"project"},{"authors":null,"categories":null,"content":"About a year after landing foot in Australia, I have been awarded with a Discovery Early Career Research Award (DECRA) by the Asutralian Research Council (acceptance 16%).\nThe project I proposed is inline with my vision to build co-learning system in which robots can teach novices some skills (see CoWriter Project) but also learn from human trainers. To find out more about the project see the page: HURL.\n","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"7b4dbece22d5305e1c779b6950e09cdf","permalink":"https://wafajohal.github.io/post/decra2021/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/post/decra2021/","section":"post","summary":"About a year after landing foot in Australia, I have been awarded with a Discovery Early Career Research Award (DECRA) by the Asutralian Research Council (acceptance 16%).\nThe project I proposed is inline with my vision to build co-learning system in which robots can teach novices some skills (see CoWriter Project) but also learn from human trainers. To find out more about the project see the page: HURL.","tags":null,"title":"üèÜ I was awarded with an ARC DECRA fellowship!","type":"post"},{"authors":["Mohammad Obaid","Wafa Johal","Omar Mubin"],"categories":null,"content":"  ","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"c0ab36859939c48522e46924828ba956","permalink":"https://wafajohal.github.io/publication/obaid_hai_2020/","publishdate":"2020-11-10T04:00:36.444312Z","relpermalink":"/publication/obaid_hai_2020/","section":"publication","summary":"Domestic robotic entities are on the rise, out of which, domestic drones are taking place in our society as one of the upcoming interactive technologies that we will see in our daily lives. In this paper, we scope for research literature that addresses the use of domestic drones within our environments to understand the current usage as well as identifying future research directions. After performing a search based collection of relevant papers in the ACM digital library (N=61 papers), we analysed the drone's application areas, their interaction modalities, the target users, and the level of autonomy of the proposed systems. The results show interesting trends in the modalities of interaction (visual projection combined with hand/foot gestures) as well as important research gaps such as child-drone interaction, and the use of drones for healthcare or education, given that currently most use cases for domestic drones are generic in nature. ","tags":["drone"],"title":"Domestic Drones: Context of Use in Research Literature","type":"publication"},{"authors":null,"categories":null,"content":"  ","date":1599782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599782400,"objectID":"690cdb80b773bfdeaa065041a42a8f61","permalink":"https://wafajohal.github.io/talk/2020_edudrone/","publishdate":"2020-09-11T00:00:00Z","relpermalink":"/talk/2020_edudrone/","section":"talk","summary":"  ","tags":null,"title":"Social (Flying) Robots for Education. September 2020.","type":"talk"},{"authors":null,"categories":null,"content":"Super excited about the new SydCHI local chapter. Stay tune!\n","date":1593043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593043200,"objectID":"8a3210f61f3d216086c5f06a9cd5c134","permalink":"https://wafajohal.github.io/post/sydchi_accepted/","publishdate":"2020-06-25T00:00:00Z","relpermalink":"/post/sydchi_accepted/","section":"post","summary":"Super excited about the new SydCHI local chapter. Stay tune!","tags":null,"title":"The SydCHI Local Chapter just got accepted","type":"post"},{"authors":["Isabel Neto","Wafa Johal","Marta Couto","Hugo Nicolau","Ana Paiva","Arzu Guneysu"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"5b4411c726fa0049025d4bd5a28e7ace","permalink":"https://wafajohal.github.io/publication/neto-using-2020/","publishdate":"2020-04-28T04:00:36.444312Z","relpermalink":"/publication/neto-using-2020/","section":"publication","summary":"Geometry and handwriting rely heavily on the visual representation of basic shapes. It can become challenging for students with visual impairments to perceive these shapes and understand complex spatial constructs. For instance, knowing how to draw is highly dependent on spatial and temporal components, which are often inaccessible to children with visual impairments. Hand-held robots, such as the Cellulo robots, open unique opportunities to teach drawing and writing through haptic feedback. In this paper, we investigate how these tangible robots could support inclusive, collaborative learning activities, particularly for children with visual impairments. We conducted a user study with 20 pupils with and without visual impairments, where they engaged in multiple drawing activities with tangible robots. We contribute novel insights on the design of children-robot interaction, learning shapes and letters, children engagement, and responses in a collaborative scenario that address the challenges of inclusive learning. ","tags":["tangible","cellulo"],"title":"Using Tabletop Robots to promote Inclusive Classroom Experiences","type":"publication"},{"authors":null,"categories":null,"content":"Our international team received the best demo award at @HRI2020 @dillenbo @AnaraSandy! #SNSF #CoKaz http://humanrobotinteraction.org/2020/best-paper-awards/\n","date":1590364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590364800,"objectID":"36855b11a301badce7a2efa7c1f26665","permalink":"https://wafajohal.github.io/post/hri2020_demo_award/","publishdate":"2020-05-25T00:00:00Z","relpermalink":"/post/hri2020_demo_award/","section":"post","summary":"Our international team received the best demo award at @HRI2020 @dillenbo @AnaraSandy! #SNSF #CoKaz http://humanrobotinteraction.org/2020/best-paper-awards/","tags":["cowriting_kazakh"],"title":"üèÜ Best Demo Award at HRI 2020!","type":"post"},{"authors":null,"categories":null,"content":"import cv2 import matplotlib.pyplot as plt import numpy as np img = cv2.imread(\u0026#34;minion.jpg\u0026#34;,0) # read as a grayscale image print(img.shape) plt.imshow(img) plt.show() (532, 800)  plt.imshow(img, cmap=\u0026#39;gray\u0026#39;, vmin=0, vmax=255) plt.show() cv2.imshow(\u0026#39;Grayscale Image\u0026#39;, img) cv2.waitKey(0) # press any key cv2.destroyAllWindows() Resize Image Let\u0026rsquo;s play with the resolution of this image and try to resize it.\ncv2.resize? \u001b[1;31mDocstring:\u001b[0m resize(src, dsize[, dst[, fx[, fy[, interpolation]]]]) -\u0026gt; dst . @brief Resizes an image. . . The function resize resizes the image src down to or up to the specified size. Note that the . initial dst type or size are not taken into account. Instead, the size and type are derived from . the `src`,`dsize`,`fx`, and `fy`. If you want to resize src so that it fits the pre-created dst, . you may call the function as follows: . @code . // explicitly specify dsize=dst.size(); fx and fy will be computed from that. . resize(src, dst, dst.size(), 0, 0, interpolation); . @endcode . If you want to decimate the image by factor of 2 in each direction, you can call the function this . way: . @code . // specify fx and fy and let the function compute the destination image size. . resize(src, dst, Size(), 0.5, 0.5, interpolation); . @endcode . To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to . enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR . (faster but still looks OK). . . @param src input image. . @param dst output image; it has the size dsize (when it is non-zero) or the size computed from . src.size(), fx, and fy; the type of dst is the same as of src. . @param dsize output image size; if it equals zero, it is computed as: . \\f[\\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\\f] . Either dsize or both fx and fy must be non-zero. . @param fx scale factor along the horizontal axis; when it equals 0, it is computed as . \\f[\\texttt{(double)dsize.width/src.cols}\\f] . @param fy scale factor along the vertical axis; when it equals 0, it is computed as . \\f[\\texttt{(double)dsize.height/src.rows}\\f] . @param interpolation interpolation method, see #InterpolationFlags . . @sa warpAffine, warpPerspective, remap \u001b[1;31mType:\u001b[0m builtin_function_or_method  half = cv2.resize(img, None, fx=0.5, fy=0.5) print(half.shape) plt.imshow(half, cmap=\u0026#39;gray\u0026#39;, vmin=0, vmax=255) plt.show() (266, 400)  # if you run this cell several times, you will see the resolution decreasing very fast. half = cv2.resize(half, None, fx=0.5, fy=0.5) print(half.shape) plt.imshow(half, cmap=\u0026#39;gray\u0026#39;, vmin=0, vmax=255) plt.show() (133, 200)  half.shape (133, 200)  half.size 26600  half.dtype ","date":1578009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578009600,"objectID":"c0365827f01d2baa3c9fe18935ba2ce0","permalink":"https://wafajohal.github.io/teaching/computer_vision/week1/w1_demo3_image_formation_resolution/","publishdate":"2020-01-03T00:00:00Z","relpermalink":"/teaching/computer_vision/week1/w1_demo3_image_formation_resolution/","section":"teaching","summary":"Play with image resolution","tags":["opencv"],"title":"Demo 3 Resolution","type":"teaching"},{"authors":["Quentin Chibaudel","Wafa Johal","Bernard Oriola","Marc J-M Mac√©","Pierre Dillenbourg","Val√©rie Tartas","Christophe Jouffrais"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"f01d75f1fe4cd29d730121295fff91d6","permalink":"https://wafajohal.github.io/publication/chibaudel-2020/","publishdate":"2021-01-16T09:38:54.382277Z","relpermalink":"/publication/chibaudel-2020/","section":"publication","summary":"Tangible User Interfaces (TUI) have been found to be relevant tools for collaborative learning by providing a shared workspace and enhancing joint visual attention. Researchers have explored the use of TUIs in a variety of curricular activities and found them particularly interesting for spatial exploration. However, very few studies have explored how TUIs could be used as a collaborative medium for people with visual impairments (VIs). In this study, we investigated the effect of tangible interaction (a small tangible robot) in a spatial collaborative task (a treasure hunt) involving two people with VIs. The aim was to evaluate the impact of the design of the TUI on the collaboration and the strategies used to perform the task. The experiment involved six dyads of people with VIs. The results showed that the collaboration was impacted by the interaction design and open interesting perspectives on the design of collaborative games for people with VIs.","tags":["\"Spatial cognition\"","\"Game\"","\"Wayfinding\"","\"Robots\"","\"Non-visual interaction\"","\"Maps\"","\"Learning\"","\"Haptics\"","\"3D model\""],"title":"\\\"If You've Gone Straight, Now, You Must Turn Left\\\" - Exploring the Use of a Tangible Interface in a Collaborative Treasure Hunt for People with Visual Impairments","type":"publication"},{"authors":["Zhanel Zhexenova","Aida Amirova","Manshuk Abdikarimova","Kuanysh Kudaibergenov","Nurakhmet Baimakhan","Bolat Tleubayev","Thibault Asselborn","Wafa Johal","Pierre Dillenbourg","Anna CohenMiller","Anara Sandygulova"],"categories":["2"],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"f4286ce525bd2507338ff84d4ccbe045","permalink":"https://wafajohal.github.io/publication/zhanel-2020/","publishdate":"2021-01-16T09:38:54.455782Z","relpermalink":"/publication/zhanel-2020/","section":"publication","summary":"This research occurred in a special context where Kazakhstan's recent decision to switch from Cyrillic to the Latin-based alphabet has resulted in challenges connected to teaching literacy, addressing a rare combination of research hypotheses and technical objectives about language learning. Teachers are not necessarily trained to teach the new alphabet, and this could result in a challenge for children with learning difficulties. Prior research studies in Human-Robot Interaction (HRI) have proposed the use of a robot to teach handwriting to children (Hood et al., 2015; Lemaignan et al., 2016). Drawing on the Kazakhstani case, our study takes an interdisciplinary approach by bringing together smart solutions from robotics, computer vision areas, and educational frameworks, language, and cognitive studies that will benefit diverse groups of stakeholders. In this study, a human-robot interaction application is designed to help primary school children learn both a newly-adopted script and also its handwriting system. The setup involved an experiment with 62 children between the ages of 7‚Äì9 years old, across three conditions: a robot and a tablet, a tablet only, and a teacher. Based on the paradigm‚Äîlearning by teaching‚Äîthe study showed that children improved their knowledge of the Latin script by interacting with a robot. Findings reported that children gained similar knowledge of a new script in all three conditions without gender effect. In addition, children's likeability ratings and positive mood change scores demonstrate significant benefits favoring the robot over a traditional teacher and tablet only approaches.","tags":["cowriting_kazakh"],"title":"A Comparison of Social Robot to Tablet and Teacher in a New Script Learning Context","type":"publication"},{"authors":["Thomas Gargot","Thibault Asselborn","Hugues Pellerin","Ingrid Zammouri","Salvatore M. Anzalone","Laurence Casteran","Wafa Johal","Pierre Dillenbourg","David Cohen","Caroline Jolly"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604290917,"objectID":"5c782e3df399b09339850ce784c0dc85","permalink":"https://wafajohal.github.io/publication/gargot-2020-acquisition/","publishdate":"2020-11-02T04:21:57.234702Z","relpermalink":"/publication/gargot-2020-acquisition/","section":"publication","summary":"Handwriting is a complex skill to acquire and it requires years of training to be mastered. Children presenting dysgraphia exhibit difficulties automatizing their handwriting. This can bring anxiety and can negatively impact education. 280 children were recruited in schools and specialized clinics to perform the Concise Evaluation Scale for Children‚Äôs Handwriting (BHK) on digital tablets. Within this dataset, we identified children with dysgraphia. Twelve digital features describing handwriting through different aspects (static, kinematic, pressure and tilt) were extracted and used to create linear models to investigate handwriting acquisition throughout education. K-means clustering was performed to define a new classification of dysgraphia. Linear models show that three features only (two kinematic and one static) showed a significant association to predict change of handwriting quality in control children. Most kinematic and statics features interacted with age. Results suggest that children with dysgraphia do not simply differ from ones without dysgraphia by quantitative differences on the BHK scale but present a different development in terms of static, kinematic, pressure and tilt features. The K-means clustering yielded 3 clusters (Ci). Children in C1 presented mild dysgraphia usually not detected in schools whereas children in C2 and C3 exhibited severe dysgraphia. Notably, C2 contained individuals displaying abnormalities in term of kinematics and pressure whilst C3 regrouped children showing mainly tilt problems. The current results open new opportunities for automatic detection of children with dysgraphia in classroom. We also believe that the training of pressure and tilt may open new therapeutic opportunities through serious games.","tags":[],"title":"Acquisition of handwriting in children with and without dysgraphia: A computational approach","type":"publication"},{"authors":["Anara Sandygulova","Wafa Johal","Zhanel Zhexenova","Bolat Tleubayev","Aida Zhanatkyzy","Aizada Turarova","Zhansaule Telisheva","Anna CohenMiller","Thibault Asselborn","Pierre Dillenbourg"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"530a07ea8569051cc9f48fe2ae922202","permalink":"https://wafajohal.github.io/publication/sandygulova-hri-2020/","publishdate":"2020-03-09T03:55:01.363162Z","relpermalink":"/publication/sandygulova-hri-2020/","section":"publication","summary":"In the Republic of Kazakhstan, the transition from Cyrillic to Latin alphabet raises challenges to training an entire population in writing the new script. This paper presents a CoWriting Kazakh system, an extension of the existing CoWriter system, aiming to implement an autonomous social robot that would assist children in transition from the old Cyrillic alphabet to a new Latin alphabet. With the aim to investigate which learning strategy yields better learning gains, we conducted an experiment with 67 children, aged 8-11 years old, who interacted with a robot in a CoWriting Kazakh learning scenario. Participants were asked to teach a humanoid NAO robot how to write Kazakh words using one of the scripts, Latin or Cyrillic. We hypothesized that a scenario in which the child is asked to mentally convert the word to Latin would be more effective than having the robot perform conversion itself. Results show that the CoWriter was successfully applied to this new script-switching task. The findings also suggest interesting gender differences in the preferred method of learning with the robot.","tags":["cowriting_kazakh"],"title":"CoWriting Kazakh: Learning a New Script with a Robot","type":"publication"},{"authors":["Bolat Tleubayev","Zhanel Zhexenova","Thibault Asselborn","Wafa Johal","Pierre Dillenbourg","Anara Sandygulova"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614465141,"objectID":"67d918c1a1b28e99d950dee03e62aa1e","permalink":"https://wafajohal.github.io/publication/bolat-hri-2020-demo/","publishdate":"2021-02-27T22:32:20.526839Z","relpermalink":"/publication/bolat-hri-2020-demo/","section":"publication","summary":"This interdisciplinary project aims to assess and manage the risks relating to the transition of Kazakh language from Cyrillic to Latin in Kazakhstan in order to address challenges of a) teaching and motivating children to learn a new script and its associated handwriting, and b) training and providing support for all demographic groups, in particular senior generation. We present the system demonstration that proposes to assist and motivate children to learn a new script with the help of a humanoid robot and a tablet with stylus.","tags":["language learning","social robot","handwriting recognition","cowriting_kazakh"],"title":"CoWriting Kazakh: Learning a New Script with a Robot - Demonstration","type":"publication"},{"authors":["Zhanel Zhexenova","Bolat Tleubayev","Aida Zhanatkyzy","Aizada Turarova","Zhansaule Telisheva","Wafa Johal","Thibault Asselborn","Pierre Dillenbourg","Anara Sandygulova"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"40b7fc1a8870bfc014c81bdf6a9f6941","permalink":"https://wafajohal.github.io/publication/zhanel-hri-2020-video/","publishdate":"2020-03-09T03:42:41.109295Z","relpermalink":"/publication/zhanel-hri-2020-video/","section":"publication","summary":"","tags":["cowriting_kazakh"],"title":"CoWriting Kazakh: Learning a New Script with a Robot - Video ","type":"publication"},{"authors":null,"categories":null,"content":"import cv2 import numpy as np from matplotlib import pyplot as plt img = cv2.imread(\u0026#39;minion.jpg\u0026#39;,0) plt.figure(figsize=(20,10)) plt.imshow(img,\u0026#39;gray\u0026#39;) plt.show() out = [] plt.figure(figsize=(30,20)) for k in range(0, 8): # create an image for each k bit plane plane = np.full((img.shape[0], img.shape[1]), 2 ** k, np.uint8) # execute bitwise and operation res = cv2.bitwise_and(plane, img) # multiply ones (bit plane sliced) with 255 just for better visualization x = cv2.equalizeHist(res) out.append(res) plt.subplot(1,8,k+1),plt.imshow(res,\u0026#39;gray\u0026#39;),plt.title(\u0026#34;plane \u0026#34;+str(k)) plt.xticks([]), plt.yticks([]) plt.show() ms_planes = out[-1] +out[-2] +out[-3] plt.imshow(ms_planes,\u0026#39;gray\u0026#39;),plt.title(\u0026#34; plane 5-7\u0026#34;) (\u0026lt;matplotlib.image.AxesImage at 0x243a2f83be0\u0026gt;, Text(0.5, 1.0, ' plane 5-7'))  ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"2fb43d4bce6e774d727411547145ee2d","permalink":"https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_bitplanes/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/computer_vision/week2/w2_-demo_image_processing_bitplanes/","section":"teaching","summary":"  ","tags":["opencv"],"title":"Demo Bit-Planes","type":"book"},{"authors":null,"categories":null,"content":"import cv2 import numpy as np from matplotlib import pyplot as plt img = cv2.imread(\u0026#39;minion.jpg\u0026#39;,1) #convert BGR to RGB img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # show the image plt.figure(figsize=(20,10)) plt.subplot(1,2,1) plt.imshow(img, cmap = \u0026#34;gray\u0026#34;) # display the historgram plt.subplot(1,2,2) plt.hist(img.flatten()) plt.show() # create the centering matrix centering_m = np.full((img.shape[0] , img.shape[1]), -1) for x in range(0,img.shape[0]): for y in range(0,img.shape[1]): centering_m[x,y] = (-1)**(x+y) plt.figure(figsize=(20,10)) plt.subplot(1,2,1) plt.imshow(centering_m[0:10,0:10]) plt.subplot(1,2,2) plt.hist(centering_m.flat) plt.show() centered_img = img *centering_m plt.figure(figsize=(20,10)) plt.subplot(1,2,1) plt.imshow(centered_img, cmap = \u0026#34;gray\u0026#34;) plt.subplot(1,2,2) plt.hist(centered_img.flatten()) plt.show() # compute the 2D FFT dft = np.fft.fft2(centered_img) plt.figure(figsize=(20,10)) plt.imshow(np.log(1+np.abs(dft)), cmap = \u0026#34;gray\u0026#34;) \u0026lt;matplotlib.image.AxesImage at 0x1dfe72c33c8\u0026gt;  def distance(point1,point2): return np.sqrt((point1[0]-point2[0])**2 + (point1[1]-point2[1])**2) Filters Ideal LowPass Filter def idealFilterLP(D0,imgShape): base = np.zeros(imgShape[:2]) rows, cols = imgShape[:2] center = (rows/2,cols/2) for x in range(cols): for y in range(rows): if distance((y,x),center) \u0026lt; D0: base[y,x] = 1 return base plt.imshow(idealFilterLP(50,img.shape), cmap=\u0026#39;gray\u0026#39;),plt.title(\u0026#39;Low Pass Filter\u0026#39;) plt.show() Ideal HighPass Filter def idealFilterHP(D0,imgShape): base = np.ones(imgShape[:2]) rows, cols = imgShape[:2] center = (rows/2,cols/2) for x in range(cols): for y in range(rows): if distance((y,x),center) \u0026lt; D0: base[y,x] = 0 return base plt.imshow(idealFilterHP(50,img.shape), cmap=\u0026#39;gray\u0026#39;),plt.title(\u0026#39;High Pass Filter\u0026#39;) plt.show() Gaussian LowPass Filter def gaussianLP(D0,imgShape): base = np.zeros(imgShape[:2]) rows, cols = imgShape[:2] center = (rows/2,cols/2) for x in range(cols): for y in range(rows): base[y,x] = np.exp(((-distance((y,x),center)**2)/(2*(D0**2)))) return base plt.imshow(gaussianLP(80,img.shape), cmap=\u0026#39;gray\u0026#39;),plt.title(\u0026#39;Gaussian Low Pass Filter\u0026#39;) plt.xticks([]), plt.yticks([]) plt.show() Gaussian HighPass Filter def gaussianHP(D0,imgShape): base = np.zeros(imgShape[:2]) rows, cols = imgShape[:2] center = (rows/2,cols/2) for x in range(cols): for y in range(rows): base[y,x] = 1 - np.exp(((-distance((y,x),center)**2)/(2*(D0**2)))) return base plt.imshow(gaussianHP(80,img.shape), cmap=\u0026#39;gray\u0026#39;),plt.title(\u0026#39;Gaussian High Pass Filter\u0026#39;) plt.xticks([]), plt.yticks([]) plt.show() Apply Filter on DFT # filter the DFT fdft = dft * idealFilterLP(50,img.shape) ## note that we do a multiplication cause we are in the frequency domain # inverse the filtered DFT idft = np.fft.ifft2(fdft) # take the real part and de center it real_centered = idft.real * centering_m plt.figure(figsize=(20,10)) plt.subplot(1,2,1) plt.imshow(np.log(1+np.abs(fdft)), cmap = \u0026#34;gray\u0026#34;) plt.title(\u0026#39;Filtered DFT\u0026#39;) plt.subplot(1,2,2) plt.imshow(real_centered, cmap = \u0026#34;gray\u0026#34;,vmin=0, vmax=255) plt.title(\u0026#39;Inversed Filtered DFT\u0026#39;) plt.show() # filter the DFT fdft = dft * gaussianLP(50,img.shape) # inverse the filtered DFT idft = np.fft.ifft2(fdft) # take the real part and de center it real_centered = idft.real * centering_m plt.figure(figsize=(20,10)) plt.subplot(1,2,1) plt.imshow(np.log(1+np.abs(fdft)), cmap = \u0026#34;gray\u0026#34;) plt.title(\u0026#39;Filtered DFT\u0026#39;) plt.subplot(1,2,2) plt.imshow(real_centered, cmap = \u0026#34;gray\u0026#34;, vmin=0, vmax=255) plt.title(\u0026#39;Inversed Filtered DFT\u0026#39;) plt.show() Demonstraring that we can go back and forth from image to frequency without loss # inverse the filtered DFT idft = np.fft.ifft2(dft) # take the real part and de center it real_centered = idft.real * centering_m plt.figure(figsize=(20,10)) plt.subplot(1,2,1) plt.imshow(img, cmap = \u0026#34;gray\u0026#34;) plt.title(\u0026#39; Original Image\u0026#39;) plt.subplot(1,2,2) plt.imshow(real_centered, cmap = \u0026#34;gray\u0026#34;, vmin=0, vmax=255) plt.title(\u0026#39;Inversed DFT\u0026#39;) plt.show() plt.imshow(real_centered - img, cmap = \u0026#34;gray\u0026#34;, vmin=0, vmax=255) plt.title(\u0026#39;Difference\u0026#39;) plt.show() ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"91b601440bb639015bca72baf493e3fe","permalink":"https://wafajohal.github.io/teaching/computer_vision/week2/w2_-demo_image_processing_fourier_smoothing/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/computer_vision/week2/w2_-demo_image_processing_fourier_smoothing/","section":"teaching","summary":"  ","tags":["opencv"],"title":"Demo Fourier Transform and Frequency filtering","type":"teaching"},{"authors":["Elmira Yadollahi","Marta Couto","Wafa Johal","Pierre Dillenbourg","Ana Paiva"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604290917,"objectID":"788eaf76220a095a30a022b68b5b2619","permalink":"https://wafajohal.github.io/publication/yadollahi-2020-exploring/","publishdate":"2020-11-02T04:21:57.079818Z","relpermalink":"/publication/yadollahi-2020-exploring/","section":"publication","summary":"Perspective taking is an important skill to have and learn, which can be applied in many different domains and disciplines. While the ability to recognize other‚Äôs perspective develops in humans from childhood and solidifies during school years, it needs to be developed in robotic and artificial agents‚Äô cognitive framework. In our quest to develop a cognitive model of perspective taking for agents and robots in educational contexts, we designed a task that requires the players (e.g., child and robot) to take the perspective of another, in order to complete and win the task successfully. In a preliminary study to test the system, we were able to evaluate children‚Äôs performance over four different age groups by focusing on their performance during the interaction with the robot. By analyzing children‚Äôs performance, we were able to make some assumptions about children‚Äôs understanding of the game and select the appropriate age group to participate in the main study.","tags":[],"title":"Exploring the Role of Perspective Taking in Educational Child-Robot Interaction","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Maximilian Jonas Wessel","Jennifer Kaitlyn Olsen","Wafa Johal","Ayberk √ñzg√ºr","Friedhelm C. Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"ff43b3272c201c032686bf6b10081eef","permalink":"https://wafajohal.github.io/publication/guneysu-ozgur-gamified-2020/","publishdate":"2020-03-09T03:42:41.109729Z","relpermalink":"/publication/guneysu-ozgur-gamified-2020/","section":"publication","summary":"Background: The increasing lifespan and the resulting change of our expectations in later life stages are dependent on a good health state. This emphasizes the development of strategies to further healthy ageing. One important aspect of good health in later life stages is sustained skilled motor function. Objective: Here, we tested the effectiveness of robotic upper limb motor training in a game-like scenario assessing game-based learning and its transfer potential. Methods: Thirty-six healthy participants (n = 18 elderly participants, n = 18 young controls) trained with a Pacman-like game using a hand-held Cellulo robot on two consecutive days. The game-related movements were conducted on a printed map displaying a maze and targets that had to be collected. Gradually, the task difficulty was adjusted between games by modifying or adding different game elements (e.g., speed and number of chasing ghosts, additional rules, haptic feedback). Transfer was assessed by scoring simple robot manipulation on two different trajectories. Results: Elderly participants were able to improve their game performance over time (t(874) = 2.97, p textless .01). The applied game-elements had similar effects on both age groups. Importantly, the game-based learning transferred to simple robot manipulation, which resembles activities of daily life. Only minor age-related differences were present (smaller overall learning gain, different effect of the wall crash penalty rule in the elderly group). Conclusions: Gamified motor training with the Cellulo system has the potential to translate into an efficient, relatively low-cost robotic motor training tool for promoting upper limb function to promote healthy aging.","tags":["tangible robots","Task Performance and Analysis","Aging","Gamified activities","Healthy Ageing","motor learning","Robotic exercise","Transfer Learning"],"title":"Gamified Motor Training with Tangible Robots in Older Adults: a Feasibility Study and Comparison with Young","type":"publication"},{"authors":null,"categories":null,"content":"Nearly 8% of children between 4 and 12 are dysgraphic or encounter serious difficulties in learning handwriting. Dysgraphia, just like dyslexia, can seriously impair the everyday school life of children and have damageable impact on their academic achievements. The detection of such difficulties should be done as soon as possible to minimize its effect on the child‚Äôs school performances. And an adaptive remediation should be provided. The CHILI Lab at EPFL showed how a robotic co-learner agent could have positive effects on the child\u0026rsquo;s motivation to practice handwriting. However, motivation is only one aspect that needs to be taken into account in order to provide a long-term adaptive training experience for children learning how to write.\nIn the iReCHeCk project, we propose a finer characterization of handwriting, as a multimodal activity, taking into account the body posture to capture important features in the handwriting process. We will develop engaging training activities with a robot to monitor the learning status of the child, allowing therapists to evaluate progresses, while adapting the robot\u0026rsquo;s attitude and the training task to the needs of the learner. Our goal being to touch a wide range of handwriting learners, these activities will be tested on two populations: Typically Developed Children that are learning how to write at school; Children with Neuro-Developmental Disorders, presenting handwriting difficulties along with other disorders (i.e. attentional, autistic).\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8f60c8b41bb198b76a403898860d54f5","permalink":"https://wafajohal.github.io/project/irecheck/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/irecheck/","section":"project","summary":"A Robotic Handwriting Social Companion","tags":["r4l","cowriter","tegami","irecheck"],"title":"iReCheck","type":"project"},{"authors":["Arzu Guneysu Ozgur","Ayberk √ñzg√ºr","Thibault Asselborn","Wafa Johal","Elmira Yadollahi","Barbara Bruno","Melissa Skweres","Pierre Dillenbourg"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"38d58b1363e51d58966c348fd4a15d22","permalink":"https://wafajohal.github.io/publication/guneysu-ozgur-iterative-2020/","publishdate":"2020-03-09T03:42:41.109531Z","relpermalink":"/publication/guneysu-ozgur-iterative-2020/","section":"publication","summary":"In this article, we investigate the role of interactive haptic-enabled tangible robots in supporting the learning of cursive letter writing for children with attention and visuomotor coordination issues. We focus on the two principal aspects of handwriting that are linked to these issues: Visual perception and visuomotor coordination. These aspects respectively enhance two features of letter representation in the learner's mind in particular, namely the shape (grapheme) and the dynamics (ductus) of the letter, which constitute the central learning goals in our activity. Building upon an initial design tested with 17 healthy children in a preliminary school, we iteratively ported the activity to an occupational therapy context in 2 different therapy centers, in the context of 3 different summer school camps involving a total of 12 children having writing difficulties. The various iterations allowed us to uncover insights about the design of robot-enhanced writing activities for special education, specifically highlighting the importance of ease of modification of the duration of an activity as well as of adaptable frequency, content, flow, and game-play and of providing a range of evaluation test alternatives. Results show that the use of robot-assisted handwriting activities could have a positive impact on the learning of the representation of letters in the context of occupational therapy (V = 1449, p textless 0.001, r=0.42). Results also highlight how the design changes made across the iterations affected the outcomes of the handwriting sessions, such as the evaluation of the performances, monitoring of the performances, and the connectedness of the handwriting.","tags":["tangible robots","handwriting","Gamified activities","Haptic interface","interactive   learning environments","iterative design","Occupational thearpy","Robots for education"],"title":"Iterative Design and Evaluation of a Tangible Robot-Assisted Handwriting Activity for Special Education","type":"publication"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8e8effa98d8903b8b47e0eb986654c2b","permalink":"https://wafajohal.github.io/teaching/human-robot-interaction/questionnaires/questionnaires/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/human-robot-interaction/questionnaires/questionnaires/","section":"teaching","summary":"  ","tags":["hri","questionnaire"],"title":"List of Typical Questionnaires used in HRI","type":"book"},{"authors":["T Gargot","D Archambault","M Chetouani","D Cohen","W Johal","SM Anzalone"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"523b54ceb9b6dd5a4173735be0149ad5","permalink":"https://wafajohal.github.io/publication/gargot-2020-p/","publishdate":"2021-01-16T09:38:54.301115Z","relpermalink":"/publication/gargot-2020-p/","section":"publication","summary":"","tags":[],"title":"P. 114 Automatic assessment of motors impairments in autism spectrum disorders: a systematic review","type":"publication"},{"authors":["Wafa Johal"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604290917,"objectID":"d689616ba8d9b3113f9752188052b110","permalink":"https://wafajohal.github.io/publication/johal-2020-research/","publishdate":"2020-11-02T04:21:56.913233Z","relpermalink":"/publication/johal-2020-research/","section":"publication","summary":" **Purpose of Review**\n\nWith the growth in the number of market-available social robots, there is an increasing interest in research on the usage of social robots in education. This paper proposes a summary of trends highlighting current research directions and potential research gaps for social robots in education. We are interested in design aspects and instructional setups used to evaluate social robotics system in an educational setting.\n\n**Recent Findings**\n\nThe literature demonstrates that as the field grows, setup, methodology, and demographics targeted by social robotics applications seem to settle and standardize‚Äîa tutoring Nao robot with a tablet in front of a child seems the stereotypical social educational robotics setup.\n\n**Summary**\n\nAn updated review on social robots in education is presented here. We propose, first, an analysis of the pioneering works in the field. Secondly, we explore the potential for education to be the ideal context to investigate central human-robot interaction research questions. A trend analysis is then proposed demonstrating the potential for educational context to nest impactful research from human-robot interaction. ","tags":[],"title":"Research Trends in Social Robots for Learning","type":"publication"},{"authors":["Wafa Johal","Yu Peng","Haipeng Mi"],"categories":[],"content":"  ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"abca8b1f66fac3a0a1af4ce9f4d0025c","permalink":"https://wafajohal.github.io/publication/johal-2020-swarm/","publishdate":"2021-01-16T09:38:53.98659Z","relpermalink":"/publication/johal-2020-swarm/","section":"publication","summary":"This study reviews published scientific literature on the use of swarm robots for education purposes in the last ten years. It focuses on user studies involving robotics swarm in order to identify the potential contributions of the incorporation of swarm robots as an educational tool and insight future research. We consider here the appearance of swarm robots, the curriculum of the experimental task and the interaction modalities between learners and robots. The outcomes of the literature review are discussed in terms of their existing challenges and opportunities for guiding researchers, educators, and practitioners.","tags":["\"educational tool\"","\"swarm robots\"","\"literature review\"","\"learning\"","\"multi-robot\""],"title":"Swarm Robots in Education: A Review of Challenges and Opportunities","type":"publication"},{"authors":null,"categories":null,"content":"Presented our work on the design of tangible devices for classroom orchestration.\n  ","date":1575417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575417600,"objectID":"3dbef871c21d9dd8cc58d6508a90bfd4","permalink":"https://wafajohal.github.io/talk/ozchi2019/","publishdate":"2019-12-04T00:00:00Z","relpermalink":"/talk/ozchi2019/","section":"talk","summary":"Presented our work on the design of tangible devices for classroom orchestration.\n  ","tags":["cellulo","animatas","tip"],"title":"TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration","type":"talk"},{"authors":null,"categories":null,"content":"","date":1575072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575072000,"objectID":"dd6e6808c6069da4c1f5843f3cee6959","permalink":"https://wafajohal.github.io/post/hri2020_accepted/","publishdate":"2019-11-30T00:00:00Z","relpermalink":"/post/hri2020_accepted/","section":"post","summary":"","tags":["cowriter","handwriting","cowriting-kazakh"],"title":"[HRI2020 Paper Accepted!] on Transposing CoWriter to Learning the New Kazakh Alphabet","type":"post"},{"authors":null,"categories":null,"content":"I gave a short talk at the HRI Mini symposium on Orchestration of Robot Populated Classrooms, a recent work started with Sina Shahmoradi, Jauwairia Nasir and Utku Norman\n  ","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574380800,"objectID":"43b36a4337a5427eac7cbcc81284dba2","permalink":"https://wafajohal.github.io/talk/ucsandiego_hrimini2019/","publishdate":"2019-11-22T00:00:00Z","relpermalink":"/talk/ucsandiego_hrimini2019/","section":"talk","summary":"I gave a short talk at the HRI Mini symposium on Orchestration of Robot Populated Classrooms, a recent work started with Sina Shahmoradi, Jauwairia Nasir and Utku Norman\n  ","tags":["animatas","robots4learning","cellulo"],"title":"UC San Diego, California, USA, November 2019","type":"talk"},{"authors":null,"categories":null,"content":"","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573430400,"objectID":"13488728da856bd836c7860f1daff81c","permalink":"https://wafajohal.github.io/talk/2020_ucsd/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/talk/2020_ucsd/","section":"talk","summary":"","tags":null,"title":"Orchestration of robot-populated classrooms. UC Sand Diego, California, USA. November 2019.","type":"talk"},{"authors":null,"categories":null,"content":"I am hounoured that the 27th International Conference on Computers in Education awarded me with the Best Reviewer Award.\nhttp://ilt.nutn.edu.tw/icce2019/04_Best%20Reviewer%20Award.html\n","date":1571702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571702400,"objectID":"51a7092301dda9b3356940fb9759629f","permalink":"https://wafajohal.github.io/post/best_reviewer_icce2019/","publishdate":"2019-10-22T00:00:00Z","relpermalink":"/post/best_reviewer_icce2019/","section":"post","summary":"I am hounoured that the 27th International Conference on Computers in Education awarded me with the Best Reviewer Award.\nhttp://ilt.nutn.edu.tw/icce2019/04_Best%20Reviewer%20Award.html","tags":null,"title":"Best Reviewer Award for ICCE","type":"post"},{"authors":null,"categories":null,"content":"I was very glad to meet my colleagues and friends at RoMan. We presented four papers and enjoyed a day off in Jaipur.\n","date":1571011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571011200,"objectID":"f2f08ecc96b447fef557b17523f79ed3","permalink":"https://wafajohal.github.io/post/roman2019/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/post/roman2019/","section":"post","summary":"I was very glad to meet my colleagues and friends at RoMan. We presented four papers and enjoyed a day off in Jaipur.","tags":null,"title":"4 Full Papers Presented @RoMan 2019, New Delhi!","type":"post"},{"authors":null,"categories":null,"content":"I was very glad to meet my colleagues and friends virtually at RoMan2020.\n","date":1571011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571011200,"objectID":"624135077ff2d3fcb455154db7d0df9c","permalink":"https://wafajohal.github.io/post/allohaptic2020/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/post/allohaptic2020/","section":"post","summary":"I was very glad to meet my colleagues and friends virtually at RoMan2020.","tags":null,"title":"Paper accepted! Allohaptic","type":"post"},{"authors":["Wafa Johal","Olguta Robu","Amaury Dame","Stephane Magnenat","Francesco Mondada"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"424d8a36948decefec7ade90164788f0","permalink":"https://wafajohal.github.io/publication/johal-augmented-2019/","publishdate":"2020-03-09T04:27:29.927544Z","relpermalink":"/publication/johal-augmented-2019/","section":"publication","summary":"In recent years, robots have been surfing on a trendy wave as standard devices for teaching programming. The tangibility of robotics platforms allows for collaborative and interactive learning. Moreover, with these robot platforms, we also observe the occurrence of a shift of visual attention from the screen (on which the programming is done) to the physical environments (i.e. the robot). In this paper, we describe an experiment aiming at studying the effect of using augmented reality (AR) representations of sensor data in a robotic learning activity. We designed an AR system able to display in real-time the data of the Infra-Red sensors of the Thymio robot. In order to evaluate the impact of AR on the learner's understanding on how these sensors worked, we designed a pedagogical lesson that can run with or without the AR rendering. Two different age groups of students participated in this between-subject experiment, counting a total of 74 children. The tests were the same for the experimental (AR) and control group (no AR). The exercises differed only through the use of AR. Our results show that AR was worth being used for younger groups dealing with difficult concepts. We discuss our findings and propose future works to establish guidelines for designing AR robotic learning sessions.","tags":["AR rendering","AR robotic learning sessions","AR system","augmented reality","Augmented Reality","augmented reality representations","augmented robotics","collaborative learning","computer aided instruction","computer science education","control group","Education","educational robots","experimental group","human-robot interaction","infrared detectors","infrared sensors","interactive learning","interactive systems","Optics","programming","programming teaching","robot platforms","robotic learning activity","Robotics","robotics platforms","sensor data","standard devices","teaching","Thymio robot","trendy wave","visual attention"],"title":"Augmented Robotics for Learners: A Case Study on Optics","type":"publication"},{"authors":null,"categories":null,"content":"The iReCheck project submitted last April will be funded by the French ANR and the Swiss NSF. The goal of the project is to explore the learning by teaching sceanrio proposed in the cowriter project in with both performance and social adaptation, and with TD learners and learners with NDD. The project brings educational expertise of the EPFL team with the NDD expertise of the French partners to build a system that can suit all types of learners.\n","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568505600,"objectID":"cf4f04d62798a5e0cf0cdda3df514596","permalink":"https://wafajohal.github.io/post/irechech-accepted/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/post/irechech-accepted/","section":"post","summary":"The iReCheck project submitted last April will be funded by the French ANR and the Swiss NSF. The goal of the project is to explore the learning by teaching sceanrio proposed in the cowriter project in with both performance and social adaptation, and with TD learners and learners with NDD. The project brings educational expertise of the EPFL team with the NDD expertise of the French partners to build a system that can suit all types of learners.","tags":null,"title":"The iReCheck project accepted!","type":"post"},{"authors":null,"categories":null,"content":"You collected Gigabits of multi-modal data from an #HRI experiment? Check out our new paper on THRI proposing an analysis framework based on Newells' time scales. w T. Asselborn, K. Sharma \u0026amp;\n@dillenbo https://dl.acm.org/citation.cfm?id=3338809 #HRI2020 #humanrobotinteraction\n","date":1565913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565913600,"objectID":"0b3efbc615dc188307f8227e5c1bbd67","permalink":"https://wafajohal.github.io/post/framework-hri-analysis/","publishdate":"2019-08-16T00:00:00Z","relpermalink":"/post/framework-hri-analysis/","section":"post","summary":"You collected Gigabits of multi-modal data from an #HRI experiment? Check out our new paper on THRI proposing an analysis framework based on Newells' time scales. w T. Asselborn, K. Sharma \u0026amp;\n@dillenbo https://dl.acm.org/citation.cfm?id=3338809 #HRI2020 #humanrobotinteraction","tags":null,"title":"Bridging multi-level time scales for HRI analysis","type":"post"},{"authors":null,"categories":null,"content":"Officially starting at @UNSWEngineering today. Looking forward to this exciting adventure! Stay tuned, Phd job offers coming soon.\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"b5ad825354e4a25ef4c7274425cefdcf","permalink":"https://wafajohal.github.io/post/unsw-start/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/unsw-start/","section":"post","summary":"Officially starting at @UNSWEngineering today. Looking forward to this exciting adventure! Stay tuned, Phd job offers coming soon.","tags":null,"title":"First day at UNSW as Lecturer (Assistant Prof)","type":"post"},{"authors":null,"categories":null,"content":"Super excited about the presentation of my first design intern. It\u0026rsquo;s been a fruitful experience to work with him this past few months. Bravo Alex! #epfl #ecal\n","date":1561075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561075200,"objectID":"173d209958447246dc247373f4bcf2bc","permalink":"https://wafajohal.github.io/post/alex-defense/","publishdate":"2019-06-21T00:00:00Z","relpermalink":"/post/alex-defense/","section":"post","summary":"Super excited about the presentation of my first design intern. It\u0026rsquo;s been a fruitful experience to work with him this past few months. Bravo Alex! #epfl #ecal","tags":null,"title":"Alex's Bachelor Design Presentation","type":"post"},{"authors":null,"categories":null,"content":"The Find your Voice project (seed project, funded by the Jacobs Foundation and the SRCD) aims to investigate the potential use of voice assistant/robot to train children on joke telling to improve their self-confidence and language literacy.\nA whole week workshop ehlp us to clearly set objectives for the first part of teh project. We conducted a small literature review and discussed the mini projects to be conducted by Mai 2020.\nThe workshop started at UCLIC. Hosted by Prof. Yvonne Rogers, we visited the HCI facilities and the offices of the startup developping the Olly robot.\nAfter two days in London, we travelled to Brighton, on the sourthen coast of England, were we presented the FyV project to the members of the Chat Lab at Sussex Universty (lead by Prof. Nicola Yulli). The discussions and feedbacks were very interesting.\n","date":1557964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557964800,"objectID":"3f0a44867d9c005a3ee7829626fe4e7a","permalink":"https://wafajohal.github.io/post/fyv-ukmeeting/","publishdate":"2019-05-16T00:00:00Z","relpermalink":"/post/fyv-ukmeeting/","section":"post","summary":"The Find your Voice project (seed project, funded by the Jacobs Foundation and the SRCD) aims to investigate the potential use of voice assistant/robot to train children on joke telling to improve their self-confidence and language literacy.\nA whole week workshop ehlp us to clearly set objectives for the first part of teh project. We conducted a small literature review and discussed the mini projects to be conducted by Mai 2020.","tags":null,"title":"London/Brighton UK, One week workshop, Find you Voice project","type":"post"},{"authors":null,"categories":null,"content":"The TA Swiss (Swiss Technology Assessment Foundation) invited me to praticipate to a focus group about robotics as an expert in robots for education. I was glad to present an overview of ther research of robotics in education. The day ended with a very interesting panl in which many asptects such as ethics, and regulations were discussed.\nSome pictures of the workshop: https://www.ta-swiss.ch/en/projects/participative-ta/focus-robots/\nThe full report is available here https://www.ta-swiss.ch/Focus-Robots-Broschuere-fr-web.pdf (in french)\n","date":1556841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556841600,"objectID":"17284e032b52baff957dc6e3747b9ce9","permalink":"https://wafajohal.github.io/post/post_focustaswiss/","publishdate":"2019-05-03T00:00:00Z","relpermalink":"/post/post_focustaswiss/","section":"post","summary":"The TA Swiss (Swiss Technology Assessment Foundation) invited me to praticipate to a focus group about robotics as an expert in robots for education. I was glad to present an overview of ther research of robotics in education. The day ended with a very interesting panl in which many asptects such as ethics, and regulations were discussed.\nSome pictures of the workshop: https://www.ta-swiss.ch/en/projects/participative-ta/focus-robots/\nThe full report is available here https://www.ta-swiss.ch/Focus-Robots-Broschuere-fr-web.pdf (in french)","tags":null,"title":"Expert for the Focus group on Robots in Education","type":"post"},{"authors":null,"categories":null,"content":"  ","date":1556841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556841600,"objectID":"b6610c36bbe1ea2889ced70e34932b6b","permalink":"https://wafajohal.github.io/talk/bern2019/","publishdate":"2019-05-03T00:00:00Z","relpermalink":"/talk/bern2019/","section":"talk","summary":"  ","tags":null,"title":"TA-Swiss, Bern, Switzerland Mai 2019","type":"talk"},{"authors":null,"categories":null,"content":"22 participants attended the 4 days training event at EPFL on Developing Technologies in Educational Settings. The winter school recieved the support of the NCCR Robotcs and the Animatas H2020 EU project. We had invited great speakers and got excellent feedback from the participants. I specially wanted to thank Florence Colomb, Dr. Jennifer Olsen, Prof. Pierre Dillenourg, Dr. Catharine Oertel, Utku Norman, Jauwairia Nasir and Sina Shahmodari for helping me organising the event.\nThe official page of the winter school is here: https://chili.epfl.ch/digital_learning_ws/\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"e33dedde676217d9fa7d5c661e33f37a","permalink":"https://wafajohal.github.io/post/edtech-winterschool/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/edtech-winterschool/","section":"post","summary":"22 participants attended the 4 days training event at EPFL on Developing Technologies in Educational Settings. The winter school recieved the support of the NCCR Robotcs and the Animatas H2020 EU project. We had invited great speakers and got excellent feedback from the participants. I specially wanted to thank Florence Colomb, Dr. Jennifer Olsen, Prof. Pierre Dillenourg, Dr. Catharine Oertel, Utku Norman, Jauwairia Nasir and Sina Shahmodari for helping me organising the event.","tags":["robots4learning"],"title":"Organising a Winter School on Developing Technologies in Educational Settings","type":"post"},{"authors":null,"categories":null,"content":"","date":1548633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548633600,"objectID":"e508ae26e44997ce12f0b909d95a66cc","permalink":"https://wafajohal.github.io/talk/amld2018/","publishdate":"2019-01-28T00:00:00Z","relpermalink":"/talk/amld2018/","section":"talk","summary":"","tags":null,"title":"Applied Machine Learning Days, Lausanne Switzerland, January 2019","type":"talk"},{"authors":null,"categories":null,"content":"","date":1547164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547164800,"objectID":"f2430f684d0e0ecf00b4052aa612dfeb","permalink":"https://wafajohal.github.io/talk/astana2019/","publishdate":"2019-01-11T00:00:00Z","relpermalink":"/talk/astana2019/","section":"talk","summary":"","tags":null,"title":"Astana, Kazakhstan, January 2019","type":"talk"},{"authors":null,"categories":null,"content":"Thibault Assselborn and I travelled to Astana, Kazakhstan to install CoWriter with Anara. We discussed learning sceanrios to help Kazakh students learn the new Latin Kazakh alphabet.\nIt was definitly a fruitful visit, but the weather was not ideal for us to sightsee (-40¬∞C).\n","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"70f5aae118bfb04461799cf024e5677f","permalink":"https://wafajohal.github.io/post/visit-cokaz/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/post/visit-cokaz/","section":"post","summary":"Thibault Assselborn and I travelled to Astana, Kazakhstan to install CoWriter with Anara. We discussed learning sceanrios to help Kazakh students learn the new Latin Kazakh alphabet.\nIt was definitly a fruitful visit, but the weather was not ideal for us to sightsee (-40¬∞C).","tags":["cowriting_kazakh"],"title":"‚úàÔ∏è üá∞üáø  Astana! Visiting our partner for the CoWriting Kazakh project","type":"post"},{"authors":["Thibault Asselborn","Kshitij Sharma","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"9c8942742600cb76c8e583246de9d4f1","permalink":"https://wafajohal.github.io/publication/asselborn-2019-bridging/","publishdate":"2019-09-22T11:07:11.716669Z","relpermalink":"/publication/asselborn-2019-bridging/","section":"publication","summary":"In this article, we present a multi-level time scales framework for the analysis of human-robot interaction (HRI). Such a framework allows HRI scientists to model the inter-relation between measures and factors of an experiment. Our final goal with the introduction of this framework is to unify scientific practice in the HRI community for better reproducibility. Our new approach transposes Newell‚Äôs framework of human actions to model human-robot interaction. Measures from the interaction are sorted into categories (time scales) corresponding to the temporal constraints proposed by Newell. According to this sorting, a bottom-up or top-down analysis can then be performed to correlate variables which allows a better understanding and explanation of the interaction. The utilization of our method within two experimental use cases is then presented. The first one, a child-robot interaction, involves two robots and one child playing a memory game. The second is based on an analysis of the PInSoRo dataset, involving 30 child-robot pairs in a freeplay interaction. Finally, we introduce clear guidelines to re-use the framework.","tags":null,"title":"Bridging Multilevel Time Scales in HRI: An Analysis Framework","type":"publication"},{"authors":["Anton Kim","Meruyert Omarova","Adil Zhaksylyk","Thibault Asselborn","Wafa Johal","Pierre Dillenbourg","Anara Sandygulova"],"categories":null,"content":"   ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e10a4f9d1b2129ca2af46668088d6671","permalink":"https://wafajohal.github.io/publication/kim-cowriting-2019/","publishdate":"2020-03-09T03:42:41.108198Z","relpermalink":"/publication/kim-cowriting-2019/","section":"publication","summary":"In the Republic of Kazakhstan, the transition from Cyrillic towards Latin alphabet raises challenges to teach the whole population in writing the new script. This paper presents a CoWriting Kazakh system that aims to implement an autonomous behavior of a social robot that would assist children in learning a new script. Considering the fact that the current generation of primary school children have to be fluent in both Kazakh scripts, this exploratory study aims to investigate which learning approach provides better effect. Participants were asked to teach a humanoid robot NAO how to write Kazakh words using one of the scripts, Latin vs Cyrillic. We hypothesize that it is more effective when a child mentally converts the word to Latin in comparison to having the robot perform conversion itself. The findings reject this hypothesis, but further research is needed as it is suggested that the way the pre-test was performed might have caused the obtained results.","tags":["cowriting_kazakh"],"title":"CoWriting Kazakh: Transitioning to a New Latin Script using Social Robots","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Maximilian J Wessel","Thibault Asselborn","Jennifer K Olsen","Wafa Johal","Ayberk Ozgur","Friedhelm C Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7bf11835c181ed178e041baf7c0e21bd","permalink":"https://wafajohal.github.io/publication/ozgur-designing-2019/","publishdate":"2020-03-09T03:42:41.108477Z","relpermalink":"/publication/ozgur-designing-2019/","section":"publication","summary":"For successful rehabilitation of a patient after a stroke or traumatic brain injury, it is crucial that rehabilitation activities are motivating, provide feedback and have a high rate of repetitions. Advancements in recent technologies provide solutions to address these aspects where needed. Additionally, through the use of gamification, we are able to increase the motivation for participants. However, many of these systems require complex set-ups, which can be a big challenge when conducting rehabilitation in a home-based setting. To address the lack of simple rehabilitation tools for arm function for a home-based application, we previously developed a system, Cellulo for rehabilitation, that is comprised of paper-supported tangible robots that are orchestrated by applications deployed on consumer tablets. These components enable different features that allow for gamification, easy setup, portability, and scalability. To support the configuration of game elements to patients' level of motor skills and strategies, their motor trajectories need to be classified. In this paper, we investigate the classification of different motor trajectories and how game elements impact these in unimpaired, healthy participants. We show that the manipulation of certain game elements do have an impact on motor trajectories, which might indicate that it is possible to adapt the arm remediation of patients by configuring game elements. These results provide a first step towards providing adaptive rehabilitation based upon patients' measured trajectories.","tags":null,"title":"Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories?","type":"publication"},{"authors":["Laila El Hamamsy","Wafa Johal","Thibault Asselborn","Jauwairia Nasir","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e518def071be9c71aa150bc89291eef0","permalink":"https://wafajohal.github.io/publication/el-hamamsy-learning-2019/","publishdate":"2019-10-09T03:42:41.108908Z","relpermalink":"/publication/el-hamamsy-learning-2019/","section":"publication","summary":"This paper presents the design of a novel and engaging collaborative learning activity for handwriting where a group of participants simultaneously tutor a Nao robot. This activity was intended to take advantage of both collaborative learning and the learning by teaching paradigm to improve children‚Äôs meta-cognition (perception of their own skills). Multiple engagement probes were integrated into the activity as a first step towards fostering long term interactions. As a lot of research targets social interactions, the goal here was to determine whether an engagement strategy focused on the task could be as, or more efficient than one focused on social interactions and participants‚Äô introspection. To that effect, two engagement strategies were implemented. They differed in content but used the same multi-modal design in order to increase participants‚Äô meta-cognitive reflection, once on the task and performances, and once on participants‚Äô enjoyment and emotions. Both strategies were compared to a baseline by probing and assessing engagement at the individual and group level, along the behavioural, emotional and cognitive dimensions, in a between subject experiment with 12 groups of children. The experiments showed that the collaborative task pushed the children to adapt their manner of writing to the group, even though the adopted solution was not always correct. Furthermore, there was no significant difference between the strategies in terms of behaviour on task (behavioural engagement), satisfaction (emotional engagement) or performance (cognitive engagement) as the group dynamics had a stronger impact on the outcome of the collaborative teaching task. Therefore, the task and social engagement strategies can be considered as efficient in the context of collaboration.","tags":null,"title":"Learning By Collaborative Teaching: An Engaging Multi-Party CoWriter Activity","type":"publication"},{"authors":["Wafa Johal","Sonia Andersen","Morgane Chevalier","Ayberk Ozgur","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"91cbca55285a57db36853cae1bffec9a","permalink":"https://wafajohal.github.io/publication/johal-2019-learning/","publishdate":"2019-09-22T11:07:11.716394Z","relpermalink":"/publication/johal-2019-learning/","section":"publication","summary":"Robots bring a new potential for embodied learning in classrooms. With our project, we aim to ease the task for teachers and to show the worth of tangible manipulation of robots in educational contexts. In this article, we present the design and the evaluation of two pedagogical activities prepared for a primary school teacher and targeting common misconceptions when learning reflective symmetry. The evaluation consisted of a comparison of remedial actions using haptic-enabled tangible robots with using regular geometrical tools in practical sessions. Sixteen 10 y.o. students participated in a between-subject experiment in a public school. We show that this training with the tangible robots helped the remediation of parallelism and perpendicularity related mistakes commonly made by students.  Our findings also suggest that the haptic modality of interaction is well suited to promote children‚Äôs abstraction of geometrical concepts from spatial representations.","tags":null,"title":"Learning Symmetry with Tangible Robots","type":"publication"},{"authors":["Ayberk √ñzg√ºr","S√©verin Lemaignan","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"263ad18439c7f7b051b5c33fdc36d9f4","permalink":"https://wafajohal.github.io/publication/ozgur-2019-magnet/","publishdate":"2019-09-22T11:07:11.715019Z","relpermalink":"/publication/ozgur-2019-magnet/","section":"publication","summary":"","tags":null,"title":"Magnet-assisted ball drive","type":"publication"},{"authors":["Sina Shahmoradi","Jennifer K Olsen","Stian Haklev","Wafa Johal","Utku Norman","Jauwairia Nasir","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"71297461d162720b6f3685131bbe091b","permalink":"https://wafajohal.github.io/publication/shahmoradi-2019-orchestration/","publishdate":"2019-09-22T11:07:11.717256Z","relpermalink":"/publication/shahmoradi-2019-orchestration/","section":"publication","summary":"Bringing robots into classrooms presents a new set of challenges for classroom management and teacher support compared to traditional technology-enhanced learning and has been left almost unexplored by the research community. In this paper, we present the opportunities and challenges of orchestrating Educational Robotics (ER) activities in classrooms. To support our discussion, we present a case study of 25 students working in pairs using handheld robots to engage in a computational thinking activity. While performing the activity, students‚Äô behavioral information was sent from the robots to an orchestration dashboard that was used in a debriefing activity. Although this work is in its preliminary stages, it contributes to framing the challenges that need to be addressed to realistically scale-up usage of ER in classrooms.","tags":null,"title":"Orchestration of Robotic Activities in Classrooms: Challenges and Opportunities","type":"publication"},{"authors":["Thibault Asselborn","Thomas Gargot","Lukasz Kidzinski","Wafa Johal","David Cohen","Caroline Jolly","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d7f0f339d199f344a651edfc99c2586f","permalink":"https://wafajohal.github.io/publication/asselborn-2019-reply/","publishdate":"2019-09-22T11:07:11.71552Z","relpermalink":"/publication/asselborn-2019-reply/","section":"publication","summary":"","tags":null,"title":"Reply: Limitations in the creation of an automatic diagnosis tool for dysgraphia","type":"publication"},{"authors":["Jauwairia Nasir","Utku Norman","Wafa Johal","Jennifer Kaitlyn Olsen","Sina Shahmoradi","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8ee984ecafae9dcb65f01c8877d8b631","permalink":"https://wafajohal.github.io/publication/nasir-2019-robot/","publishdate":"2019-09-22T11:07:11.716993Z","relpermalink":"/publication/nasir-2019-robot/","section":"publication","summary":"In this paper, we propose that the data generated by educational robots can be better used by applying learning analytics methods and techniques which can lead to a deeper understanding of the learners‚Äô apprehension and behavior as well as refined guidelines for roboticists and improved interventions by the teachers. As a step towards this, we put forward analyzing behavior and task performance at team and/or individual levels by coupling robot data with the data from conventional methods of assessment through quizzes. Classifying learners/teams in the behavioral feature space with respect to the task performance gives insight into the behavior patterns relevant for high performance, which could be backed by feature ranking. As a use case, we present an open-ended learning activity using tangible haptic-enabled Cellulo robots in a classroom-level setting. The pilot study, spanning over approximately an hour, is conducted with 25 children in teams of two that are aged between 11-12. A linear separation is observed between the high and low performing teams where two of the behavioral features, namely number of distinct attempts and the visits to the destination, are found to be important. Although the pilot study in its current form has limitations, e.g. its low sample size, it contributes to highlighting the potential of the use of learning analytics in educational robotics.","tags":null,"title":"Robot Analytics: What Do Human-Robot Interaction Traces Tell Us About Learning?","type":"publication"},{"authors":["Wafa Johal","Anara Sandygulova","Jan De Wit","Mirjam De Haas","Brian Scassellati"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e4092c40a199788b4cf48c9571fe2bc4","permalink":"https://wafajohal.github.io/publication/johal-2019-robots/","publishdate":"2019-09-22T11:07:11.715294Z","relpermalink":"/publication/johal-2019-robots/","section":"publication","summary":"","tags":null,"title":"Robots for Learning-R4L: Adaptive Learning","type":"publication"},{"authors":["Elmira Yadollahi","Wafa Johal","Joao Dias","Pierre Dillenbourg","Ana Paiva"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f567776878948bb883a897f551308091","permalink":"https://wafajohal.github.io/publication/yadollahi-studying-2019/","publishdate":"2020-03-09T03:42:41.10912Z","relpermalink":"/publication/yadollahi-studying-2019/","section":"publication","summary":"The use of robots as peers is more and more studied in human-robot interaction with co-learning interactions being complex and rich involving cognitive, affective, verbal and non-verbal processes. We aim to study the co-learning interaction with robots in the light of perspective-taking; a cognitive dimension that is important for interaction, engagement, and learning of the child. This work-in-progress details one of the studies we are developing in understanding perspective-taking from the Piagetian point of view. The study tried to understand how changes in the robot's cognitive-affective state affect children's behavior, emotional state, and perception of the robot. The experiment details a scenario in which child and the robot take turn to play a game by instructing their counterpart to reach a goal. The interaction consists of a condition in which the robot expresses frustration when the child gives egocentric instructions. We manipulate the robot's emotional responses to the child's instructions as our independent variable. We hypothesize that children will try to change their perspective more when the robot expresses frustration and follow the instructions wrongly, e.g. does not understand their perspective. Moreover, in the frustration groups, we are interested to observe if children reciprocates the robot's behavior by showing frustration to the robot if it is egocentric. Consequently, we expect our analyses to help us to integrate a perspective-taking model in our robotic platform that can adapt its perspective according to educational or social aspect of the interaction.","tags":null,"title":"Studying the Effect of Robot Frustration on Children's Change of Perspective","type":"publication"},{"authors":["Konrad Zolna","Thibault Asselborn","Caroline Jolly","Laurence Casteran","Wafa Johal","Pierre Dillenbourg"," others"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ba90b070c7f02f999cc6183a2b854929","permalink":"https://wafajohal.github.io/publication/zolna-2019-dynamics/","publishdate":"2019-09-22T11:07:11.715786Z","relpermalink":"/publication/zolna-2019-dynamics/","section":"publication","summary":"Handwriting disorder (termed dysgraphia) is a far from a singular problem as nearly 8.6% of the population in France is considered dysgraphic. Moreover, research highlights the fundamental importance to detect and remediate these handwriting difficulties as soon as possible as they may affect a child's entire life, undermining performance and self-confidence in a wide variety of school activities. At the moment, the detection of handwriting difficulties is performed through a standard test called BHK. This detection, performed by therapists, is laborious because of its high cost and subjectivity. We present a digital approach to identify and characterize handwriting difficulties via a Recurrent Neural Network model (RNN). The child under investigation is asked to write on a graphics tablet all the letters of the alphabet as well as the ten digits. Once complete, the RNN delivers a diagnosis in a few milliseconds and demonstrates remarkable efficiency as it correctly identifies more than 90% of children diagnosed as dysgraphic using the BHK test. The main advantage of our tablet-based system is that it captures the dynamic features of writing -- something a human expert, such as a teacher, is unable to do. We show that incorporating the dynamic information available by the use of tablet is highly beneficial to our digital test to discriminate between typically-developing and dysgraphic children.","tags":null,"title":"The Dynamics of Handwriting Improves the Automated Diagnosis of Dysgraphia","type":"publication"},{"authors":["Wafa Johal","Alex Tran","Hala Khodr","Aykerk Ozgur","Pierre Dillenbourg"],"categories":null,"content":"  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"14ebcbb029697ef75189bfc88c8bd127","permalink":"https://wafajohal.github.io/publication/johal-2019-tip/","publishdate":"2019-12-12T10:12:15.828976Z","relpermalink":"/publication/johal-2019-tip/","section":"publication","summary":"While digital tools are more and more used in classrooms, teachers‚Äô common practice remain to use photocopied documents to share and collect learning exercises from their students. With TIP (Tangible e-Ink Paper) we aim to explore the use of tangible manipulatives to interact with paper sheets and bridge between digital and paper traces of learning tasks. Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs is envisioned to be used as a versatile tool across various curriculum activities. In this paper, we present the design principle of TIPs and a first functional prototype. We conclude by future work in evaluating TIPs as a distributed sensor for teacher in their classroom, including learning scenario examples to illustrate our statements. ","tags":null,"title":"TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Louis P Faucon","Pablo Maceira-Elvira","Maximilian J Wessel","Wafa Johal","Ayberk √ñzg√ºr","And√©ol Cadic-Melchior","Friedhelm C Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"59423486f13bfece013522077ea6808d","permalink":"https://wafajohal.github.io/publication/ozgur-2019-towards/","publishdate":"2019-09-22T11:07:11.716077Z","relpermalink":"/publication/ozgur-2019-towards/","section":"publication","summary":"A key feature of a successful game is its ability to provide the player with an adequate level of challenge. However, the objective of difficulty adaptation in serious games is not only to maintain the player‚Äôs motivation by challenging, but also to ensure the completion of training objectives. This paper describes our proposed upper-limb rehabilitation game with tangible robots and investigates the effect of game elements and gameplay on the amount of the performed motion in several planes and percentage of failure by using the data from 33 unimpaired subjects who played 53 games within two consecutive days. In order to provide a more generic adaptation strategy in the future, we discretize the game area to circular zones. We then show the effect of changing these zones during gameplay on the activation of different muscles through EMG data in a pilot study. The study shows that it is possible to increase the challenge level by adding more active agents chasing the player and increasing the speed of these agents. However, only the increase in number of agents significantly increases the users‚Äô motion on both planes. Analysis of player behaviors leads us to suggest that by adapting the behaviour of these active agents in specific zones, it is possible to change the trajectory of the user, and to provide a focus on the activation of specific muscles.","tags":null,"title":"Towards an Adaptive Upper Limb Rehabilitation Game with Tangible Robots","type":"publication"},{"authors":null,"categories":null,"content":"","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"042f6b77b0c6b6fe99d886bad461dc1f","permalink":"https://wafajohal.github.io/talk/ethz2018/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/talk/ethz2018/","section":"talk","summary":"","tags":null,"title":"ETH, Zurich, Switzerland, December 2018","type":"talk"},{"authors":null,"categories":null,"content":"","date":1542412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542412800,"objectID":"3ea6da5e1f21653af385b1fb030fe1cf","permalink":"https://wafajohal.github.io/talk/hris2018/","publishdate":"2018-11-17T00:00:00Z","relpermalink":"/talk/hris2018/","section":"talk","summary":"","tags":null,"title":"Human-Robot Interaction Mini Symposium,  Standford, November 2018","type":"talk"},{"authors":null,"categories":null,"content":"Visiting Lukasc Kidnzinski at Stanford, I gave a talk on our paper on diagnosis of disgraphia using RF applied to a series of novel features based on pen\u0026rsquo;s postion, pressure and tilt.\n","date":1542412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542412800,"objectID":"c5b026b400b3ee51e5f71ab5ad8426c5","permalink":"https://wafajohal.github.io/talk/standford2018/","publishdate":"2018-11-17T00:00:00Z","relpermalink":"/talk/standford2018/","section":"talk","summary":"Visiting Lukasc Kidnzinski at Stanford, I gave a talk on our paper on diagnosis of disgraphia using RF applied to a series of novel features based on pen\u0026rsquo;s postion, pressure and tilt.","tags":null,"title":"Standford, November 2018","type":"talk"},{"authors":null,"categories":null,"content":"Teaching children to write using robots with haptic feedback. I gave a small talk on our IDC paper [?] using the Cellulo robots for a handwriting activity.\n","date":1542412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542412800,"objectID":"b1cff5181033e1d4efcbc41d7e015954","permalink":"https://wafajohal.github.io/post/talk-hris2018/","publishdate":"2018-11-17T00:00:00Z","relpermalink":"/post/talk-hris2018/","section":"post","summary":"Teaching children to write using robots with haptic feedback. I gave a small talk on our IDC paper [?] using the Cellulo robots for a handwriting activity.","tags":null,"title":"Talk at the HRI 2018 Mini Symposium, Standford CA","type":"post"},{"authors":null,"categories":null,"content":"","date":1541980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541980800,"objectID":"37ccf7c6259ad777429b29cb0e2f65ee","permalink":"https://wafajohal.github.io/talk/entre2lac/","publishdate":"2018-11-12T00:00:00Z","relpermalink":"/talk/entre2lac/","section":"talk","summary":"","tags":null,"title":"Teacher symposium on Computational Thinking, Yverdon-les-Bains, Switzerland, November 2018","type":"talk"},{"authors":null,"categories":null,"content":"Pierre Dillenbour and I wonder \u0026ldquo;What are robots doing in schools?\u0026rdquo; in a blog post on Bold : https://bold.expert/what-are-robots-doing-in-schools/\n","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"e2ecd4a315c21440b54d0a935dd49788","permalink":"https://wafajohal.github.io/post/writingbold/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/writingbold/","section":"post","summary":"Pierre Dillenbour and I wonder \u0026ldquo;What are robots doing in schools?\u0026rdquo; in a blog post on Bold : https://bold.expert/what-are-robots-doing-in-schools/","tags":null,"title":"Bold Blog Article on R4L is out","type":"post"},{"authors":null,"categories":null,"content":"The CHILI Lab presented several demo at the Swiss Converntion Center. Always a great opportunity to meet with colleagues from the NCCR and robotics actor of the reagion.\n","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"12fdc4a16727a7923d8df241eeb936e7","permalink":"https://wafajohal.github.io/post/nccr_id2019/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/nccr_id2019/","section":"post","summary":"The CHILI Lab presented several demo at the Swiss Converntion Center. Always a great opportunity to meet with colleagues from the NCCR and robotics actor of the reagion.","tags":null,"title":"Demos at NCCR Robotics Industry Days","type":"post"},{"authors":null,"categories":null,"content":"","date":1541289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541289600,"objectID":"f2676f9213d8c8ffe67dd69cf424feb6","permalink":"https://wafajohal.github.io/talk/unsw2018/","publishdate":"2018-11-04T00:00:00Z","relpermalink":"/talk/unsw2018/","section":"talk","summary":"","tags":null,"title":"UNSW, November 2018","type":"talk"},{"authors":null,"categories":null,"content":"","date":1539734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539734400,"objectID":"c8c4885ce360309519459075f329e8f5","permalink":"https://wafajohal.github.io/talk/rome2018/","publishdate":"2018-10-17T00:00:00Z","relpermalink":"/talk/rome2018/","section":"talk","summary":"","tags":null,"title":"Workshop on Swarm Robotics, Rome, October 2018","type":"talk"},{"authors":null,"categories":null,"content":"This last weekend, we had a booth at the Robotic Showcase of the EPFL Drone Days. In the summer, we received 80 young girls who were doing a coding camp on campus for demos at the CHILI Lab. I also participate to other outreach activities with the NCCR Robotics (Swiss Industry Days) or with the Service for Promoting Sciences ‚Äì SPS at EPFL by being interviewed by small girls on my job as researcher in robotics.\nDuring these events we could present some learning activities that we developed for children and explain our research goals. This kind of events are very tiring, people come and go and are often impatient to test. It can occur during the weekend and we can often feel that we could be working on something more valuable for our research instead and it is a waste of time. But I do think that these events help us as researchers. First, it shows the face of scientists to the public, they can talk to us. As a scientist you also hear the concerns that people have with some of your work and it is interesting to try to reassure the public and take into account these ethical concerns. Robotics has had a bad image in the society these past decades, as the 4th industrial revolution announces that many people will loose their jobs. I personally work on assistive robotics and I want to show the public that robots could be beneficial for their life or for society and that we are researching for what applications they can be an added value. You also want to show the limits of AI and Robotics system. We sometimes make our work sound fantastic when writing papers (the limitation section being a small paragraph at the end of the paper), but when running experiments / demo in the wild, you see the limits of autonomous systems and you show the public that you still need a human to be around to interact with the robots. I also helps me to network. I often do user studies and need to find participants for it. I have presented a recruitment sheet during this kind of events and collected contact of people willing to participate to my next experiment. At least once during the showcase, you will meet this guy (more often a guy but could be a woman) that will want to debate with you. This guy is often affiliated to the field, he knows about cs and robotics, he is retired and thinks that we are now going too far. I usually take my time with these people, try to show and explain that we are doing to research and testing new things, that there is a lot of room for improvement, and that I appreciate his feedback. A lot of people think like him and it is interesting as a scientist and developer to build your argumentation for this kind of debate.\n","date":1536883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536883200,"objectID":"15dd02b6111ffa300ae7e62d45000f61","permalink":"https://wafajohal.github.io/post/post_outreach/","publishdate":"2018-09-14T00:00:00Z","relpermalink":"/post/post_outreach/","section":"post","summary":"This last weekend, we had a booth at the Robotic Showcase of the EPFL Drone Days. In the summer, we received 80 young girls who were doing a coding camp on campus for demos at the CHILI Lab. I also participate to other outreach activities with the NCCR Robotics (Swiss Industry Days) or with the Service for Promoting Sciences ‚Äì SPS at EPFL by being interviewed by small girls on my job as researcher in robotics.","tags":null,"title":"Outreach activities","type":"post"},{"authors":null,"categories":null,"content":"We got a new grant for travel mainly that will enable me to collaborate more with Anara Sandygulova from Nazarbayev University. As Kazakhstan is planning to move from Cyrillic to Roman alphabet, we are planning to use the Cowriter setup to motivate children to learn the new alphabet. This work will also continue our work on dysgraphia by collecting handwriting data samples from children in the two alphabet. The goal will be to discover features that are independent of the alphabet to characterize handwriting skills. Excited for my first visit to Kazakhstan. üôÇ\n","date":1536883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536883200,"objectID":"d99c0ca665f798e5e5d1ad102ce8a3d9","permalink":"https://wafajohal.github.io/post/post_cokazakh/","publishdate":"2018-09-14T00:00:00Z","relpermalink":"/post/post_cokazakh/","section":"post","summary":"We got a new grant for travel mainly that will enable me to collaborate more with Anara Sandygulova from Nazarbayev University. As Kazakhstan is planning to move from Cyrillic to Roman alphabet, we are planning to use the Cowriter setup to motivate children to learn the new alphabet. This work will also continue our work on dysgraphia by collecting handwriting data samples from children in the two alphabet. The goal will be to discover features that are independent of the alphabet to characterize handwriting skills.","tags":["cowriting_kazakh"],"title":"Project CoWriting Kazakh Accepted!","type":"post"},{"authors":null,"categories":null,"content":"Computational Thinking, Kezako? Computational Thinking such a buzz word these days. I got more and more interested by CT when I saw that there was so much debate on its definition and utility to be taught in school. I recently submitted a proposal to get funding for a project dealing with CT but I will not say much more now as it is being reviewed. In July, we received the visit of my good friend Anara from Kazakhstan, who stayed at the lab for a month; just enough to design an experiment and start the development. To get some inspiration we browsed and got some great ideas from the CSunplugged website : https://csunplugged.org/en/ We chose to work on Sorting Networks.\nIf you are an CS engineer you are very familiar with the classical sorting algorithms such as Insertion Sort, Quick Sort and Bubble Sort (Obama‚Äôs favorite) but maybe less familiar with the Sorting Networks. Sorting is everywhere and you use sorting machines all the time, each time you do a Google search, as Google is sorting the search results for you to get the most relevant one on the top of the list. This sorting by relevance can be done in multiple ways. Sorting Networks is one of the ways that as the advantage of allowing for parallel processing of the sorting task. In sorting networks, the only operation applied is comparison. Each thread compares elements of the network two by two and tag them as the higher or lower element of the two. Sorting networks can hence be applied to any kind of element as long as they can be compared (numbers, letters, etc). Another advantage of Sorting Networks is that the number of operation (and hence the execution time of the sorting) can be calculated from the just knowing the number of elements to be sorted (the shape of the network is determined at that time also).\n","date":1533340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533340800,"objectID":"3bc46891aebeb09e80bf02d91a9e2337","permalink":"https://wafajohal.github.io/post/compthining/","publishdate":"2018-08-04T00:00:00Z","relpermalink":"/post/compthining/","section":"post","summary":"Computational Thinking, Kezako? Computational Thinking such a buzz word these days. I got more and more interested by CT when I saw that there was so much debate on its definition and utility to be taught in school. I recently submitted a proposal to get funding for a project dealing with CT but I will not say much more now as it is being reviewed. In July, we received the visit of my good friend Anara from Kazakhstan, who stayed at the lab for a month; just enough to design an experiment and start the development.","tags":null,"title":"Working on a Computational Thinking application","type":"post"},{"authors":null,"categories":null,"content":"Awarded in #IDC2018 for our study on Deictic Gestures in a Reading Learning by Teaching scenario with a Nao. Checkout the paper https://dl.acm.org/citation.cfm?id=3202743 . Bravo @ElmiraYadollahi Ana Paiva and @dillenbo #CHILI @nccrrobotics\n","date":1529625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529625600,"objectID":"f174163b7935524c28203ee41da69c59","permalink":"https://wafajohal.github.io/post/bpa_idc2018/","publishdate":"2018-06-22T00:00:00Z","relpermalink":"/post/bpa_idc2018/","section":"post","summary":"Awarded in #IDC2018 for our study on Deictic Gestures in a Reading Learning by Teaching scenario with a Nao. Checkout the paper https://dl.acm.org/citation.cfm?id=3202743 . Bravo @ElmiraYadollahi Ana Paiva and @dillenbo #CHILI @nccrrobotics","tags":null,"title":"Honorable Mention Award at IDC2018!","type":"post"},{"authors":["Thibault Asselborn","Thomas Gargot","Lukasz Kidzinski","Wafa Johal","David Cohen","Caroline Jolly","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"030cceb3757b91071046240d889aa335","permalink":"https://wafajohal.github.io/publication/asselborn-2018-automated/","publishdate":"2019-09-22T00:00:00Z","relpermalink":"/publication/asselborn-2018-automated/","section":"publication","summary":"The academic and behavioral progress of children is associated with the timely development of reading and writing skills. Dysgraphia, characterized as a handwriting learning disability, is usually associated with dyslexia, developmental coordination disorder (dyspraxia), or attention deficit disorder, which are all neuro-developmental disorders. Dysgraphia can seriously impair children in their everyday life and require therapeutic care. Early detection of handwriting difficulties is, therefore, of great importance in pediatrics. Since the beginning of the 20th century, numerous handwriting scales have been developed to assess the quality of handwriting. However, these tests usually involve an expert investigating visually sentences written by a subject on paper, and, therefore, they are subjective, expensive, and scale poorly. Moreover, they ignore potentially important characteristics of motor control such as writing dynamics, pen pressure, or pen tilt. However, with the increasing availability of digital tablets, features to measure these ignored characteristics are now potentially available at scale and very low cost. In this work, we developed a diagnostic tool requiring only a commodity tablet. To this end, we modeled data of 298 children, including 56 with dysgraphia. Children performed the BHK test on a digital tablet covered with a sheet of paper. We extracted 53 handwriting features describing various aspects of handwriting, and used the Random Forest classifier to diagnose dysgraphia. Our method achieved 96.6% sensibility and 99.2% specificity. Given the intra-rater and inter-rater levels of agreement in the BHK test, our technique has comparable accuracy for experts and can be deployed directly as a diagnostics tool.","tags":null,"title":"Automated human-level diagnosis of dysgraphia using a consumer tablet","type":"publication"},{"authors":["Thibault Asselborn","Arzu Guneysu","Khalil Mrini","Elmira Yadollahi","Ayberk Ozgur","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"a6cab791bd4c3281eff1ded37f6d97ca","permalink":"https://wafajohal.github.io/publication/asselborn-2018-bringing/","publishdate":"2019-09-22T10:59:49.226312Z","relpermalink":"/publication/asselborn-2018-bringing/","section":"publication","summary":" In this paper, we present a robotic approach to improve the teaching of handwriting using the tangible, haptic-enabled and classroom-friendly Cellulo robots. Our efforts presented here are in line with the philosophy of the Cellulo platform: we aim to create a ready-to-use tool (i.e. a set of robot-assisted activities) to be used for teaching handwriting, one that is to coexist harmoniously with traditional tools and will contribute new added values to the learning process, complementing existing teaching practices.\nTo maximize our potential contributions to this learning process, we focus on two promising aspects of handwriting: the visual perception and the visual-motor coordination. These two aspects enhance in particular two sides of the representation of letters in the mind of the learner: the shape of the letter (the grapheme) and the way it is drawn, namely the dynamics of the letter (the ductus).\nWith these two aspects in mind, we do a detailed content analysis for the process of learning the representation of letters, which leads us to discriminate the specific skills involved in letter representation. We then compare our robotic method with traditional methods as well as with the combination of the two methods, in order to discover which of these skills can benefit from the use of Cellulo.\nAs handwriting is taught from age 5, we conducted our experiments with 17 five-year-old children in a public school. Results show a clear potential of our robot-assisted learning activities, with a visible improvement in certain skills of handwriting, most notably in creating the ductus of the letters, discriminating a letter among others and in the average handwriting speed.\nMoreover, we show that the benefit of our learning activities to the handwriting process increases when it is used after traditional learning methods. These results lead to the initial insights into how such a tangible robotic learning technology may be used to create cost-effective collaborative scenarios for the learning of handwriting. ","tags":null,"title":"Bringing letters to life: handwriting with haptic-enabled tangible robots","type":"publication"},{"authors":["Ayberk √ñzg√ºr","Wafa Johal","Arzu G√ºneysu √ñzg√ºr","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"131617ddeaf7d3212b31145a52ea8dc6","permalink":"https://wafajohal.github.io/publication/ozgur-2018-declarative/","publishdate":"2019-09-22T11:07:11.714798Z","relpermalink":"/publication/ozgur-2018-declarative/","section":"publication","summary":"","tags":null,"title":"Declarative Physicomimetics for Tangible Swarm Application Development","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Maximilian Jonas Wessel","Wafa Johal","Kshitij Sharma","Ayberk √ñzg√ºr","Philippe Vuadens","Francesco Mondada","Friedhelm Christoph Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"e1e5778df3538110182a3cf92a3145ad","permalink":"https://wafajohal.github.io/publication/guneysu-2018-iterative/","publishdate":"2019-09-22T10:51:36.52784Z","relpermalink":"/publication/guneysu-2018-iterative/","section":"publication","summary":"Rehabilitation aims to ameliorate deficits in motor control via intensive practice with the affected limb. Current strategies, such as one-on-one therapy done in rehabilitation centers, have limitations such as treatment frequency and intensity, cost and requirement of mobility. Thus, a promising strategy is home-based therapy that includes task specific exercises. However, traditional rehabilitation tasks may frustrate the patient due to their repetitive nature and may result in lack of motivation and poor rehabilitation. In this article, we propose the design and verification of an effective upper extremity rehabilitation game with a tangible robotic platform named Cellulo as a novel solution to these issues. We first describe the process of determining the design rationales to tune speed, accuracy and challenge. Then we detail our iterative participatory design process and test sessions conducted with the help of stroke, brachial plexus and cerebral palsy patients (18 in total) and 7 therapists in 4 different therapy centers. We present the initial quantitative results, which support several aspects of our design rationales and conclude with our future study plans.","tags":null,"title":"Iterative design of an upper limb rehabilitation game with tangible robots","type":"publication"},{"authors":["Wafa Johal","James Kennedy","Vicky Charisi","Hae Won Park","Ginevra Castellano","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"58de15836ceb30e2b0c057832c3a28de","permalink":"https://wafajohal.github.io/publication/johal-2018-robots/","publishdate":"2019-09-22T10:51:36.528186Z","relpermalink":"/publication/johal-2018-robots/","section":"publication","summary":"","tags":null,"title":"Robots for Learning-R4L: Inclusive Learning","type":"publication"},{"authors":["Vicky Charisi","Alyssa M Alcorn","James Kennedy","Wafa Johal","Paul Baxter","Chronis Kynigos"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5129fb0f55d1f2c09590b25f675e41a9","permalink":"https://wafajohal.github.io/publication/charisi-2018-near/","publishdate":"2019-09-22T10:59:49.226623Z","relpermalink":"/publication/charisi-2018-near/","section":"publication","summary":"Robotics is a multidisciplinary and highly innovative field. Recently, multiple and often minimally connected sub-communities of child-robot interaction have started to emerge, variously focusing on the design issues, engineering, and applications of robotic platforms and toolkits. Despite increasing public interest in robots, including robots for children, child-robot interaction research remains highly fragmented and lacks regular cross-disciplinary venues for discussion and dissemination. This workshop will bring together researchers with diverse scientific backgrounds. It will serve as a venue in which to reflect on the current circumstances in which child-robot research is conducted, articulate emerging and ‚Äúnear future‚Äù challenges, and discuss actions and tools with which to meet those challenges and consolidate the field. ","tags":null,"title":"The near future of children's robotics","type":"publication"},{"authors":["Damien Pellier","Carole Adam","Wafa Johal","Humbert Fiorino","Sylvie Pesty"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"eafb3f6f9267a2ea9820663f9fc718af","permalink":"https://wafajohal.github.io/publication/pellier-2018-architecture/","publishdate":"2019-09-22T10:51:36.524394Z","relpermalink":"/publication/pellier-2018-architecture/","section":"publication","summary":"","tags":null,"title":"Une architecture cognitive et affective orient√©e interaction","type":"publication"},{"authors":["Elmira Yadollahi","Wafa Johal","Ana Paiva","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"0ab6460cce36faf6b48d03abe4cdbbf3","permalink":"https://wafajohal.github.io/publication/yadollahi-2018-deictic/","publishdate":"2019-09-22T10:59:49.22591Z","relpermalink":"/publication/yadollahi-2018-deictic/","section":"publication","summary":"This paper describes research aimed at supporting children's reading practices using a robot designed to interact with children as their reading companion. We use a learning by teaching scenario in which the robot has a similar or lower reading level compared to children, and needs help and extra practice to develop its reading skills. The interaction is structured with robot reading to the child and sometimes making mistakes as the robot is considered to be in the learning phase. Child corrects the robot by giving it instant feedbacks. To understand what kind of behavior can be more constructive to the interaction especially in helping the child, we evaluated the effect of a deictic gesture, namely pointing on the child's ability to find reading mistakes made by the robot. We designed three types of mistakes corresponding to different levels of reading mastery. We tested our system in a within-subject experiment with 16 children. We split children into a high and low reading proficiency even-though they were all beginners. For the high reading proficiency group, we observed that pointing gestures were beneficial for recognizing some types of mistakes that the robot made. For the earlier stage group of readers pointing were helping to find mistakes that were raised upon a mismatch between text and illustrations. However, surprisingly, for this same group of children, the deictic gestures were disturbing in recognizing mismatches between text and meaning.","tags":null,"title":"When deictic gestures in a robot can harm child-robot collaboration","type":"publication"},{"authors":null,"categories":null,"content":"The 4th Workshop on Robots for Learning will focus on Inclusive Learning trying to address how robots\n could benefit to learners with special needs or disabilities, improve social well-being in learning enhance learners collaborations and creativity.  Checkout the website for more information: http://r4l.epfl.ch/HRI2018\n","date":1514160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514160000,"objectID":"e5c92ee5dc76a1a854e05bc0fd30971c","permalink":"https://wafajohal.github.io/post/r4lhri2018cfp/","publishdate":"2017-12-25T00:00:00Z","relpermalink":"/post/r4lhri2018cfp/","section":"post","summary":"The 4th Workshop on Robots for Learning will focus on Inclusive Learning trying to address how robots\n could benefit to learners with special needs or disabilities, improve social well-being in learning enhance learners collaborations and creativity.  Checkout the website for more information: http://r4l.epfl.ch/HRI2018","tags":null,"title":"New CFP Robots for Inclusive Learning @HRI2018","type":"post"},{"authors":null,"categories":null,"content":"We presented our robotics projects and had some demos for industrial and academics partners. It is always the best place to network with great actors of robotics in Switzerland.\nhttp://www.youtube.com/watch?v=zv6nDMQCWCo\n","date":1514160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514160000,"objectID":"c4e3cb54546e385706c26fc381766f40","permalink":"https://wafajohal.github.io/post/nccr-id2017/","publishdate":"2017-12-25T00:00:00Z","relpermalink":"/post/nccr-id2017/","section":"post","summary":"We presented our robotics projects and had some demos for industrial and academics partners. It is always the best place to network with great actors of robotics in Switzerland.\nhttp://www.youtube.com/watch?v=zv6nDMQCWCo","tags":null,"title":"Swiss NCCR Robotics Industry Days","type":"post"},{"authors":null,"categories":null,"content":"It was a real adventure to travel to Moldavia for this panel meeting for which I was invited, as a HRI specialist, to review russo-european projects.\n","date":1508198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508198400,"objectID":"81602728b71392acce7fe5e773aa7111","permalink":"https://wafajohal.github.io/post/eraplus/","publishdate":"2017-10-17T00:00:00Z","relpermalink":"/post/eraplus/","section":"post","summary":"It was a real adventure to travel to Moldavia for this panel meeting for which I was invited, as a HRI specialist, to review russo-european projects.","tags":null,"title":"ERA.Net Plus RUS ‚Äì Robotics Panelist","type":"post"},{"authors":null,"categories":null,"content":"Our paper on the Cellulo project got the Best Paper Award in Vienna at the HRI2017! We had another study paper and a demo paper at HRI this year with this same project. üôÇ\n","date":1492387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492387200,"objectID":"0389eb1ffb94304c8ed28ece0cc26c82","permalink":"https://wafajohal.github.io/post/bphri2017/","publishdate":"2017-04-17T00:00:00Z","relpermalink":"/post/bphri2017/","section":"post","summary":"Our paper on the Cellulo project got the Best Paper Award in Vienna at the HRI2017! We had another study paper and a demo paper at HRI this year with this same project. üôÇ","tags":null,"title":"Best Paper Award @HRI2017!","type":"post"},{"authors":null,"categories":null,"content":"Currently or done reveiwing papers for CHI2017, HRI2017, CSCW2017, IJSORO, Transaction on Learning Technologies and the Journal of Child Computer Interaction\n","date":1484611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484611200,"objectID":"2345b40df143d3ee8f6adeca7e49665f","permalink":"https://wafajohal.github.io/post/review/","publishdate":"2017-01-17T00:00:00Z","relpermalink":"/post/review/","section":"post","summary":"Currently or done reveiwing papers for CHI2017, HRI2017, CSCW2017, IJSORO, Transaction on Learning Technologies and the Journal of Child Computer Interaction","tags":null,"title":"Reviewing ...","type":"post"},{"authors":["Ayberk √ñzg√ºr","S√©verin Lemaignan","Wafa Johal","Maria Beltran","Manon Briod","L√©a Pereyre","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a7703455c6ee33172092ec4d8a078878","permalink":"https://wafajohal.github.io/publication/ozgur-2017-cellulo/","publishdate":"2019-09-22T10:51:36.525732Z","relpermalink":"/publication/ozgur-2017-cellulo/","section":"publication","summary":"","tags":null,"title":"Cellulo: Versatile handheld robots for education","type":"publication"},{"authors":["Alexis David Jacq","Wafa Johal","Ana Paiva","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"868574a51b30c39ab419e7a73cf85a0b","permalink":"https://wafajohal.github.io/publication/jacq-2017-expressing/","publishdate":"2019-09-22T10:51:36.527496Z","relpermalink":"/publication/jacq-2017-expressing/","section":"publication","summary":"","tags":null,"title":"Expressing Motivations By Facilitating Other‚Äôs Inverse Reinforcement Learning","type":"publication"},{"authors":["Ayberk √ñzg√ºr","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"20bf7cadb088b2c86335061085570055","permalink":"https://wafajohal.github.io/publication/ozgur-2017-haptic/","publishdate":"2019-09-22T10:51:36.526065Z","relpermalink":"/publication/ozgur-2017-haptic/","section":"publication","summary":"The Cellulo robots are small tangible robots that are designed to represent virtual interactive point-like objects that reside on a plane within carefully designed learning activities. In the context of these activities, our robots not only display autonomous motion and act as tangible interfaces, but are also usable as haptic devices in order to exploit, for instance, kinesthetic learning. In this article, we present the design and analysis of the haptic interaction module of the Cellulo robots. We first detail our hardware and controller design that is low-cost and versatile. Then, we describe the task-based experimental procedure to evaluate the robot's haptic abilities. We show that our robot is usable in most of the tested tasks and extract perceptive and manipulative guidelines for the design of haptic elements to be integrated in future learning activities. We conclude with limitations of the system and future work.","tags":null,"title":"Haptic-enabled handheld mobile robots: Design and analysis","type":"publication"},{"authors":["Thibault Asselborn","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"446ad379b2355f18f33cad7501345296","permalink":"https://wafajohal.github.io/publication/asselborn-2017-keep/","publishdate":"2019-09-22T10:51:36.526821Z","relpermalink":"/publication/asselborn-2017-keep/","section":"publication","summary":"","tags":null,"title":"Keep on moving! Exploring anthropomorphic effects of motion during idle moments","type":"publication"},{"authors":["Ayberk √ñzg√ºr","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bc093dcff3b336c7b7a6bf9b43265bde","permalink":"https://wafajohal.github.io/publication/ozgur-2017-windfielddemo/","publishdate":"2020-04-11T21:56:56.818814Z","relpermalink":"/publication/ozgur-2017-windfielddemo/","section":"publication","summary":"","tags":null,"title":"Windfield: demonstrating wind meteorology with handheld haptic robots","type":"publication"},{"authors":["Ayberk √ñzg√ºr","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f9824efd1b7280392de65bd7b769f838","permalink":"https://wafajohal.github.io/publication/ozgur-2017-windfield/","publishdate":"2019-09-22T10:51:36.525411Z","relpermalink":"/publication/ozgur-2017-windfield/","section":"publication","summary":"","tags":null,"title":"Windfield: learning wind meteorology with handheld haptic robots","type":"publication"},{"authors":["Wafa Johal","Paul Vogt","James Kennedy","Mirjam de Haas","Ana Paiva","Ginevra Castellano","Sandra Okita","Fumihide Tanaka","Tony Belpaeme","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"d0685337fc9c356852644e315048cc16","permalink":"https://wafajohal.github.io/publication/johal-2017-workshop/","publishdate":"2019-09-22T10:51:36.526492Z","relpermalink":"/publication/johal-2017-workshop/","section":"publication","summary":"While robots have been popular as a tool for STEM teaching, the use of robots in other learning scenarios is novel. The field of HRI has started to report on how to make effective robots usable in educational contexts. However, many challenges remain. For instance, which interaction strategies aid learning, and which hamper learning? How can we deal with the current technical limitations of robots? Answering these and other questions requires a multidisciplinary effort, including contributions from pedagogy, developmental psychology, (computational) linguistics, artificial intelligence and HRI, among others. This abstract provides a brief overview of the current state-of-the-art in social robots designed for learning and describes the aims of the Robots for Learning (R4L) workshop in bringing together a multidisciplinary audience for furthering the development of market-ready educational robots.","tags":null,"title":"Workshop on Robots for Learning: R4L","type":"publication"},{"authors":null,"categories":null,"content":"I am happy to announce that we are launching a special issue on Robots for Learning in the Springer International Journal of Social Robotics. http://r4l.epfl.ch/IJSORO #R4L #robots4learning\n","date":1481846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481846400,"objectID":"3a99d97ea2177984c810a7459a42578a","permalink":"https://wafajohal.github.io/post/cfpjsoro/","publishdate":"2016-12-16T00:00:00Z","relpermalink":"/post/cfpjsoro/","section":"post","summary":"I am happy to announce that we are launching a special issue on Robots for Learning in the Springer International Journal of Social Robotics. http://r4l.epfl.ch/IJSORO #R4L #robots4learning","tags":null,"title":"CfP Special Issue in JSORO on Robots for Learning","type":"post"},{"authors":null,"categories":null,"content":"Today, I gave a seminar at Xerox R\u0026amp;D in a lovely manor near Grenoble. Glad to see people interested to port their NLP and ML algorithms to robots.\n","date":1481846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481846400,"objectID":"435e815727001077deeed004efc53182","permalink":"https://wafajohal.github.io/post/xerox/","publishdate":"2016-12-16T00:00:00Z","relpermalink":"/post/xerox/","section":"post","summary":"Today, I gave a seminar at Xerox R\u0026amp;D in a lovely manor near Grenoble. Glad to see people interested to port their NLP and ML algorithms to robots.","tags":null,"title":"Talk @Xerox R\u0026D, Grenoble","type":"post"},{"authors":null,"categories":null,"content":"","date":1481673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481673600,"objectID":"9757e9422048461fbdf10f62f40cbafc","permalink":"https://wafajohal.github.io/talk/xerox2016/","publishdate":"2016-12-14T00:00:00Z","relpermalink":"/talk/xerox2016/","section":"talk","summary":"","tags":null,"title":"Xerox Europe R-D Grenoble, France, December 2016","type":"talk"},{"authors":null,"categories":null,"content":"Interviewed on our Educational Robotics Projects at CHILI/EPFL:\nhttp://thelongandshort.org/machines/could-robots-teach-our-children\n","date":1470873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470873600,"objectID":"d146d99b554ff6590d8d4b19bbf56950","permalink":"https://wafajohal.github.io/post/interviewlongshort/","publishdate":"2016-08-11T00:00:00Z","relpermalink":"/post/interviewlongshort/","section":"post","summary":"Interviewed on our Educational Robotics Projects at CHILI/EPFL:\nhttp://thelongandshort.org/machines/could-robots-teach-our-children","tags":null,"title":"Interviewed by The Long and Short","type":"post"},{"authors":null,"categories":null,"content":"I am co-organizing a workshop on Social Robots in Education. For more info : https://socialrobotsineducation.wordpress.com/\n","date":1470873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470873600,"objectID":"4a517e83a302db8e1f82e5fbc9b10236","permalink":"https://wafajohal.github.io/post/wsnewfriends2016/","publishdate":"2016-08-11T00:00:00Z","relpermalink":"/post/wsnewfriends2016/","section":"post","summary":"I am co-organizing a workshop on Social Robots in Education. For more info : https://socialrobotsineducation.wordpress.com/","tags":null,"title":"Workshop on Social Robots in Education @NewFriends 2016","type":"post"},{"authors":null,"categories":null,"content":"Animatas is a H2020 MCS International Training Network involving 8 insitutions from Europe and several international partners (amoong which UNSW). The goal of the project is to investigate social learning and embodiement of autonomous agents in learning contexts.\nThree aspects are investigated at EPFL with some collaborators:\n Modelling Engagement in Collaborative Learning Scenario - Jauwairia  How can we build a group model of engagement? Can we quantify engagement with regard to learning and find the optimal engagement for the group of learners?\n Mutual Understanding in Learner-Robot Interaction - Utku  Can we build an agent able to detect mis-understanding? to repair them or create them (for the sake of learning)?\n Orchestration of Robot Populated Classroom - Sina  How to enable teachers to manage and monitor their classroom during robotics practical? What are the advantages and drawbacks of robots in the classroom in terms of teacher\u0026rsquo;s orchestration?\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"659b92da72f9dfd5a5c57145d11e331c","permalink":"https://wafajohal.github.io/project/animatas/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/animatas/","section":"project","summary":"Advancing intuitive human-machine interaction with human-like social capabilities for education in schools","tags":["r4l","cellulo","animatas"],"title":"Animatas","type":"project"},{"authors":null,"categories":null,"content":"In the field of human-robot interaction (HRI) as in many other technical fields, an innumerable number of different metrics to analyze the interaction can be found in as many studies. Even-though many of these metrics are not\ncomparable between studies, we observe that the research community in HRI, but also in many other research domain, is starting to seek for reproducibility [10]; a consensus begins to appear concerning common measures that can be used across a wide range of studies. In social HRI, the evaluation of the quality of an interaction is complex because it is very task dependent. However, certain metrics such as engagement seem to well reflect the quality of interactions between a robot agent and the human.\nOne aspect of acceptability of a robot is the home environment in which it is to be able to perceive when it will be solicited. The goal for the robot is to not disturb the user and to be able to predict that it will be solicited. This is something we do all the time as humans (we can see a street vendor approaching us and we know they will talk to us). The process for us human relies on proxemics (speed and angle of approach) but not only.\nIn my work on modeling engagement [11, 12], I used multi-modal data to train a SVM to detect engagement. We\ncollected data from various sensors embedded in the Kompai robot and reduced the number of crucial features from 32 to 7. Interestingly shoulder orientation and position of the face in the image are among these crucial features. If we transpose these features to what humans do, these features seem coherent with behavioral analysis. The previous model aimed to predict the engagement, but once the user is engaged, it is important to evaluate its attitude and mood. Using the COST and HAART dataset, we trained a model to detect social touch. Together with colleagues, we won the Social Touch Challenge at ICMI 2015 improving the gestures recognition from 60% accuracy to 70% training a Random Forest Model. [13]\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"58649cc8a6ece3333314200594b29e71","permalink":"https://wafajohal.github.io/project/engagement/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/engagement/","section":"project","summary":"Understanding human actions, intentions is key","tags":["affective computing","cowriter"],"title":"Behavioural Analysis in HRI","type":"project"},{"authors":null,"categories":null,"content":"I have been developing a model of the so called behavioral styles. These styles act like a filter over communicative gestures to transpose a way of performing an action. We used multiple platforms (Nao ‚Äì humanoid, and Reeti ‚Äì facial expression) to test this rendering on facial and bodily communication [14]. We showed that these styles were perceptible and could influence the attitude of the child interacting with the robot[15, 16]. More recently, we showed that idle movements (movements that have no communicative intention) when displayed by a humanoid robot increases the anthropomorphic perception of the robot by the user [17]. These findings help in designing more natural interaction with humanoid robots, making them more acceptable and socially intelligent. Research Perspectives: We will be continuing research in this area working within the ANIMATAS EU project (starting in January 2018) on synchrony and how alignment can keep learners engaged in a collaborative task with a robot.\n     ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2565fa501f241664cf0ff7bc073c37f8","permalink":"https://wafajohal.github.io/project/stylebot/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/stylebot/","section":"project","summary":"Generating context aware gestures","tags":["styles","moca"],"title":"Behavioural Styles","type":"project"},{"authors":null,"categories":null,"content":"I have been developing a model of the so called behavioral styles. These styles act like a filter over communicative gestures to transpose a way of performing an action. We used multiple platforms (Nao ‚Äì humanoid, and Reeti ‚Äì facial expression) to test this rendering on facial and bodily communication [14]. We showed that these styles were perceptible and could influence the attitude of the child interacting with the robot[15, 16]. More recently, we showed that idle movements (movements that have no communicative intention) when displayed by a humanoid robot increases the anthropomorphic perception of the robot by the user [17]. These findings help in designing more natural interaction with humanoid robots, making them more acceptable and socially intelligent. Research Perspectives: We will be continuing research in this area working within the ANIMATAS EU project (starting in January 2018) on synchrony and how alignment can keep learners engaged in a collaborative task with a robot.\n     ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2755ee3a5e88be643936ba166168a3fe","permalink":"https://wafajohal.github.io/project/under_grad_past_projects/curriculum_analysis/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/under_grad_past_projects/curriculum_analysis/","section":"project","summary":"Generating context aware gestures","tags":["styles","moca"],"title":"Behavioural Styles","type":"project"},{"authors":null,"categories":null,"content":"With the Cellulo project [6], a part of the Transversal Educational Activities of the NCCR Robotics, we introduced a new robotic platform which is small and easily deployable. A dotted pattern printed on regular paper enables the Cellulo robots with absolute localization with a precision of 270 microns [7]. The robots also have a new locomotion system with a drive relying on a permanent magnet to actuate coated metal balls [8]. This new drive design allows backdrivability; i.e. it allows the robot to move and to be moved without damaging it. With this system, we also implemented a haptic feedback modality, allowing the user to feel forces when grasping the robot [9]. The robots are connected via Bluetooth to a master (PC or tablet) that handles the logical and computation of the activity. The onboard PCB of the robots only allows for proceeding the localization (image capture and decoding of the pattern) and the control of the three wheels actuation. During two years, we developed several learning activities using the robots. The Figure for example shows the Feel the Wind activity, in which the learners were taught that the wind was formed by air moving from with high to low pressure points.\n  In the Cellulo project, we also started to explore the use of haptic feedback for learners. Haptic feedback enables us to render forces, but also borders, angles, or points. We developed a series of haptic capabilities and small interaction tasks that can be included in learning activities to inform the learner [9]. We tested the haptic feedback with children for instance in the symmetry activity, in which the child is able to formulate hypothesis on the placement of the symmetrical shape and to verify their claims by feel haptically the shape on paper (left Figure). We also tested with some pilot with visually impaired children who were able to explore a map of their classroom using the Cellulo robots. Research Perspectives for Tangible Swarm Interaction and Haptic for Learners: We are now exploring the dynamics of the group of learners in manipulating the robots. The collaboration among learner is not always optimal, and a challenge would be to use the swarm robots to analyses and regulate the collaboration among learners. As these shared resource can be intelligent agents, they could rearrange themselves according to the collaborative state of the group.\n    ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"fcd133806e4dabbfec24bdc44e8750ef","permalink":"https://wafajohal.github.io/project/cellulo/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/cellulo/","section":"project","summary":"Tangible Haptic-Enabled Swarm for Learners","tags":["cellulo","tangible robot","swarm","haptic"],"title":"Cellulo","type":"project"},{"authors":null,"categories":null,"content":"In the CoWriter Project, we explore how the learning by teaching approach can be employed to motivate children to practice their handwriting skills with a small humanoid robot.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2ccbc2d5d36c6415c2b7146e1bf45e45","permalink":"https://wafajohal.github.io/project/cowriter/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/cowriter/","section":"project","summary":"Learning Hanwriting with a Robot Companion","tags":["handwriting","cowriter"],"title":"CoWriter","type":"project"},{"authors":null,"categories":null,"content":"Project Summary This research occurred in a special context where Kazakhstan‚Äôs recent decision to switch from Cyrillic to the Latin-based alphabet has resulted in challenges connected with teaching literacy, addressing a rare combination of research hypotheses and technical objectives about language learning. Teachers are not necessarily trained to teach the new alphabet, and this could result in a harder challenge for children with difficulties. Prior research studies in Human-Robot Interaction (HRI) have proposed the use of a robot to teach handwriting to children. Drawing on the Kazakhstani case, our study takes an interdisciplinary approach by bringing together smart solutions from robotics, computer vision areas and educational frameworks, language and cognitive studies that will benefit diverse groups of stakeholders.\nRelatively little is known about the transfer of writing skills across scripts and it is extremely difficult to isolate the role of motor skills in this transfer, as compared to other cognitive and linguistic skills. Since handwriting combines visual, language, and motor dimensions, neuroimaging does not provide a clear account of cerebral substrates of handwriting. A recent change of policy in Kazakhstan gave us an opportunity to measure transfer, as the Latin-based Kazakh alphabet has not yet been introduced. With the aim to understand if the script change of the Kazakh language from Cyrillic to Latin would have an effect on handwriting performance of young children and potentially increase the number of children with learning disabilities such as dyslexia and dysgraphia, we investigated the transfer of handwriting skills (from Cyrillic to Latin) in primary school children. To this end, we conducted an analysis of children‚Äôs handwriting dynamic data in both scripts (acquired from 200 children aged 6-11 years old who were asked to write a short text on a digital tablet using its stylus) in order to identify if the number of years spent practicing Cyrillic has an effect on the quality of handwriting in the Latin alphabet. The results showed that some of the differences between the two scripts were constant across all grades. These differences thus reflect the intrinsic differences in the handwriting dynamics between the two alphabets. For instance, several features related to the pen pressure on the tablet are quite different. Other features, however, revealed decreasing differences between the two scripts across grades. While we found that the quality of Cyrillic writing increased from grades 1 to 4, due to increased practice, we also found that the quality of the Latin writing increased as well, despite the fact that all of the pupils had the same absence of experience in writing in Latin. We can therefore interpret this improvement in Latin script as an indicator of the transfer of fine motor control skills from Cyrillic to Latin. This result is especially surprising given that one could instead hypothesize a negative transfer, i.e., that the finger controls automated for one alphabet would interfere with those required by the other alphabet.\nDeliverables   We proposed the first dataset for handwriting recognition of Cyrillic-based languages such as Kazakh and Russian, which is appropriate for the use by machine learning approaches. It contains 121,234 samples of 42 Cyrillic letters. The performance of Cyrillic-MNIST is evaluated using standard deep learning approaches and is compared to the Extended MNIST (EMNIST) dataset. The dataset is available at https://github.com/bolattleubayev/cmnist. This work is under review in Frontiers in Big Data journal. It was conducted in Kazakhstan.\n  We implemented an autonomous social robot that would assist and motivate children in transition from the old Cyrillic alphabet to a new Latin alphabet. The hardware components of the system include a humanoid NAO robot and a tablet with a stylus. The software components of the CoWriting Kazakh system include: a) handwriting recognition of Cyrillic languages trained on over 120,000 samples of the Cyrillic-MNIST dataset; b) Learning by Demonstration (LbD) in robot font to dynamically adapt to each child\u0026rsquo;s individual handwriting style where trajectories of human handwriting are collected in real time; c) Latin-to-Latin Learning by Teaching (LbT): a humanoid robot plays a unique social role and asks for help to learn Kazakh language but since it cannot read Cyrillic script, a child becomes robot\u0026rsquo;s teacher who is committed to try her best to write the words in Latin so that the robot can read and learn Kazakh. Such an approach allows us to keep a child engaged in the learning process that we utilized in the experiment with 67 children. The system was submitted as a demonstration paper titled ‚ÄúCoWriting Kazakh: Learning a new script with a robot - Demonstration‚Äù. This demonstration got the ‚ÄúBest Demonstration‚Äù award at the prestigious HRI 2020 conference. This work was conducted in Kazakhstan.\n  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1f74630fb648c43f4c4d2fe7a564ff54","permalink":"https://wafajohal.github.io/project/cowriting_kazakh/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/cowriting_kazakh/","section":"project","summary":"A social robot to help the transition from Cyrillic to Latin alphabet in Kazakhstan","tags":["cowriter","handwriting","cowriting_kazakh"],"title":"CoWriting Kazakh","type":"project"},{"authors":null,"categories":null,"content":"Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, not speak the native language as their first language, be new to a culture (migrants), or have a special need involving communication impairments (e.g., autism, speech, and hearing) [1]. Their voices often go unheard as they find it hard to contribute to conversations and group work, in both face to face and online settings, which puts them at risk for poor socio-emotional and achievement outcomes.\nVarious interventions have been proposed in an effort to help these children integrate better into social settings, including ice-breaking activities, support groups, [2][3]. Another promising alternative is to teach children how to interpret and use humor effectively to facilitate social interaction [4]. Having a sense of humour might help children to reduce stress [5] and ease social interactions, enabling them to overcome their social inhibitions and make friends more easily. Also, humour could promotes children‚Äôs language, literacy, and inference skills, since understanding jokes require a mastery of language ambiguity, and comprehension [6]. One manner to use humor effectively is by ‚Äòjoke telling‚Äô. A joke is the shortest clearly-structured conversation one can have with one or more peers. Telling jokes can help children improve their confidence, socialization, [4], literacy [8], and cultural learning [7]. For example, a child could start a ‚Äòjoke‚Äô that implies a question. He/she knows the answer, but not his/her peers. Then, he/she has the advantage of knowing what the answer is. This helps him/her to gain confidence. In terms of literacy, telling a joke requires the reinterpretation of the syntax underlying the question, then, understanding the ambiguity in a joke might help them in the comprehension of language [8]. Regarding socialization, this dynamic allows children to take part in role-taking, and understanding what other people needs to know to get it. Also, it can create a positive environment where children can laugh together and share their understanding. In terms of learning, a jokes encapsulate cultural assumptions, for example, differences between pronunciation, a dual meaning of a word, and the structural and ambiguity of sentences [7]. The question this raises is: how can we help children from all walks of life learn the art of telling jokes and importantly feel comfortable, and in doing so, increase their confidence, social skills and even literacy? To help children develop the ability to tell jokes with their peers, we propose leveraging a new interactive technology ‚Äì voice assistant robots (e.g., Alexa), which have become increasingly affordable and commonplace in homes. These robots, which are portable and have lifelike voices, can model how to tell a joke and using voice recognition software, respond to the jokes that children tell. Children thus have unlimited opportunities for scaffolded practice and can receive timely feedback in a non-threatening environment before telling their jokes to family and friends. In addition to the accessibility and human-like verbal responses of voice assistant robots, robots can be customized to tell jokes that are appropriate for each child‚Äôs age, language, and culture. The goal of our project therefore is to enable children to improve their ‚Äòjoke telling‚Äô skills by learning the intonation, timing, through interacting with a virtual assistants such as Alexa. Our research project will explore the feasibility of coding Alexa and other voice assistants to be able to respond and engage with children telling a diversity of jokes. Several aspects need to be taken into account when delivering a joke that children can imitate and practice with. We will look into how the timing and intonation will need to be evaluated from the audio recording in order to provide an adaptive feedback to the child training to deliver jokes. This feedback could be from both sides, the voice assistant robot helping the child in how telling jokes, or the child helping the assistant to improve how telling jokes. The final goal is, once the child has learnt how to tell jokes, to understand when and how will he/she be able to share the joke with their peers, and how long might this process take and how would we be able to tell they have become more confident. Finally, an engaging joke-telling learning scenario will need to be designed in order to train children for a long-term impact. Our project will also consider how this might materialize. In the future, virtual assistants will be able to be designed to give encouraging feedback to the child as they interact with it. This could be verbal, visual or both. This feedback could encourage children for finding their voice. Based on the initial findings from the pilot project we would like to develop a more extensive project proposal with the goal of investigating how to develop new interventions using the next generation of interactive robots, such as Olly, whose developers, we are also collaborating with (https://heyolly.com/), that have more scope for providing customized feedback can be used.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"45f8841eb64aea3141bafd94c1162fff","permalink":"https://wafajohal.github.io/project/fyv/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/fyv/","section":"project","summary":"Helping children's language understanding through joke telling with home assistant","tags":["affective computing","learning companion","home assistant"],"title":"Find Your Voice","type":"project"},{"authors":null,"categories":null,"content":"With the increasing number of engineering jobs, politics have more recently turned towards introduction of engineering subjects in early stages of curricula. More and more countries have started to introduce programming (and even robotics) to young children. However, this constitutes a real challenge for the teaching professionals, who are not trained for that, and are often skeptical to use new technologies in classrooms. Hence, the challenge is to introduce robots as tools to be used not only in programming or robotics based courses, but in the whole curricula in order to make them attractive to teachers as a pedagogical tool. ‚ÄùEducational robotics‚Äù does not really constitute a research community per say. On one hand, there are scholars working on ‚Äùhow to teach robotics‚Äù using robotics platforms such as Thymio and Mindstorms. Some scholars perform research in this domain (for example, measuring which learning activities produce faster learning), but their numbers are scarce, they typically meet in half-day workshops before ed‚Äôtech conferences (CSCL, AI\u0026amp;Ed, ‚Ä¶.). On the other hand, one finds scholars who mainly do research on HRI and consider education as an interesting place for testing child-robot interactions. I launched in 2016 a series of workshops to build a Robots for Learning (R4L) research community. The first event was a workshop, along with the RoMan 2016 conference. The event gathered 30 participants from all around the world. The second event was a workshop along the HRI 2017 conference in Vienna, Austria, with 60 participants and 10 presentations. This fall, we hosted the first stand alone event, in Switzerland, for which we invited main actors of research in HRI for Learning. A new workshop is planned for HRI 2018. These workshops aim to mix scientists from field of robotics with those from digital education and learning technologies (See https://robot4learning.github.io/ ).\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e4802fe1f529e6392518b0e58822db5d","permalink":"https://wafajohal.github.io/project/robots4learning/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/robots4learning/","section":"project","summary":"Understading the value of robots for leaners","tags":["r4l"],"title":"Robots for Learning (R4L)","type":"project"},{"authors":null,"categories":null,"content":"Grants for postdocs\nGrans for visitors: french unsw others\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"98e785ba537fca98e25a25185efab61f","permalink":"https://wafajohal.github.io/prospective/visitors/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/prospective/visitors/","section":"prospective","summary":"Grants for postdocs\nGrans for visitors: french unsw others","tags":null,"title":"Visitors and PostDoc","type":"prospective"},{"authors":null,"categories":null,"content":"We demonstrated an activity on wind with Cellulo robots at the Cambridge Science Festival. About 2000 people came to our stand. This demonstration was prepared by all the Cellulo project members. We were kindly invited by Swissnex Boston.\nhttps://youtu.be/g_7glQmTIVo\nhttps://youtu.be/s3KAoQNUPZs\n","date":1460764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460764800,"objectID":"fe5c203562c82a7f4a7d9bf39ea7e364","permalink":"https://wafajohal.github.io/post/cellulocsf2016/","publishdate":"2016-04-16T00:00:00Z","relpermalink":"/post/cellulocsf2016/","section":"post","summary":"We demonstrated an activity on wind with Cellulo robots at the Cambridge Science Festival. About 2000 people came to our stand. This demonstration was prepared by all the Cellulo project members. We were kindly invited by Swissnex Boston.\nhttps://youtu.be/g_7glQmTIVo\nhttps://youtu.be/s3KAoQNUPZs","tags":null,"title":"Cellulo @ the Robots Zoo, Cambridge Science Festival","type":"post"},{"authors":null,"categories":null,"content":"I recently contributed to the translation in French of the godspeed questionnaire.\nThis questionnaire aims to measure user‚Äôs perception of robots.\n","date":1460764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460764800,"objectID":"f1f51e1875f5934f81b750801a272a6c","permalink":"https://wafajohal.github.io/post/frenchgodspeed/","publishdate":"2016-04-16T00:00:00Z","relpermalink":"/post/frenchgodspeed/","section":"post","summary":"I recently contributed to the translation in French of the godspeed questionnaire.\nThis questionnaire aims to measure user‚Äôs perception of robots.","tags":null,"title":"French Translation of Bartneck‚Äôs Godspeed Questionnaire","type":"post"},{"authors":null,"categories":null,"content":"My team and I won the Social Touch Grand Challenge of ICMI 2015.\nI presented our results at the Grand Challenge session. We had the best scores for recognition of social gestures.\n","date":1460764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460764800,"objectID":"6127f95f372a72a04850edc217b90be0","permalink":"https://wafajohal.github.io/post/icmi2015/","publishdate":"2016-04-16T00:00:00Z","relpermalink":"/post/icmi2015/","section":"post","summary":"My team and I won the Social Touch Grand Challenge of ICMI 2015.\nI presented our results at the Grand Challenge session. We had the best scores for recognition of social gestures.","tags":null,"title":"Social Touch Grand Challenge @ICMI 2015","type":"post"},{"authors":null,"categories":null,"content":"I gave a talk and discussed with some members of the Personal Robots Group. I also got to see Tega, Leonardo and Nexi for real. üôÇ\nUnfortunately, no Jibo around yet.\nWe also took a tour in the MIT Media Lab building.\n","date":1460764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460764800,"objectID":"fe6c13eed81ddccbb58e619ae89c5dc2","permalink":"https://wafajohal.github.io/post/visitmitprg2016/","publishdate":"2016-04-16T00:00:00Z","relpermalink":"/post/visitmitprg2016/","section":"post","summary":"I gave a talk and discussed with some members of the Personal Robots Group. I also got to see Tega, Leonardo and Nexi for real. üôÇ\nUnfortunately, no Jibo around yet.\nWe also took a tour in the MIT Media Lab building.","tags":null,"title":"Visit the Personal Robots Group at MIT Media Lab","type":"post"},{"authors":null,"categories":null,"content":"","date":1460678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460678400,"objectID":"360e26b1dc4bd2025fb13745767637ad","permalink":"https://wafajohal.github.io/talk/mit2016/","publishdate":"2016-04-15T00:00:00Z","relpermalink":"/talk/mit2016/","section":"talk","summary":"","tags":null,"title":"MIT Media Lab, Personal Robotics Group, April 2016","type":"talk"},{"authors":null,"categories":null,"content":"","date":1460592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460592000,"objectID":"06d56ad61cb20887d581111b4caf4ece","permalink":"https://wafajohal.github.io/talk/swissnex2016/","publishdate":"2016-04-14T00:00:00Z","relpermalink":"/talk/swissnex2016/","section":"talk","summary":"","tags":null,"title":"Swissnex Boston, April 2016","type":"talk"},{"authors":["Wafa Johal","Alexis Jacq","Ana Paiva","Pierre Dillenbourg"],"categories":null,"content":"  ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"4a69452696c10f2aa57a5d50b54cdb76","permalink":"https://wafajohal.github.io/publication/johal-2016-child/","publishdate":"2019-09-22T10:51:36.524069Z","relpermalink":"/publication/johal-2016-child/","section":"publication","summary":"In  this  paper,  we  present  an  experiment  in  the context of a child-robot interaction where we study the influence of  the  child-robot  spatial  arrangement  on  the  child‚Äôs  focus  of attention and the perception of the robot‚Äôs performance. In the ‚ÄúCo-Writer learning by teaching‚Äù activity, the child teaches a Nao robot how to handwrite. Usually only face-to-face spatial arrangements are tested in educational child robot interactions, but  we  explored  two  spatial  conditions  from  Kendon‚Äôs  F-formation,  the  side-by-side  and  the  face-to-face  formations  in a  within  subject  experiment.  We  estimated  the  gaze  behavior of  the  child  and  their  consistency  in  grading  the  robot  with regard  to  the  robot‚Äôs  progress  in  writing.  Even-though  the demonstrations provided by children were not different between the two conditions (i.e. the robot‚Äôs learning didn‚Äôt differ), the results showed that in the side-by-side condition children tended to  be  more  indulgent  with  the  robot‚Äôs  mistakes  and  to  give it  better  feedback.  These  results  highlight  the  influence  of experimental choices in child-robot interaction","tags":null,"title":"Child-robot spatial arrangement in a learning by teaching activity","type":"publication"},{"authors":["Alexis Jacq","Wafa Johal","Pierre Dillenbourg","Ana Paiva"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"5db07a2a81b91de3a83f41a1cc4e9a47","permalink":"https://wafajohal.github.io/publication/jacq-2016-cognitive/","publishdate":"2019-09-22T10:51:36.523763Z","relpermalink":"/publication/jacq-2016-cognitive/","section":"publication","summary":"","tags":null,"title":"Cognitive architecture for mutual modelling","type":"publication"},{"authors":["Ayberk √ñzg√ºr","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"e36af435970f54d5ee280f35968e53e4","permalink":"https://wafajohal.github.io/publication/ozgur-2016-permanent/","publishdate":"2019-09-22T10:51:36.523414Z","relpermalink":"/publication/ozgur-2016-permanent/","section":"publication","summary":"","tags":null,"title":"Permanent magnet-assisted omnidirectional ball drive","type":"publication"},{"authors":["Francesco Mondada","Evgeniia Bonnet","Shaniel Davrajh","Wafa Johal","Riaan Stopforth"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7142bad334ef2027986481c415b041b1","permalink":"https://wafajohal.github.io/publication/mondada-2016-r-2-t-2/","publishdate":"2019-09-22T10:51:36.525073Z","relpermalink":"/publication/mondada-2016-r-2-t-2/","section":"publication","summary":"","tags":null,"title":"R2T2: Robotics to integrate educational efforts in South Africa and Europe","type":"publication"},{"authors":["Carole Adam","Wafa Johal","Damien Pellier","Humbert Fiorino","Sylvie Pesty"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7e174f5b06ad4e07d7e887492dc444cf","permalink":"https://wafajohal.github.io/publication/adam-2016-social/","publishdate":"2019-09-22T10:51:36.524708Z","relpermalink":"/publication/adam-2016-social/","section":"publication","summary":"","tags":null,"title":"Social human-robot interaction: A new cognitive and affective interaction-oriented architecture","type":"publication"},{"authors":["Dominique Vaufreydaz","Wafa Johal","Claudine Combe"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"c5350178a8c0355d12bae0757a84dd78","permalink":"https://wafajohal.github.io/publication/vaufreydaz-2016-starting/","publishdate":"2019-09-22T10:51:36.52114Z","relpermalink":"/publication/vaufreydaz-2016-starting/","section":"publication","summary":"Recognition of intentions is a subconscious cognitive process vital to human communication. This skill enables anticipation and increases the quality of interactions between humans. Within the context of engagement, non-verbal signals are used to communicate the intention of starting the interaction with a partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Originality of our approach resides in taking inspiration from social and cognitive sciences to perform our perception task. We investigate meaningful features, i.e. human readable features, and elicit which of these are important for recognizing someone‚Äôs intention of starting an interaction. Classically, spatial information like the human position and speed, the human‚Äìrobot distance are used to detect the engagement. Our approach integrates multimodal features gathered using a companion robot equipped with a Kinect. The evaluation on our corpus collected in spontaneous conditions highlights its robustness and validates the use of such a technique in a real environment. Experimental validation shows that multimodal features set gives better precision and recall than using only spatial and speed features. We also demonstrate that 7 selected features are sufficient to provide a good starting engagement detection score. In our last investigation, we show that among our full 99 features set, the space reduction is not a solved task. This result opens new researches perspectives on multimodal engagement detection.","tags":null,"title":"Starting engagement detection towards a companion robot using multimodal features","type":"publication"},{"authors":["Wafa Johal","Damien Pellier","Carole Adam","Humbert Fiorino","Sylvie Pesty"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"8b77c395ebf8d06e51fc50f17e1988c3","permalink":"https://wafajohal.github.io/publication/johal-2015-cognitive/","publishdate":"2019-09-22T10:51:36.522473Z","relpermalink":"/publication/johal-2015-cognitive/","section":"publication","summary":"","tags":null,"title":"A cognitive and affective architecture for social human-robot interaction","type":"publication"},{"authors":["Wafa Benkaouar Johal"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"815db4b8be8d9f59f4aaf552407df2a9","permalink":"https://wafajohal.github.io/publication/johal-2015-companion/","publishdate":"2019-09-22T10:51:36.527156Z","relpermalink":"/publication/johal-2015-companion/","section":"publication","summary":"","tags":null,"title":"Companion Robots Behaving with Style: Towards Plasticity in Social Human-Robot Interaction","type":"publication"},{"authors":["Wafa Johal","Ga√´lle Calvary","Sylvie Pesty"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"d805c068954b4ff05daabdf1774ccb39","permalink":"https://wafajohal.github.io/publication/johal-2015-non/","publishdate":"2019-09-22T10:51:36.523095Z","relpermalink":"/publication/johal-2015-non/","section":"publication","summary":"","tags":null,"title":"Non-verbal Signals in HRI: Interference in Human Perception","type":"publication"},{"authors":["Wafa Johal"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"8704a63253d14622dbe48d75098551ef","permalink":"https://wafajohal.github.io/publication/johal-2015-robots/","publishdate":"2019-09-22T10:51:36.522159Z","relpermalink":"/publication/johal-2015-robots/","section":"publication","summary":"","tags":null,"title":"Robots Interacting with Style","type":"publication"},{"authors":["Viet-Cuong Ta","Wafa Johal","Maxime Portaz","Eric Castelli","Dominique Vaufreydaz"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"f9523271ca0ab1ffc94f95f895a52ef2","permalink":"https://wafajohal.github.io/publication/ta-2015-grenoble/","publishdate":"2019-09-22T10:51:36.52279Z","relpermalink":"/publication/ta-2015-grenoble/","section":"publication","summary":"New technologies and especially robotics is going towards more natural user interfaces. Works have been done in different modality of interaction such as sight (visual computing), and audio (speech and audio recognition) but some other modalities are still less researched. The touch modality is one of the less studied in HRI but could be valuable for naturalistic interaction. However touch signals can vary in semantics. It is therefore necessary to be able to recognize touch gestures in order to make human-robot interaction even more natural. We propose a method to recognize touch gestures. This method was developed on the CoST corpus and then directly applied on the HAART dataset as a participation of the Social Touch Challenge at ICMI 2015. Our touch gesture recognition process is detailed in this article to make it reproducible by other research teams. Besides features set description, we manually filtered the training corpus to produce 2 datasets. For the challenge, we submitted 6 different systems. A Support Vector Machine and a Random Forest classifiers for the HAART dataset. For the CoST dataset, the same classifiers are tested in two conditions: using all or filtered training datasets. As reported by organizers, our systems have the best correct rate in this year's challenge (70.91% on HAART, 61.34% on CoST). Our performances are slightly better that other participants but stay under previous reported state-of-the-art results.","tags":null,"title":"The Grenoble system for the social touch challenge at ICMI 2015","type":"publication"},{"authors":null,"categories":null,"content":"Demo using iPOPO, OpenHab via MQTT with Nao as natural remote control of the smart home sensors and actuators. (in collaboration with Shadi Abras, Thomas Calmant and Amr Alyafi).\nhttps://youtu.be/4vBSJ7csp8g\n","date":1397606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397606400,"objectID":"88278cad198816c1d431b816b24aa6fd","permalink":"https://wafajohal.github.io/post/eclipseiot2014/","publishdate":"2014-04-16T00:00:00Z","relpermalink":"/post/eclipseiot2014/","section":"post","summary":"Demo using iPOPO, OpenHab via MQTT with Nao as natural remote control of the smart home sensors and actuators. (in collaboration with Shadi Abras, Thomas Calmant and Amr Alyafi).\nhttps://youtu.be/4vBSJ7csp8g","tags":null,"title":"Demo at Eclipse IOT Day 2014, Grenoble","type":"post"},{"authors":["Wafa Johal","Gaelle Calvary","Sylvie Pesty"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"00da623763bdef5e3c44a84531e7a714","permalink":"https://wafajohal.github.io/publication/johal-2014-robot/","publishdate":"2019-09-22T10:51:36.520174Z","relpermalink":"/publication/johal-2014-robot/","section":"publication","summary":"","tags":null,"title":"A Robot with Style, because you are Worth it!","type":"publication"},{"authors":["Wafa Johal","Carole Adam","Humbert Fiorino","Sylvie Pesty","C√©line Jost","Dominique Duhaut"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5275f877fb800832d7e6b6d1c3c0a112","permalink":"https://wafajohal.github.io/publication/johal-2014-acceptability/","publishdate":"2019-09-22T10:51:36.521482Z","relpermalink":"/publication/johal-2014-acceptability/","section":"publication","summary":"","tags":null,"title":"Acceptability of a companion robot for children in daily life situations","type":"publication"},{"authors":["Wafa Johal","Sylvie Pesty","Ga√´lle Calvary"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"3449b6f6eaab0c4731831c6e2c871b3c","permalink":"https://wafajohal.github.io/publication/johal-2014-styles/","publishdate":"2019-09-22T10:51:36.521838Z","relpermalink":"/publication/johal-2014-styles/","section":"publication","summary":"","tags":null,"title":"Des Styles pour une Personnalisation de l'Interaction Homme-Robot","type":"publication"},{"authors":["Wafa Johal","Sylvie Pesty","Ga√´lle Calvary"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"90b5a2537feadc3dac7866a597f0f0a3","permalink":"https://wafajohal.github.io/publication/johal-2014-expressing/","publishdate":"2019-09-22T10:51:36.5205Z","relpermalink":"/publication/johal-2014-expressing/","section":"publication","summary":"","tags":null,"title":"Expressing Parenting Styles with Companion Robots","type":"publication"},{"authors":["Wafa Johal","Sylvie Pesty","Gaelle Calvary"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ee11b2e0b1d7eface3aeda38af0392f8","permalink":"https://wafajohal.github.io/publication/johal-2014-towards/","publishdate":"2019-09-22T10:51:36.520816Z","relpermalink":"/publication/johal-2014-towards/","section":"publication","summary":"","tags":null,"title":"Towards companion robots behaving with style","type":"publication"},{"authors":["Carole Adam","Wafa Johal","Ilef Ben Farhat","C√©line Jost","Humbert Fiorono","Sylvie Pesty","Dominique Duhaut"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"bdabfe673ea5442c37aa6672017dcd59","permalink":"https://wafajohal.github.io/publication/adam-2013-acceptabilite/","publishdate":"2019-09-22T10:51:36.519868Z","relpermalink":"/publication/adam-2013-acceptabilite/","section":"publication","summary":"","tags":null,"title":"Acceptabilit√© d'un robot compagnon dans des situations de la vie quotidienne.","type":"publication"},{"authors":["Wafa Johal","Julie Dugdale","Sylvie Pesty"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"d5abe417be4d6de04bf78e4927c52a1d","permalink":"https://wafajohal.github.io/publication/johal-2013-modelling/","publishdate":"2019-09-22T10:51:36.519511Z","relpermalink":"/publication/johal-2013-modelling/","section":"publication","summary":"","tags":null,"title":"Modelling interactions in a mixed agent world.","type":"publication"},{"authors":["Wafa Benkaouar Johal"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618203739,"objectID":"4214a59d4d950eef3d82db4b16a3fa23","permalink":"https://wafajohal.github.io/publication/benkaouar-2012/","publishdate":"2012-06-12T05:02:18.234182Z","relpermalink":"/publication/benkaouar-2012/","section":"publication","summary":"","tags":[],"title":"Detection of non-verbal communication cues using multi-modal sensors : engagement detection ","type":"publication"},{"authors":["Wafa Benkaouar","Dominique Vaufreydaz"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"bd7f2b732ade4c8dbe767ed7db29a545","permalink":"https://wafajohal.github.io/publication/benkaouar-2012-multi/","publishdate":"2019-09-22T10:51:36.51914Z","relpermalink":"/publication/benkaouar-2012-multi/","section":"publication","summary":"Recognition of intentions is an unconscious cognitive process vital to human communication. This skill enables anticipation and increases interactive exchanges quality between humans. Within the context of engagement, i.e. intention for interaction, non-verbal signals are used to communicate this intention to the partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Classically, the human position and speed, the human-robot distance are used to detect the engagement. Our hypothesis is that this method is not enough in a context of a home environment. The chosen approach integrates multimodal features gathered using a robot enhanced with a Kinect. The evaluation of this new method of detection on our corpus collected in spontaneous conditions highlights its robustness and validates use of such technique in real environment. Experimental validation shows that the use of multimodal sensors gives better precision and recall than the detector using only spatial and speed features. We also demonstrate that 7 multimodal features are sufÔ¨Åcient to provide a good engagement detection score.","tags":null,"title":"Multi-sensors engagement detection with a robot companion in a home environment","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"https://wafajohal.github.io/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"One of my interests is to make robots able to autonomously sustain interactions with users, and in order to do so, they have to be able to reason about the users‚Äô and their environment. During my PhD, I have been working on a Cognitive Architecture able to reason on Emotion, named CAIO (Cognitive and Affective Interaction-Oriented) Architecture [3, 4]. This architecture, based on symbolic reasoning, showed promising results in modeling cognitive processes and specifically allowing decision making based on emotions. As shown in the figure, this architecture works as a two loops process, similar to the Dual-Process Theory - a deliberative loop generating intentions and a sensorimotor loop handling reflexes.\nMore recently, we have been working on second order reasoning in the context of the CoWriter project [5]. In the CoWriter project, the child‚Äôs teaches a Nao robot how to write. We use the learning by teaching paradigm to enhance motivation and engagement. In a collaborative learning task between a robot and a child, the idea is to model the child‚Äôs understanding and the child‚Äôs believes of the understanding of the co-learner robot. This way the robot could detect misunderstandings in view to correct them; or the robot could even create misunderstandings to enhance learning (by fostering questioning). Since my arrival on the CoWriter project, we initiated a project on diagnosis of dysgraphia using data collected via a graphic tablet (Wacom). Our first results using RNN are very promising (a patent and a journal paper have been submitted). This work will later on be integrated in the CoWriter handwriting activities to adapt the learning path according to the diagnosis and the learner‚Äôs handwriting difficulties.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56381d78247340c21f180f4c2e39fcfa","permalink":"https://wafajohal.github.io/project/caio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/caio/","section":"project","summary":"Cognitive and Affective Interaction-Oriented","tags":["reasoning","caio"],"title":"CAIO","type":"project"},{"authors":null,"categories":null,"content":"Context Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development (e.g., autism spectrum disorder, speech or hearing disorders).\nThe voices of these children often go unheard, as they find it hard to contribute to a conversation.\nThe Find your Voice (FyV, http://wafa.johal.org/project/fyv/) project was initiated to investigate how joke telling could help children to speak up and gain confidence. We are also interested in story telling and general conversation. Improvements in communication can have a significant impact in confidence, and help children:\n reduce stress improve self-confidence ease social interactions make friends more easily improving literacy and language  To help children develop the ability to communicate, tell jokes or stories to their peers, we propose leveraging social robots (e.g. NAO) and voice assistants (e.g., Alexa, Olly and Google Home) to:\n Model how to tell jokes/stories and respond to other children during conversations . Practice joke/story telling with a ‚Äòfriendly‚Äô and ‚Äònon-judgmental‚Äô audience. Practice turn taking during conversation. TLearn jokes, stories and interesting facts to tell other children.  The overall goals of the project are:\n To enable children to improve their social communication skills by learning intonation and timing, through interacting with voice assistants To learn to how to perform in front of peers and family To make children more confident in social situations  The FyV project involves partners in London and California.\nGoals \u0026amp; Milestones At UNSW, our main goal will be to develop a ‚ÄòLearning by Teaching‚Äô application using a robot or voice assistant. This application will allow the user to teach a virtual agent (robot or voice assistant) a joke/story. As the agent learns by demonstration, the user can practice and refine how the story/joke is told until the voice assistant (and the child) is able to tell the joke/story in a satisfactory way.\n Design the Learning Scenario Explore TTS software for speech conversion Implement a new Alexa Skill Run a Pilot demonstrating the learning of joke/story telling features (e.g. pauses and intonations)  Topics Voice Assistant, Machine Learning, HCI\nPrerequisites  Skills: Python or C++. Git.  References  https://github.com/soobinseo/Transformer-TTS https://github.com/barronalex/Tacotron https://developer.amazon.com/en-US/alexa/alexa-skills-kit  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3fccfa65c20178130e3b95f7972f12e7","permalink":"https://wafajohal.github.io/prospective/undergrad/fyv_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/fyv_2020/","section":"prospective","summary":"Context Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development (e.g., autism spectrum disorder, speech or hearing disorders).\nThe voices of these children often go unheard, as they find it hard to contribute to a conversation.\nThe Find your Voice (FyV, http://wafa.johal.org/project/fyv/) project was initiated to investigate how joke telling could help children to speak up and gain confidence.","tags":["voice-assistant","cse-honours"],"title":"Find Your Voice: Use of Voice Assistant for Learning","type":"prospective"},{"authors":null,"categories":null,"content":"Context Action Recognition is curcial for robots to perfoma around humans. Indeed, robot need to asses human action and intentions in order to assist them in everyday life tasks and to collaborative efficiently.\nThe field of action recognition has aimed to use typical sensors found on robots to recognize agnts, objects and actions they are performing. Typical approach is to record a dataset of various action and label them. But often theses actions are not natural and it can be difficult to represent the variety of ways to perform actions with a lab built dataset. In this project we propose to use audio desription movies to label actions. AD movies integrate a form of narration to allow virually impaired veiwers to undertsnad the visual element sowed on screen. This information often deals with action actually depicted on the scene.\nGoals \u0026amp; Milestones During this project, the student will:\n Develop a pipeline to collect and crop clip of AD movies for at home actions. This extraction tool should be fexible and allow for integration of next action. It will for instance feature video and text processing to extract [Subject+ Action + Object] type of data. Investigate methods for HAR Implement a tree model combaning HAR with YOLO to identify agent and objects Evaluate the HAR pipeline with the Toyota Robot HSR  Topics Human Action Recognition,\nPrerequisites  Skills: Python, C++, Git.  References  https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54 https://prior.allenai.org/projects/charades https://arxiv.org/pdf/1708.02696.pdf https://arxiv.org/pdf/1806.11230.pdf  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"190eaed982907f37b92ff19ced9aea03","permalink":"https://wafajohal.github.io/prospective/undergrad/human_action_recognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/human_action_recognition/","section":"prospective","summary":"Context Action Recognition is curcial for robots to perfoma around humans. Indeed, robot need to asses human action and intentions in order to assist them in everyday life tasks and to collaborative efficiently.\nThe field of action recognition has aimed to use typical sensors found on robots to recognize agnts, objects and actions they are performing. Typical approach is to record a dataset of various action and label them. But often theses actions are not natural and it can be difficult to represent the variety of ways to perform actions with a lab built dataset.","tags":["machine learning","robotics","cse-honours"],"title":"Human Action Recognition from AD Movies","type":"prospective"},{"authors":null,"categories":null,"content":"Context In the context of the (HURL project)[https://wafa.johal.org/project/hurl/], we investigate various ways to enable novice users to train robots to perform everyday life tasks. In particular, we employ gamification and Augmented Reality (AR) to make the training more engaging for the human trainer.\nIn this project, your goal will be to implement a game \u0026ldquo;I spy\u0026rdquo; to be played with a robot in a home environment. For this game, we would like to use AR with the Hololens 2\nGoals \u0026amp; Milestones During this project, the student will:\n Learn about Hololens and Augmented Reality development Develop a game such as \u0026ldquo;I spy\u0026rdquo; using object recognition models (such as YOLO) for both the human and robot to play Evaluate the game in the robotics lab at UNSW  Topics Augmented Reality, Object Recognition, Machine Learning, Human-Robot Interaction\nPrerequisites or willing to learn Skills: C#, Python, Git D or above in a machine learning or AI course\nReferences  https://github.com/AlturosDestinations/Alturos.Yolo HURL project: https://wafa.johal.org/project/hurl/ https://github.com/doughtmw/YoloDetectionHoloLens-Unity  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"30f695fe1c10c75fc8d82db8ada6ab34","permalink":"https://wafajohal.github.io/prospective/undergrad/ispy_hololens_2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/ispy_hololens_2021/","section":"prospective","summary":"Context In the context of the (HURL project)[https://wafa.johal.org/project/hurl/], we investigate various ways to enable novice users to train robots to perform everyday life tasks. In particular, we employ gamification and Augmented Reality (AR) to make the training more engaging for the human trainer.\nIn this project, your goal will be to implement a game \u0026ldquo;I spy\u0026rdquo; to be played with a robot in a home environment. For this game, we would like to use AR with the Hololens 2","tags":["machine-learning","augmented-reality","human-robot-interaction","cse-honours"],"title":"I Spy - An AR Game with a Robot in Home Environment","type":"prospective"},{"authors":null,"categories":null,"content":"Context The field of social human-robot interaction is growing. Understanding how communication between humans (human-human) generalises during human-robot communication is crucial in building fluent and enjoyable interactions with social robots. Everyday, more datasets that feature social interaction between humans and between humans and robots are made freely available online.\nIn this project we propose to take a data-driven approach to build predictive models of social interactions between humans (HH) ( and between humans and robots (HR) interaction using 3 different datasets. Relevant research questions include:\n Which multi-modal features can be transferable from HH to HR setups? Are there common features that discriminate human behaviour in HH or HR scenarios (e.g. \u0026lsquo;Do people speak less or slower with robots?\u0026rsquo; \u0026hellip; )  Goals \u0026amp; Milestones During this project, the student will:\n Explore datasets (PinSoRo, MHHRI and P2PSTORY): type of data (video; audio, point cloud), available labels and annotation \u0026hellip; Extract relevant features multimodal on each dataset Evaluate predictive models for each dataset (i.e. engagement) Explore transfer learning from one dataset to another  There is also potential to use UNSW‚Äôs National Facility for Human-Robot Interaction Research to create a new dataset.\nTopics Machine Learning, Human-Robot Interaction\nPrerequisites  Skills: Python, ROS, Git.  References  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999 https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/ https://www.media.mit.edu/projects/p2pstory/overview/  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e82d019d099947b47f1a8349d2ec6a2c","permalink":"https://wafajohal.github.io/prospective/undergrad/hri-dataset-mine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/hri-dataset-mine/","section":"prospective","summary":"Context The field of social human-robot interaction is growing. Understanding how communication between humans (human-human) generalises during human-robot communication is crucial in building fluent and enjoyable interactions with social robots. Everyday, more datasets that feature social interaction between humans and between humans and robots are made freely available online.\nIn this project we propose to take a data-driven approach to build predictive models of social interactions between humans (HH) ( and between humans and robots (HR) interaction using 3 different datasets.","tags":["machine-learning","hri","cse-honours","affective-computing"],"title":"Machine Learning for Social Interaction Modelling","type":"prospective"},{"authors":null,"categories":null,"content":"Context Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few). Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user‚Äôs knowledge.\nBehavioural styles allow robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture. Behavioural styles have been studied in the past to improve robot\u0026rsquo;s behaviour during human-robot interaction [2].\nIn this project, we will explore how behavioural styles can influence engagement, trust and persuasion during human-robot interaction.\nGoals \u0026amp; Milestones  Implement behavioural styles for the Nao robot (voice and behaviour) and for a voice assistant (voice only) Design at least two behaviour styles based on human behaviour and personality styles Evaluate and compare these styles via experimentation Design a scenario similar to the one described in paper [3] Setup a data collection environment (posture, video and audio) in the HRI Lab facility of UNSW Select appropriate tasks and/or questionnaires to measure engagement, trust and/or persuasion Evaluate the system via an experiment with users Complete the data analysis  Topics Robotics, HRI, Psychology\nPrerequisites  Skills: Python, ROS and Git.  References  [1]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2019/10/14081257/Robots_social_impact_eng.pdf [2] Johal, W., Pesty, S., \u0026amp; Calvary, G. (2014, August). Towards companion robots behaving with style. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1063-1068). IEEE. [3] Bainbridge, W. A., Hart, J. W., Kim, E. S., \u0026amp; Scassellati, B. (2011). The benefits of interactions with physically present robots over video-displayed agents. International Journal of Social Robotics, 3(1), 41-52. [4] Peters, R., Broekens, J., Li, K., \u0026amp; Neerincx, M. A. (2019, July). Robot Dominance Expression Through Parameter-based Behaviour Modulation. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp. 224-226). ACM. [5] Shane Saunderson et al. It Would Make Me Happy if You Used My Guess: Comparing Robot Persuasive Strategies in Social Human‚ÄìRobot Interaction, IEEE Robotics and Automation Letters (2019). DOI: 10.1109/LRA.2019.2897143  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"19e44b1e4a8ff02a1ff888bd0cd84523","permalink":"https://wafajohal.github.io/prospective/undergrad/behavioural-styles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/behavioural-styles/","section":"prospective","summary":"Context Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few). Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user‚Äôs knowledge.\nBehavioural styles allow robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture.","tags":["robotics","human-robot interaction","experiment","cse-honours"],"title":"Persuasive Robots - Exploring Behavioural Styles","type":"prospective"},{"authors":null,"categories":null,"content":" Candidate Profile Required Skills  Bachelor or masters degree in Computer Sciences or a related field Good programming skills are required: C++, Python or others Training and skills in AI, machine learning or robotics is preferable Knowledge of classical programming frameworks in these domains is appreciated: Keras, PyTorch, TensorFlow, ROS, etc. English proficiency  Skills Nice to Have  A taste for user/application and impact driven research  Research Environment Depending on the research focus, joint supervisors specialised in Design, Robotics, AI or HCI, will be invited. UNSW and CSE will provide you with access to the HCI and Robotics Labs, GPU clusters and the HRI experimental facility equipped with more than 200 sensors (available to run studies). UNSW Sydney is in the top 100 universities in the world, with more than 59,000 students and a 7,000-strong research community. At UNSW, you will benefit from an international and vibrant research community in a pleasant environment. The net amount of the scholarship will be approximately AUD 2400 per month, increasing annually. You will also receive a holiday allowance. Additional financial support is available for attending conferences and workshops. There will also be possibilities for a research visit (industry or international) during the PhD.\nQueries For informal queries, do not hesitate to contact wafa.johal@unsw.edu.au\nYour application should include:\n A letter highlighting your motivation for your application including: Why do you wish to pursue a PhD in human-robot interaction? What prior experience do you have? How would you approach human-robot interaction research? Also indicate your research interests. A CV, with copies of relevant grades, masters thesis work or publications.   Admissions and Scholarships To enrol in the PhD program in computer science at UNSW, you are expected to have a degree that is equivalent to a First Class Honours degree in Computer Science, Mathematics, or a related field. Obtaining a PhD scholarship from UNSW is a very competitive process. UNSW\u0026rsquo;s online self-assessment tool gives a good indication of how competitive you are for scholarships. PhD students under my supervision are often eligible for top-up scholarships from Data61.\nAlong with the documents mentioned above, please send me the results of the online self-assessment tool.\nAll scholarship applicants need to prepare a research proposal in consultation with the prospective supervisor (around 500 words). The selection also usually takes into account your grades (mainly for the last 2 years of study), the ranking of your previous university, and your publications (if any).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6b07f4aefb9ac0a9f23dbaec9a52fd74","permalink":"https://wafajohal.github.io/prospective/phd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/phd/","section":"prospective","summary":"UNSW PhD program \u0026 Scholarships","tags":["position","social HRI","applied machine learning","user experiments"],"title":"PhD Positions","type":"prospective"},{"authors":null,"categories":null,"content":"Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school.\nIn this project we propose to explore new methods for a robot to write using a pen. The project will use the Fetch Robot and aim to evaluate how a robot could learn handwriting trajectories using human demonstrations.\nGoals \u0026amp; Milestones During this project, the student will:\n Learn about ROS Develop a ROS package that enables to control the Fetch Robot arm given a series of handwriting strokes Explore the use of different methods for the robot to learn letter writing from demonstrations Evaluate the implemented method compare to other state of the art methods  Topics ROS, Learning by Demonstration, Robotics, Handwriting\nPrerequisites  Skills: Python, C++, ROS, Git.  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"814e3f6b844a10aed2e5bd78ea0b4b88","permalink":"https://wafajohal.github.io/prospective/undergrad/writing_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/writing_2020/","section":"prospective","summary":"Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school.\nIn this project we propose to explore new methods for a robot to write using a pen. The project will use the Fetch Robot and aim to evaluate how a robot could learn handwriting trajectories using human demonstrations.\nGoals \u0026amp; Milestones During this project, the student will:","tags":["ROS","robotics","handwriting","co-writer","cse-honours","active"],"title":"Robot Writing","type":"prospective"},{"authors":null,"categories":null,"content":"Context While digital tools are more and more used in classrooms, teachers' common practice remains to use photocopied paper documents to share and collect learning exercises from their students. With the Tangible e-Ink Paper (TIP) system, we aim to explore the use of tangible manipulatives interacting with paper sheets as a bridge between digital and paper traces of learning. Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs are envisioned to be used as a versatile tool across various curriculum activities.\nGoals \u0026amp; Milestones  Literature Review on Tangible UI (TUI) in Education Implement and test a proof of concept of TIPs for learning Assemble 3 TIPs (3D printing of parts, soldering, etc.) Install libraries on Rasberry PI ( e.g. libdots - used for self-paper-based localisation, bluetooth communication) Develop two demo applications using TIPs for  individual work (on A4 sheet of paper) collaborative work (on min A2)    Topics Tangible User Interfaces, HCI\nPrerequisites and Learning Outcomes  Skills: Javascript, Python or C++. Git. Qt, Rasberry Pi  References  https://infoscience.epfl.ch/record/271833/files/paper.pdf https://infoscience.epfl.ch/record/224129/files/paper.pdf https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4319114a2a5ebb9ad7aa543b433c590c","permalink":"https://wafajohal.github.io/prospective/undergrad/tip_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/tip_2020/","section":"prospective","summary":"Context While digital tools are more and more used in classrooms, teachers' common practice remains to use photocopied paper documents to share and collect learning exercises from their students. With the Tangible e-Ink Paper (TIP) system, we aim to explore the use of tangible manipulatives interacting with paper sheets as a bridge between digital and paper traces of learning. Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs are envisioned to be used as a versatile tool across various curriculum activities.","tags":["tangible-interfaces","education","cse-honours"],"title":"Tangible e-Ink Paper Interfaces for Learners","type":"prospective"},{"authors":null,"categories":null,"content":"Context: Visuo-Motor coordination problems can impair children in their academic achievements and in their everyday life. Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children\u0026rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing. Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder or cerebral palsy and need undergo physical therapy. The therapy sessions are often not engaging for children and conducted individually. In this project, we aim to design new forms of interaction with a swarm for enhance visuo-motor coordination. We propose to develop a game that allows multiple children to play collaboratively on the same table.\nGoals \u0026amp; Milestones  Implement the set of basic swarm behaviour using 4 Cellulo robots Integrate collaorative and tangible interactions Test the system with a participants. We plan ti integrate a measure of cognitive load using eye tracking data  Topics HCI, Health, Game, Swarm Robotics\nPrerequisites  Skills: Python, C++, Js  References  https://www.epfl.ch/labs/chili/index-html/research/cellulo/  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff4b56ea1caf8a7cc2de574775ac7d8e","permalink":"https://wafajohal.github.io/prospective/undergrad/h-swarm_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/h-swarm_2020/","section":"prospective","summary":"Context: Visuo-Motor coordination problems can impair children in their academic achievements and in their everyday life. Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children\u0026rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing. Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder or cerebral palsy and need undergo physical therapy. The therapy sessions are often not engaging for children and conducted individually.","tags":["cellulo","tangible","swarm","cse-honours"],"title":"Tangible Human Swarm Interaction","type":"prospective"},{"authors":null,"categories":null,"content":"Context: Online learning presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student\u0026rsquo;s part to plan and stay assiduous in their learning.\nGoals \u0026amp; Milestones In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots. The project will consist in:\n developing a tool allowing the design of online activities for two or more robots to be connected implementing a demonstrator for this new library that will embed a series of small exercises hilightling the new capability of remote haptic-assisted collaboration evaluating the demonstrator with a user experiment  Topics HCI, Haptics, Robot, Collaborative Work (Training/Gaminig)\nPrerequisites  Skills: C++, Js,  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC\n Schneider, B., Jermann, P., Zufferey, G., \u0026amp; Dillenbourg, P. (2011). Benefits of a Tangible Interface for Collaborative Learning and Interaction. IEEE Transactions on Learning Technologies, 4(3), 222‚Äì232. https://doi.org/10.1109/TLT.2010.36 Asselborn, T., Guneysu, A., Mrini, K., Yadollahi, E., Ozgur, A., Johal, W., \u0026amp; Dillenbourg, P. (2018). Bringing letters to life: Handwriting with haptic-enabled tangible robots. Proceedings of the 17th ACM Conference on Interaction Design and Children, 219‚Äì230. East, B., DeLong, S., Manshaei, R., Arif, A., \u0026amp; Mazalek, A. (2016). Actibles: Open Source Active Tangibles. Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces, 469‚Äì472. https://doi.org/10.1145/2992154.2996874 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2019a). RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 318‚Äì328. https://doi.org/10.1145/3308561.3353804 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2019b). RoboGraphics: Using Mobile Robots to Create Dynamic Tactile Graphics. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 673‚Äì675. https://doi.org/10.1145/3308561.3354597 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2018). The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations. Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces, 203‚Äì211. https://doi.org/10.1145/3279778.3279805 Guneysu, A., Johal, W., Ozgur, A., \u0026amp; Dillenbourg, P. (2018). Tangible Robots Mediated Collaborative Rehabilitation Design: Can we Find Inspiration from Scripting Collaborative Learning? Workshop on Robots for Learning R4L HRI2018. Guneysu Ozgur, A., Wessel, M. J., Johal, W., Sharma, K., Ozgur, A., Vuadens, P., Mondada, F., Hummel, F. C., \u0026amp; Dillenbourg, P. (2018). Iterative design of an upper limb rehabilitation game with tangible robots. Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 241‚Äì250. Guneysu Ozgur, A., Wessel, M. J., Olsen, J. K., Johal, W., √ñzg√ºr, A., Hummel, F. C., \u0026amp; Dillenbourg, P. (2020). Gamified Motor Training with Tangible Robots in Older Adults: A Feasibility Study and Comparison with Young. Frontiers in Aging Neuroscience, 12. https://doi.org/10.3389/fnagi.2020.00059 Ishii, H., \u0026amp; Ullmer, B. (1997). Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 234‚Äì241. https://doi.org/10.1145/258549.258715 Johal, W., Tran, A., Khodr, H., √ñzg√ºr, A., \u0026amp; Dillenbourg, P. (2019). TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration. Proceedings of the 31st Australian Conference on Human-Computer-Interaction, 595‚Äì598. https://doi.org/10.1145/3369457.3369539 Loparev, A., Westendorf, L., Flemings, M., Cho, J., Littrell, R., Scholze, A., \u0026amp; Shaer, O. (2017). BacPack: Exploring the Role of Tangibles in a Museum Exhibit for Bio-Design. Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction, 111‚Äì120. https://doi.org/10.1145/3024969.3025000 Okerlund, J., Shaer, O., \u0026amp; Latulipe, C. (2016). Teaching Computational Thinking Through Bio-Design (Abstract Only). Proceedings of the 47th ACM Technical Symposium on Computing Science Education, 698. https://doi.org/10.1145/2839509.2850569 O‚ÄôMalley, C., \u0026amp; Fraser, D. S. (2004). Literature review in learning with tangible technologies. Ozgur, A. G., Wessel, M. J., Asselborn, T., Olsen, J. K., Johal, W., √ñzg√ºr, A., Hummel, F. C., \u0026amp; Dillenbourg, P. (2019). Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories? 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 5326‚Äì5330. https://doi.org/10.1109/EMBC.2019.8857508 Ozgur, A., Johal, W., Mondada, F., \u0026amp; Dillenbourg, P. (2017). Haptic-enabled handheld mobile robots: Design and analysis. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2449‚Äì2461. Ozgur, A., Lemaignan, S., Johal, W., Beltran, M., Briod, M., Pereyre, L., Mondada, F., \u0026amp; Dillenbourg, P. (2017). Cellulo: Versatile handheld robots for education. 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI, 119‚Äì127.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d1d93155497c3217eaa38e6d820f3236","permalink":"https://wafajohal.github.io/prospective/undergrad/tangible_online/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/tangible_online/","section":"prospective","summary":"Context: Online learning presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student\u0026rsquo;s part to plan and stay assiduous in their learning.\nGoals \u0026amp; Milestones In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots.","tags":["cellulo","tangible","collaborative learning","cse-honours"],"title":"Tangible Robots for Collaborative Online Learning","type":"prospective"},{"authors":null,"categories":null,"content":"Context: Service robots are becoming more common in households. Tasks that they may be assigned include vacuuming the floor or grabbing objects from the top shelf. Augmented reality (AR) is one of the many interfaces the robot and the human user may communicate through. During teleoperation or robot training, human disengagement can be an issue (the task demonstrated by the human often repetitive).\nAs a preliminary step, we aim to explore how gamification can affect human engagement in an AR game setting where the objective of the game is to complete a common household task (such as clearing the table). The AR application will be developed for the Microsoft HoloLens 2 headset. The game can be thought of as encouraging the user to complete the task.\nThe student will in particular explore how game mechanics and appealing visuals could be inserted through AR to engage the user in repetitive tasks.\nGoals \u0026amp; Milestones:  Design the game Implement teleoperation module for the Kinova Gen 3 Robot. Implement gamification elements Evaluate the impact of the gamification elements via a user study  Topics: Augmented Reality, Games, Gamification, Robot Teleoperation\nPrerequisites and Skills (demonstrated or willing to learn): Python, C#, Unity Framework, ROS (Robotics OS) Game Design, Computer Vision\nReferences:  http://cs.brown.edu/people/er35/publications/testing.pdf https://ieeexplore.ieee.org/abstract/document/8673306/?casa_token=YjUuijSVKagAAAAA:_SEgCzetsb-CGXU9TLM_3D62yvn8cTyTiMooaiVSbsw2Jm1jj-kgb4QYCOFKc7vCPQTekd_pQWU https://dl.acm.org/doi/abs/10.1145/3171221.3171251?casa_token=n9ai5qrDzZcAAAAA:Zr_zUFNrar-Zdidk2cMwzWcySh1SeEAIYtagwLHgZFyuze_lbn9cm0ybobYFGQYit6VQYa7rn_KssFA  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4f2698aee198e3fc264a3a238546e2a5","permalink":"https://wafajohal.github.io/prospective/undergrad/cleanup_2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/cleanup_2021/","section":"prospective","summary":"Context: Service robots are becoming more common in households. Tasks that they may be assigned include vacuuming the floor or grabbing objects from the top shelf. Augmented reality (AR) is one of the many interfaces the robot and the human user may communicate through. During teleoperation or robot training, human disengagement can be an issue (the task demonstrated by the human often repetitive).\nAs a preliminary step, we aim to explore how gamification can affect human engagement in an AR game setting where the objective of the game is to complete a common household task (such as clearing the table).","tags":["augmented-reality","human-robot-interaction","cse-honours"],"title":"TeleOp Game - Explore Gamification of Robot Teleoperation","type":"prospective"},{"authors":null,"categories":null,"content":"Context Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.\nA scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.\nGoals \u0026amp; Milestones During this project, the student will:\n Learn about Google DialogFlow and ROS Develop a ROS package that enables to access and manipulates DialogFlow features Develop a Cellulo Rehabilitation Game Test the game with a pilot experiment  Topics Voice-Assistant, Human-Robot Interaction, ROS\nPrerequisites  Skills: Python, C++, ROS, Git.  References  https://dialogflow.com/ https://www.ros.org/ http://wafa.johal.org/project/cellulo/ Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., \u0026amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE. Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore. Beirl, D., Yuill, N., \u0026amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., LavoueÃÅ, E., Gweon, C. H., \u0026amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"52562cd547497343def7c8bf9ba1c493","permalink":"https://wafajohal.github.io/prospective/undergrad/voice-robot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/voice-robot/","section":"prospective","summary":"Context Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.","tags":["ROS","robotics","voice-assistant","cellulo","cse-honours"],"title":"Voice for ROS","type":"prospective"}]